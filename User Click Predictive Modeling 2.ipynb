{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a PCAsearch to find ideal number of components to keep in the pipeline\n",
    "\n",
    "We will use the same pipeline but grid search the number of components. We noticed that the entire balanced training set does not fit into memory, therefore we will need to use a random sample from the training set. for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Read the pickled balanced training set\n",
    "X_train_balanced = pd.read_pickle(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_train_balanced.pkl\")\n",
    "y_train_balanced = pd.read_pickle(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/y_train_balanced.pkl\")\n",
    "\n",
    "#let's try to get a small random sample (~10%) from the class-balanced set to train the pipeline\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(112)\n",
    "rows = random.sample(list(range(0,X_train_balanced.shape[0])),80000)\n",
    "\n",
    "X_train_balanced_sample = X_train_balanced.iloc[rows,]\n",
    "y_train_balanced_sample = y_train_balanced.iloc[rows]\n",
    "\n",
    "\n",
    "# Label text features\n",
    "Text_features = [\"app\",\"device\",\"os\",\"channel\"]\n",
    "\n",
    "##############################################################\n",
    "# Define utility function to parse and process text features\n",
    "##############################################################\n",
    "# Note we avoid lambda functions since they don't pickle when we want to save the pipeline later   \n",
    "def column_text_processer_nolambda(df,text_columns = Text_features):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \"\"\"\"A function that will merge/join all text in a given row to make it ready for tokenization. \n",
    "    - This function should take care of converting missing values to empty strings. \n",
    "    - It should also convert the text to lowercase.\n",
    "    df= pandas dataframe\n",
    "    text_columns = names of the text features in df\n",
    "    \"\"\" \n",
    "    # Select only non-text columns that are in the df\n",
    "    text_data = df[text_columns]\n",
    "    \n",
    "    # Fill the missing values in text_data using empty strings\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    \n",
    "    # Concatenate feature name to each category encoding for each row\n",
    "    # E.g: encoding 3 at device column will read as device3 to make each encoding unique for a given feature\n",
    "    for col_index in list(text_data.columns):\n",
    "        text_data[col_index] = col_index + text_data[col_index].astype(str)\n",
    "    \n",
    "    # Join all the strings in a given row to make a vector\n",
    "    # text_vector = text_data.apply(lambda x: \" \".join(x), axis = 1)\n",
    "    text_vector = []\n",
    "    for index,rows in text_data.iterrows():\n",
    "        text_item = \" \".join(rows).lower()\n",
    "        text_vector.append(text_item)\n",
    "\n",
    "    # return text_vector as pd.Series object to enter the tokenization pipeline\n",
    "    return pd.Series(text_vector)\n",
    "\n",
    "#######################################################################\n",
    "# Define custom processing functions to add the log_total_clicks and \n",
    "# log_total_click_time features, and remove the unwanted base features\n",
    "#######################################################################\n",
    "def column_time_processer(X_train):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert click_time to datetime64 dtype \n",
    "    X_train.click_time = pd.to_datetime(X_train.click_time)\n",
    "\n",
    "    # Calculate the log_total_clicks for each ip and add as a new feature to temp_data\n",
    "    temp_data = pd.DataFrame(np.log(X_train.groupby([\"ip\"]).size()),\n",
    "                                    columns = [\"log_total_clicks\"]).reset_index()\n",
    "\n",
    "\n",
    "    # Calculate the log_total_click_time for each ip and add as a new feature to temp_data\n",
    "    # First define a function to process selected ip group \n",
    "    def get_log_total_click_time(group):\n",
    "        diff = (max(group.click_time) - min(group.click_time)).seconds\n",
    "        return np.log(diff+1)\n",
    "\n",
    "    # Then apply this function to each ip group and extract the total click time per ip group\n",
    "    log_time_frame = pd.DataFrame(X_train.groupby([\"ip\"]).apply(get_log_total_click_time),\n",
    "                                  columns=[\"log_total_click_time\"]).reset_index()\n",
    "\n",
    "    # Then add this new feature to the temp_data\n",
    "    temp_data = pd.merge(temp_data,log_time_frame, how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Combine temp_data with X_train to maintain X_train key order\n",
    "    temp_data = pd.merge(X_train,temp_data,how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Drop features that are not needed\n",
    "    temp_data = temp_data[[\"log_total_clicks\",\"log_total_click_time\"]]\n",
    "\n",
    "    # Return only the numeric features as a tensor to integrate into the numeric feature branch of the pipeline\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# We need to wrap these custom utility functions using FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# FunctionTransformer wrapper of utility functions to parse text and numeric features\n",
    "# Note how we avoid putting any arguments into column_text_processer or column_time_processer\n",
    "#############################################################################\n",
    "get_numeric_data = FunctionTransformer(func = column_time_processer, validate=False) \n",
    "get_text_data = FunctionTransformer(func = column_text_processer_nolambda,validate=False) \n",
    "\n",
    "#############################################################################\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "# #Note this regex will match either a whitespace or a punctuation to tokenize \n",
    "# the string vector on these preferences, in our case we only have white spaces in our text  \n",
    "#############################################################################\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "\n",
    "# Load a validation set to use in the new pca pipeline\n",
    "X_val1 = pd.read_pickle(\"X_val1.pkl\")\n",
    "y_val1 = pd.read_pickle(\"y_val1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our main search loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline search using: 10 components.\n",
      "Completed pipeline fit and transform using 10 components, it took: 2.25 minutes.\n",
      "Completed model fit and it took: 0.0 minutes.\n",
      "ROC score in the training set: 0.865398828872\n",
      "Completed validation set transformation, it took: 0.0 minutes.\n",
      "ROC score in the validation set 1: 0.880240252656\n",
      "Saved the pipeline search using: 10 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca10.pkl\n",
      "Completed pipeline search using: 10 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 600 components.\n",
      "Completed pipeline fit and transform using 600 components, it took: 3.533333333333333 minutes.\n",
      "Completed model fit and it took: 5.316666666666666 minutes.\n",
      "ROC score in the training set: 0.960074287179\n",
      "Completed validation set transformation, it took: 5.316666666666666 minutes.\n",
      "ROC score in the validation set 1: 0.931542083596\n",
      "Saved the pipeline search using: 600 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca600.pkl\n",
      "Completed pipeline search using: 600 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 700 components.\n",
      "Completed pipeline fit and transform using 700 components, it took: 4.3 minutes.\n",
      "Completed model fit and it took: 6.666666666666667 minutes.\n",
      "ROC score in the training set: 0.9605818804\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2 # We will use chi-squared as a scoring function to select features for classification\n",
    "from sklearn.metrics import auc\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module since it was saved into working directory as SparseInteractions.py\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "disk_directory = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "component_list = [10,600,700,800]\n",
    "\n",
    "for n_components in component_list:\n",
    "    print(\"Starting pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    \n",
    "    # Generic pipeline to cycle for each component search\n",
    "    userclick_pipeline_pcasearch = Pipeline([\n",
    "\n",
    "        (\"union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [\n",
    "\n",
    "                (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                    (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                    (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                ])), # End of: numeric_subpipeline\n",
    "\n",
    "                (\"text_subpipeline\",Pipeline([\n",
    "                    (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                    (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                 ngram_range = (1,1),\n",
    "                                                 non_negative=True, \n",
    "                                                 norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                    ('dim_red', SelectKBest(chi2,300)) # Step3: use dimension reduction to select 300 best features using chi2 as scoring function\n",
    "                ]))\n",
    "            ]\n",
    "\n",
    "        )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "        # Common steps:\n",
    "\n",
    "        (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "        (\"scaler\",StandardScaler(with_mean=False)), # Standardize the features for a more gaussian distribution. \n",
    "        (\"dim_red\", TruncatedSVD(n_components=n_components))      \n",
    "    ])# End of: userclick_pipeline_pcasearch\n",
    "\n",
    "    # Fit and transform the X_train_balanced_sample set to get the features using new pipeline\n",
    "    start = datetime.datetime.now()\n",
    "    X_train_balanced_sample_trans_pl_pcasearch = userclick_pipeline_pcasearch.fit(X_train_balanced_sample,y_train_balanced_sample).transform(X_train_balanced_sample)\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed pipeline fit and transform using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    # Train the classifier and get estimates\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_balanced_sample_trans_pl_pcasearch,y_train_balanced_sample)\n",
    "\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed model fit and it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "    probs = clf.predict_proba(X_train_balanced_sample_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the training set: \" + str(roc_auc_score(y_train_balanced_sample,probs)))\n",
    "\n",
    "    # Transform the validation set\n",
    "    start = datetime.datetime.now()\n",
    "    X_val1_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_val1)\n",
    "    print(\"Completed validation set transformation, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    probs = clf.predict_proba(X_val1_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the validation set 1: \" + str(roc_auc_score(y_val1,probs)))\n",
    "    \n",
    "    # Save the current pipeline\n",
    "    filename = disk_directory + \"userclick_pipeline_pca\" + str(n_components) + \".pkl\"\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(userclick_pipeline_pcasearch,f)\n",
    "    print(\"Saved the pipeline search using: \"+ str(n_components)+ \" components \" + \"as :\" + filename)\n",
    "    \n",
    "    print(\"Completed pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    print(\"--\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the 700 components, kernel breaks down, therefore this is not feasible to search components in this way. Let's try to modify the pipeline for different strategies in feature selection and dimension reduction.\n",
    "\n",
    "How about simple feature selection at the end of the pipeline instead of PCA, could it be faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline search using: 10 components.\n",
      "Completed pipeline fit using 10 components, it took: 0.0 minutes.\n",
      "Completed pipeline transform using 10 components, it took: 0.8666666666666667 minutes.\n",
      "Completed model fit and it took: 0.0 minutes.\n",
      "ROC score in the training set: 0.811026316688\n",
      "Completed validation set transformation, it took: 0.0 minutes.\n",
      "ROC score in the validation set 1: 0.558237052807\n",
      "Saved the pipeline search using: 10 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_nfeatures10.pkl\n",
      "Completed pipeline search using: 10 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 100 components.\n",
      "Completed pipeline fit using 100 components, it took: 0.0 minutes.\n",
      "Completed pipeline transform using 100 components, it took: 0.7666666666666667 minutes.\n",
      "Completed model fit and it took: 0.016666666666666666 minutes.\n",
      "ROC score in the training set: 0.944834862071\n",
      "Completed validation set transformation, it took: 0.016666666666666666 minutes.\n",
      "ROC score in the validation set 1: 0.888466684363\n",
      "Saved the pipeline search using: 100 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_nfeatures100.pkl\n",
      "Completed pipeline search using: 100 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 600 components.\n",
      "Completed pipeline fit using 600 components, it took: 0.016666666666666666 minutes.\n",
      "Completed pipeline transform using 600 components, it took: 0.8166666666666667 minutes.\n",
      "Completed model fit and it took: 0.06666666666666667 minutes.\n",
      "ROC score in the training set: 0.962187273708\n",
      "Completed validation set transformation, it took: 0.06666666666666667 minutes.\n",
      "ROC score in the validation set 1: 0.922357116847\n",
      "Saved the pipeline search using: 600 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_nfeatures600.pkl\n",
      "Completed pipeline search using: 600 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 800 components.\n",
      "Completed pipeline fit using 800 components, it took: 0.06666666666666667 minutes.\n",
      "Completed pipeline transform using 800 components, it took: 0.7833333333333333 minutes.\n",
      "Completed model fit and it took: 0.08333333333333333 minutes.\n",
      "ROC score in the training set: 0.963342697526\n",
      "Completed validation set transformation, it took: 0.08333333333333333 minutes.\n",
      "ROC score in the validation set 1: 0.922793092699\n",
      "Saved the pipeline search using: 800 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_nfeatures800.pkl\n",
      "Completed pipeline search using: 800 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 1000 components.\n",
      "Completed pipeline fit using 1000 components, it took: 0.08333333333333333 minutes.\n",
      "Completed pipeline transform using 1000 components, it took: 0.7833333333333333 minutes.\n",
      "Completed model fit and it took: 0.08333333333333333 minutes.\n",
      "ROC score in the training set: 0.964220175802\n",
      "Completed validation set transformation, it took: 0.08333333333333333 minutes.\n",
      "ROC score in the validation set 1: 0.924431806602\n",
      "Saved the pipeline search using: 1000 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_nfeatures1000.pkl\n",
      "Completed pipeline search using: 1000 components.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest # We will use default scoring function to select features for classification\n",
    "from sklearn.metrics import auc\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module since it was saved into working directory as SparseInteractions.py\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "disk_directory = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "component_list = [10,100,600,800,1000]\n",
    "\n",
    "for n_components in component_list:\n",
    "    print(\"Starting pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    \n",
    "    # Generic pipeline to cycle for each component search\n",
    "    userclick_pipeline_pcasearch = Pipeline([\n",
    "\n",
    "        (\"union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [\n",
    "\n",
    "                (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                    (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                    (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                ])), # End of: numeric_subpipeline\n",
    "\n",
    "                (\"text_subpipeline\",Pipeline([\n",
    "                    (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                    (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                 ngram_range = (1,1),\n",
    "                                                 non_negative=True, \n",
    "                                                 norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                    ('dim_red', SelectKBest(k = 300)) # Step3: use dimension reduction to select 300 best features using chi2 as scoring function\n",
    "                ]))\n",
    "            ]\n",
    "\n",
    "        )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "        # Common steps:\n",
    "        (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "        (\"scaler\",StandardScaler(with_mean=False)), # Standardize the features for a more gaussian distribution. \n",
    "        (\"dim_red2\", SelectKBest(k = n_components))      \n",
    "    ])# End of: userclick_pipeline_pcasearch\n",
    "\n",
    "    # Fit and transform the X_train_balanced_sample set to get the features using new pipeline\n",
    "    start = datetime.datetime.now()\n",
    "    userclick_pipeline_pcasearch.fit(X_train_balanced_sample,y_train_balanced_sample)\n",
    "    print(\"Completed pipeline fit using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    X_train_balanced_sample_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_train_balanced_sample)\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed pipeline transform using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    # Train the classifier and get estimates\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_balanced_sample_trans_pl_pcasearch,y_train_balanced_sample)\n",
    "\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed model fit and it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "    probs = clf.predict_proba(X_train_balanced_sample_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the training set: \" + str(roc_auc_score(y_train_balanced_sample,probs)))\n",
    "\n",
    "    # Transform the validation set\n",
    "    start = datetime.datetime.now()\n",
    "    X_val1_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_val1)\n",
    "    print(\"Completed validation set transformation, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    probs = clf.predict_proba(X_val1_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the validation set 1: \" + str(roc_auc_score(y_val1,probs)))\n",
    "    \n",
    "    # Save the current pipeline\n",
    "    filename = disk_directory + \"userclick_pipeline_nfeatures\" + str(n_components) + \".pkl\"\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(userclick_pipeline_pcasearch,f)\n",
    "    print(\"Saved the pipeline search using: \"+ str(n_components)+ \" components \" + \"as :\" + filename)\n",
    "    \n",
    "    print(\"Completed pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    print(\"--\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of selection is indeed faster and also fits into memory. However, out-of-the-box performance of the selected features are not as good as the PCA features. How about we perform a feature union of first 20 Principal components and 1000 - 2000 best features selected before decomposition? Beside, let's try to use the entire training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline search using: 20 components.\n",
      "Completed pipeline fit using 20 components, it took: 0.26666666666666666 minutes.\n",
      "Completed pipeline transform using 20 components, it took: 2.85 minutes.\n",
      "The shape of the transformed set is: (835222, 40)\n",
      "Completed model fit and it took: 0.2833333333333333 minutes.\n",
      "ROC score in the training set: 0.94698564278\n",
      "Completed validation set transformation, it took: 2.8333333333333335 minutes.\n",
      "The shape of the transformed validation set is: (1000000, 40)\n",
      "ROC score in the validation set 1: 0.916702108304\n",
      "Saved the pipeline search using: 20 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca20_nfeatures20.pkl\n",
      "Completed pipeline search using: 20 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 50 components.\n",
      "Completed pipeline fit using 50 components, it took: 2.8333333333333335 minutes.\n",
      "Completed pipeline transform using 50 components, it took: 2.85 minutes.\n",
      "The shape of the transformed set is: (835222, 70)\n",
      "Completed model fit and it took: 0.31666666666666665 minutes.\n",
      "ROC score in the training set: 0.951986784653\n",
      "Completed validation set transformation, it took: 2.8666666666666667 minutes.\n",
      "The shape of the transformed validation set is: (1000000, 70)\n",
      "ROC score in the validation set 1: 0.928977579534\n",
      "Saved the pipeline search using: 50 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca20_nfeatures50.pkl\n",
      "Completed pipeline search using: 50 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 500 components.\n",
      "Completed pipeline fit using 500 components, it took: 2.8666666666666667 minutes.\n",
      "Completed pipeline transform using 500 components, it took: 2.8833333333333333 minutes.\n",
      "The shape of the transformed set is: (835222, 520)\n",
      "Completed model fit and it took: 1.6333333333333333 minutes.\n",
      "ROC score in the training set: 0.967881145431\n",
      "Completed validation set transformation, it took: 2.933333333333333 minutes.\n",
      "The shape of the transformed validation set is: (1000000, 520)\n",
      "ROC score in the validation set 1: 0.948686052653\n",
      "Saved the pipeline search using: 500 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca20_nfeatures500.pkl\n",
      "Completed pipeline search using: 500 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 1000 components.\n",
      "Completed pipeline fit using 1000 components, it took: 2.933333333333333 minutes.\n",
      "Completed pipeline transform using 1000 components, it took: 2.9166666666666665 minutes.\n",
      "The shape of the transformed set is: (835222, 1020)\n",
      "Completed model fit and it took: 1.7333333333333334 minutes.\n",
      "ROC score in the training set: 0.968969147801\n",
      "Completed validation set transformation, it took: 2.9 minutes.\n",
      "The shape of the transformed validation set is: (1000000, 1020)\n",
      "ROC score in the validation set 1: 0.9517481426\n",
      "Saved the pipeline search using: 1000 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca20_nfeatures1000.pkl\n",
      "Completed pipeline search using: 1000 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 1200 components.\n",
      "Completed pipeline fit using 1200 components, it took: 2.9 minutes.\n",
      "Completed pipeline transform using 1200 components, it took: 2.8833333333333333 minutes.\n",
      "The shape of the transformed set is: (835222, 1220)\n",
      "Completed model fit and it took: 1.75 minutes.\n",
      "ROC score in the training set: 0.969082668647\n",
      "Completed validation set transformation, it took: 2.85 minutes.\n",
      "The shape of the transformed validation set is: (1000000, 1220)\n",
      "ROC score in the validation set 1: 0.951744156493\n",
      "Saved the pipeline search using: 1200 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca20_nfeatures1200.pkl\n",
      "Completed pipeline search using: 1200 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 1500 components.\n",
      "Completed pipeline fit using 1500 components, it took: 2.85 minutes.\n",
      "Completed pipeline transform using 1500 components, it took: 2.8666666666666667 minutes.\n",
      "The shape of the transformed set is: (835222, 1520)\n",
      "Completed model fit and it took: 2.55 minutes.\n",
      "ROC score in the training set: 0.969268358839\n",
      "Completed validation set transformation, it took: 2.8833333333333333 minutes.\n",
      "The shape of the transformed validation set is: (1000000, 1520)\n",
      "ROC score in the validation set 1: 0.952078808684\n",
      "Saved the pipeline search using: 1500 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca20_nfeatures1500.pkl\n",
      "Completed pipeline search using: 1500 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 2000 components.\n",
      "Completed pipeline fit using 2000 components, it took: 2.8833333333333333 minutes.\n",
      "Completed pipeline transform using 2000 components, it took: 2.8666666666666667 minutes.\n",
      "The shape of the transformed set is: (835222, 2020)\n",
      "Completed model fit and it took: 2.35 minutes.\n",
      "ROC score in the training set: 0.969519886446\n",
      "Completed validation set transformation, it took: 2.8333333333333335 minutes.\n",
      "The shape of the transformed validation set is: (1000000, 2020)\n",
      "ROC score in the validation set 1: 0.95227810283\n",
      "Saved the pipeline search using: 2000 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca20_nfeatures2000.pkl\n",
      "Completed pipeline search using: 2000 components.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest # We will use default scoring function to select features for classification\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module since it was saved into working directory as SparseInteractions.py\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "disk_directory = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "component_list = [20,50,500,1000,1200,1500,2000]\n",
    "\n",
    "for n_components in component_list:\n",
    "    print(\"Starting pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    \n",
    "    # Generic pipeline to cycle for each component search\n",
    "    userclick_pipeline_pcasearch = Pipeline([\n",
    "\n",
    "        (\"union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [\n",
    "\n",
    "                (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                    (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                    (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                ])), # End of: numeric_subpipeline\n",
    "\n",
    "                (\"text_subpipeline\",Pipeline([\n",
    "                    (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                    (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                 ngram_range = (1,1),\n",
    "                                                 non_negative=True, \n",
    "                                                 norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                    ('dim_red', SelectKBest(k = 300)) # Step3: use dimension reduction to select 300 best features using chi2 as scoring function\n",
    "                ]))\n",
    "            ]\n",
    "\n",
    "        )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "        # Common steps:\n",
    "        (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "        (\"scaler\",StandardScaler(with_mean=False)), # Standardize the features for a more gaussian distribution. \n",
    "        \n",
    "        # A new feature union\n",
    "        (\"final_union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [ (\"dim_red_feature\", SelectKBest(k = n_components)),\n",
    "                                 (\"dim_red_pca\", TruncatedSVD(n_components = 20)) ]\n",
    "        ))   \n",
    "             \n",
    "    ])# End of: userclick_pipeline_pcasearch\n",
    "\n",
    "    # Fit and transform the X_train_balanced_sample set to get the features using new pipeline\n",
    "    start = datetime.datetime.now()\n",
    "    userclick_pipeline_pcasearch.fit(X_train_balanced,y_train_balanced)\n",
    "    print(\"Completed pipeline fit using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    X_train_balanced_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_train_balanced)\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed pipeline transform using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    print(\"The shape of the transformed set is: \" + str(X_train_balanced_trans_pl_pcasearch.shape))\n",
    "    # Train the classifier and get estimates\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_balanced_trans_pl_pcasearch,y_train_balanced)\n",
    "\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed model fit and it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "    probs = clf.predict_proba(X_train_balanced_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the training set: \" + str(roc_auc_score(y_train_balanced,probs)))\n",
    "\n",
    "    # Transform the validation set\n",
    "    start = datetime.datetime.now()\n",
    "    X_val1_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_val1)\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed validation set transformation, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    print(\"The shape of the transformed validation set is: \" + str(X_val1_trans_pl_pcasearch.shape))\n",
    "    probs = clf.predict_proba(X_val1_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the validation set 1: \" + str(roc_auc_score(y_val1,probs)))\n",
    "    \n",
    "    # Save the current pipeline\n",
    "    filename = disk_directory + \"userclick_pipeline_pca20_nfeatures\" + str(n_components) + \".pkl\"\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(userclick_pipeline_pcasearch,f)\n",
    "    print(\"Saved the pipeline search using: \"+ str(n_components)+ \" components \" + \"as :\" + filename)\n",
    "    \n",
    "    print(\"Completed pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    print(\"--\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to improve the pipeline in this way. I am also curious what would be the impact of changing the features selected right after the tokenization (i.e.:binarization) step. Can we increase performance by tuning that step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline search using: 400 components.\n",
      "Completed pipeline fit using 400 components, it took: 2.8333333333333335 minutes.\n",
      "Completed pipeline transform using 400 components, it took: 3.35 minutes.\n",
      "The shape of the transformed set is: (835222, 2020)\n",
      "Completed model fit and it took: 3.9166666666666665 minutes.\n",
      "ROC score in the training set: 0.969724668347\n",
      "Completed validation set transformation, it took: 3.55 minutes.\n",
      "The shape of the transformed validation set is: (1000000, 2020)\n",
      "ROC score in the validation set 1: 0.952993052132\n",
      "Saved the pipeline search using: 400 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca20_2000features_nbinary400.pkl\n",
      "Completed pipeline search using: 400 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 500 components.\n",
      "Completed pipeline fit using 500 components, it took: 3.55 minutes.\n",
      "Completed pipeline transform using 500 components, it took: 3.95 minutes.\n",
      "The shape of the transformed set is: (835222, 2020)\n",
      "Completed model fit and it took: 4.0 minutes.\n",
      "ROC score in the training set: 0.969743617868\n",
      "Completed validation set transformation, it took: 3.9833333333333334 minutes.\n",
      "The shape of the transformed validation set is: (1000000, 2020)\n",
      "ROC score in the validation set 1: 0.953123650192\n",
      "Saved the pipeline search using: 500 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca20_2000features_nbinary500.pkl\n",
      "Completed pipeline search using: 500 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 750 components.\n",
      "Completed pipeline fit using 750 components, it took: 3.9833333333333334 minutes.\n",
      "Completed pipeline transform using 750 components, it took: 5.866666666666666 minutes.\n",
      "The shape of the transformed set is: (835222, 2020)\n",
      "Completed model fit and it took: 3.3333333333333335 minutes.\n",
      "ROC score in the training set: 0.969751192794\n",
      "Completed validation set transformation, it took: 5.933333333333334 minutes.\n",
      "The shape of the transformed validation set is: (1000000, 2020)\n",
      "ROC score in the validation set 1: 0.953250790473\n",
      "Saved the pipeline search using: 750 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca20_2000features_nbinary750.pkl\n",
      "Completed pipeline search using: 750 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 1000 components.\n",
      "Completed pipeline fit using 1000 components, it took: 5.933333333333334 minutes.\n",
      "Completed pipeline transform using 1000 components, it took: 8.383333333333333 minutes.\n",
      "The shape of the transformed set is: (835222, 2020)\n",
      "Completed model fit and it took: 4.05 minutes.\n",
      "ROC score in the training set: 0.969772394341\n",
      "Completed validation set transformation, it took: 8.566666666666666 minutes.\n",
      "The shape of the transformed validation set is: (1000000, 2020)\n",
      "ROC score in the validation set 1: 0.953305941476\n",
      "Saved the pipeline search using: 1000 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca20_2000features_nbinary1000.pkl\n",
      "Completed pipeline search using: 1000 components.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We keep the final feature union constant and focus on the selection following the binarization step \n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest # We will use default scoring function to select features for classification\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module since it was saved into working directory as SparseInteractions.py\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "disk_directory = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "component_list = [400,500,750,1000] # Note these refer to best features to be kept after the binarization step\n",
    "\n",
    "for n_components in component_list:\n",
    "    print(\"Starting pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    \n",
    "    # Generic pipeline to cycle for each component search\n",
    "    userclick_pipeline_pcasearch = Pipeline([\n",
    "\n",
    "        (\"union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [\n",
    "\n",
    "                (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                    (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                    (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                ])), # End of: numeric_subpipeline\n",
    "\n",
    "                (\"text_subpipeline\",Pipeline([\n",
    "                    (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                    (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                 ngram_range = (1,1),\n",
    "                                                 non_negative=True, \n",
    "                                                 norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                    ('dim_red', SelectKBest(k = n_components)) # Step3: use dimension reduction to select n_components number of best features using chi2 as scoring function\n",
    "                ]))\n",
    "            ]\n",
    "\n",
    "        )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "        # Common steps:\n",
    "        (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "        (\"scaler\",StandardScaler(with_mean=False)), # Standardize the features for a more gaussian distribution. \n",
    "        \n",
    "        # A new feature union\n",
    "        (\"final_union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [ (\"dim_red_feature\", SelectKBest(k = 2000)),\n",
    "                                 (\"dim_red_pca\", TruncatedSVD(n_components = 20)) ]\n",
    "        ))   \n",
    "             \n",
    "    ])# End of: userclick_pipeline_pcasearch\n",
    "\n",
    "    # Fit and transform the X_train_balanced_sample set to get the features using new pipeline\n",
    "    start = datetime.datetime.now()\n",
    "    userclick_pipeline_pcasearch.fit(X_train_balanced,y_train_balanced)\n",
    "    print(\"Completed pipeline fit using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    X_train_balanced_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_train_balanced)\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed pipeline transform using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    print(\"The shape of the transformed set is: \" + str(X_train_balanced_trans_pl_pcasearch.shape))\n",
    "    # Train the classifier and get estimates\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_balanced_trans_pl_pcasearch,y_train_balanced)\n",
    "\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed model fit and it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "    probs = clf.predict_proba(X_train_balanced_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the training set: \" + str(roc_auc_score(y_train_balanced,probs)))\n",
    "\n",
    "    # Transform the validation set\n",
    "    start = datetime.datetime.now()\n",
    "    X_val1_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_val1)\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed validation set transformation, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    print(\"The shape of the transformed validation set is: \" + str(X_val1_trans_pl_pcasearch.shape))\n",
    "    probs = clf.predict_proba(X_val1_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the validation set 1: \" + str(roc_auc_score(y_val1,probs)))\n",
    "    \n",
    "    # Save the current pipeline\n",
    "    filename = disk_directory + \"userclick_pipeline_pca20_2000features_nbinary\" + str(n_components) + \".pkl\"\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(userclick_pipeline_pcasearch,f)\n",
    "    print(\"Saved the pipeline search using: \"+ str(n_components)+ \" components \" + \"as :\" + filename)\n",
    "    \n",
    "    print(\"Completed pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    print(\"--\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of selected features post-binarization helped to improve pipeline performance. Let's continue to add some more PCs to see if it can further improve the performance of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline search using: 30 components.\n",
      "Completed pipeline fit using 30 components, it took: 8.566666666666666 minutes.\n",
      "Completed pipeline transform using 30 components, it took: 8.616666666666667 minutes.\n",
      "The shape of the transformed set is: (835222, 2030)\n",
      "Completed model fit and it took: 4.716666666666667 minutes.\n",
      "ROC score in the training set: 0.969774271669\n",
      "Completed validation set transformation, it took: 8.783333333333333 minutes.\n",
      "The shape of the transformed validation set is: (1000000, 2030)\n",
      "ROC score in the validation set 1: 0.95338300366\n",
      "Saved the pipeline search using: 30 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_2000features_1000binary_npca30.pkl\n",
      "Completed pipeline search using: 30 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 40 components.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d03226097dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Fit and transform the X_train_balanced_sample set to get the features using new pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0muserclick_pipeline_pcasearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_balanced\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_balanced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Completed pipeline fit using \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\" components, it took: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" minutes.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m             delayed(_fit_transform_one)(trans, weight, X, y,\n\u001b[1;32m    738\u001b[0m                                         **fit_params)\n\u001b[0;32m--> 739\u001b[0;31m             for name, trans, weight in self._iter())\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \"\"\"\n\u001b[1;32m    280\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    117\u001b[0m                           DeprecationWarning)\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkw_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, y, func, kw_args)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         return func(X, *((y,) if pass_y else ()),\n\u001b[0;32m--> 161\u001b[0;31m                     **(kw_args if kw_args else {}))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-34d4b16c97e7>\u001b[0m in \u001b[0;36mcolumn_time_processer\u001b[0;34m(X_train)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Then apply this function to each ip group and extract the total click time per ip group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     log_time_frame = pd.DataFrame(X_train.groupby([\"ip\"]).apply(get_log_total_click_time),\n\u001b[0m\u001b[1;32m     79\u001b[0m                                   columns=[\"log_total_click_time\"]).reset_index()\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode.chained_assignment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n\u001b[0;32m--> 720\u001b[0;31m                                                    self.axis)\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m   1711\u001b[0m                 hasattr(splitter, 'fast_apply') and axis == 0):\n\u001b[1;32m   1712\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1713\u001b[0;31m                 \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1714\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidApply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36mfast_apply\u001b[0;34m(self, f, names)\u001b[0m\n\u001b[1;32m   4381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4382\u001b[0m         \u001b[0msdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sorted_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4383\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_frame_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/reduce.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.apply_frame_axis0 (pandas/_libs/lib.c:41912)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-34d4b16c97e7>\u001b[0m in \u001b[0;36mget_log_total_click_time\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# First define a function to process selected ip group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_log_total_click_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2036\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2038\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2039\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest # We will use default scoring function to select features for classification\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module since it was saved into working directory as SparseInteractions.py\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "disk_directory = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "component_list = [30,40,50,100] # This time it refers to PCs to be added\n",
    "\n",
    "for n_components in component_list:\n",
    "    print(\"Starting pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    \n",
    "    # Generic pipeline to cycle for each component search\n",
    "    userclick_pipeline_pcasearch = Pipeline([\n",
    "\n",
    "        (\"union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [\n",
    "\n",
    "                (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                    (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                    (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                ])), # End of: numeric_subpipeline\n",
    "\n",
    "                (\"text_subpipeline\",Pipeline([\n",
    "                    (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                    (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                 ngram_range = (1,1),\n",
    "                                                 non_negative=True, \n",
    "                                                 norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                    ('dim_red', SelectKBest(k = 1000)) # Step3: use dimension reduction to select n_components number of best features using chi2 as scoring function\n",
    "                ]))\n",
    "            ]\n",
    "\n",
    "        )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "        # Common steps:\n",
    "        (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "        (\"scaler\",StandardScaler(with_mean=False)), # Standardize the features for a more gaussian distribution. \n",
    "        \n",
    "        # A new feature union\n",
    "        (\"final_union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [ (\"dim_red_feature\", SelectKBest(k = 2000)),\n",
    "                                 (\"dim_red_pca\", TruncatedSVD(n_components = n_components)) ]\n",
    "        ))   \n",
    "             \n",
    "    ])# End of: userclick_pipeline_pcasearch\n",
    "\n",
    "    # Fit and transform the X_train_balanced_sample set to get the features using new pipeline\n",
    "    start = datetime.datetime.now()\n",
    "    userclick_pipeline_pcasearch.fit(X_train_balanced,y_train_balanced)\n",
    "    print(\"Completed pipeline fit using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    X_train_balanced_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_train_balanced)\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed pipeline transform using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    print(\"The shape of the transformed set is: \" + str(X_train_balanced_trans_pl_pcasearch.shape))\n",
    "    # Train the classifier and get estimates\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_balanced_trans_pl_pcasearch,y_train_balanced)\n",
    "\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed model fit and it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "    probs = clf.predict_proba(X_train_balanced_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the training set: \" + str(roc_auc_score(y_train_balanced,probs)))\n",
    "\n",
    "    # Transform the validation set\n",
    "    start = datetime.datetime.now()\n",
    "    X_val1_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_val1)\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed validation set transformation, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    print(\"The shape of the transformed validation set is: \" + str(X_val1_trans_pl_pcasearch.shape))\n",
    "    probs = clf.predict_proba(X_val1_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the validation set 1: \" + str(roc_auc_score(y_val1,probs)))\n",
    "    \n",
    "    # Save the current pipeline\n",
    "    filename = disk_directory + \"userclick_pipeline_2000features_1000binary_npca\" + str(n_components) + \".pkl\"\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(userclick_pipeline_pcasearch,f)\n",
    "    print(\"Saved the pipeline search using: \"+ str(n_components)+ \" components \" + \"as :\" + filename)\n",
    "    \n",
    "    print(\"Completed pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    print(\"--\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a little helpful. How about we change the n-gram range to 2 and 3? Even though this is not a real text processing, I am interested in checking the impact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline search using: 2 components.\n",
      "Completed pipeline fit using 2 components, it took: 8.783333333333333 minutes.\n",
      "Completed pipeline transform using 2 components, it took: 8.983333333333333 minutes.\n",
      "The shape of the transformed set is: (835222, 2020)\n",
      "Completed model fit and it took: 3.35 minutes.\n",
      "ROC score in the training set: 0.968488349059\n",
      "Completed validation set transformation, it took: 9.35 minutes.\n",
      "The shape of the transformed validation set is: (1000000, 2020)\n",
      "ROC score in the validation set 1: 0.950154974333\n",
      "Saved the pipeline search using: 2 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_2000features_1000binary_20pca_ngrams2.pkl\n",
      "Completed pipeline search using: 2 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 3 components.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c63c42c74b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Fit and transform the X_train_balanced_sample set to get the features using new pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0muserclick_pipeline_pcasearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_balanced\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_balanced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Completed pipeline fit using \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\" components, it took: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" minutes.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m             delayed(_fit_transform_one)(trans, weight, X, y,\n\u001b[1;32m    738\u001b[0m                                         **fit_params)\n\u001b[0;32m--> 739\u001b[0;31m             for name, trans, weight in self._iter())\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \"\"\"\n\u001b[1;32m    280\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    117\u001b[0m                           DeprecationWarning)\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkw_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, y, func, kw_args)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         return func(X, *((y,) if pass_y else ()),\n\u001b[0;32m--> 161\u001b[0;31m                     **(kw_args if kw_args else {}))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-34d4b16c97e7>\u001b[0m in \u001b[0;36mcolumn_text_processer_nolambda\u001b[0;34m(df, text_columns)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# text_vector = text_data.apply(lambda x: \" \".join(x), axis = 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mtext_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrows\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mtext_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mtext_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 data = _sanitize_array(data, index, dtype, copy,\n\u001b[0;32m--> 248\u001b[0;31m                                        raise_cast_failure=True)\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m   2950\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2951\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2952\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_try_cast\u001b[0;34m(arr, take_fast_path)\u001b[0m\n\u001b[1;32m   2913\u001b[0m         \u001b[0;31m# perf shortcut as this is the most common case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtake_fast_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2915\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mmaybe_castable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2916\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_castable\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mmaybe_castable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;31m# return False to force a non-fastpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest # We will use default scoring function to select features for classification\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module since it was saved into working directory as SparseInteractions.py\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "disk_directory = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "component_list = [2,3] # This time it refers to n_gram range\n",
    "\n",
    "for n_components in component_list:\n",
    "    print(\"Starting pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    \n",
    "    # Generic pipeline to cycle for each component search\n",
    "    userclick_pipeline_pcasearch = Pipeline([\n",
    "\n",
    "        (\"union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [\n",
    "\n",
    "                (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                    (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                    (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                ])), # End of: numeric_subpipeline\n",
    "\n",
    "                (\"text_subpipeline\",Pipeline([\n",
    "                    (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                    (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                 ngram_range = (1,n_components),\n",
    "                                                 non_negative=True, \n",
    "                                                 norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                    ('dim_red', SelectKBest(k = 1000)) # Step3: use dimension reduction to select n_components number of best features using chi2 as scoring function\n",
    "                ]))\n",
    "            ]\n",
    "\n",
    "        )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "        # Common steps:\n",
    "        (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "        (\"scaler\",StandardScaler(with_mean=False)), # Standardize the features for a more gaussian distribution. \n",
    "        \n",
    "        # A new feature union\n",
    "        (\"final_union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [ (\"dim_red_feature\", SelectKBest(k = 2000)),\n",
    "                                 (\"dim_red_pca\", TruncatedSVD(n_components = 20)) ]\n",
    "        ))   \n",
    "             \n",
    "    ])# End of: userclick_pipeline_pcasearch\n",
    "\n",
    "    # Fit and transform the X_train_balanced_sample set to get the features using new pipeline\n",
    "    start = datetime.datetime.now()\n",
    "    userclick_pipeline_pcasearch.fit(X_train_balanced,y_train_balanced)\n",
    "    print(\"Completed pipeline fit using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    X_train_balanced_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_train_balanced)\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed pipeline transform using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    print(\"The shape of the transformed set is: \" + str(X_train_balanced_trans_pl_pcasearch.shape))\n",
    "    # Train the classifier and get estimates\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_balanced_trans_pl_pcasearch,y_train_balanced)\n",
    "\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed model fit and it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "    probs = clf.predict_proba(X_train_balanced_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the training set: \" + str(roc_auc_score(y_train_balanced,probs)))\n",
    "\n",
    "    # Transform the validation set\n",
    "    start = datetime.datetime.now()\n",
    "    X_val1_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_val1)\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed validation set transformation, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    print(\"The shape of the transformed validation set is: \" + str(X_val1_trans_pl_pcasearch.shape))\n",
    "    probs = clf.predict_proba(X_val1_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the validation set 1: \" + str(roc_auc_score(y_val1,probs)))\n",
    "    \n",
    "    # Save the current pipeline\n",
    "    filename = disk_directory + \"userclick_pipeline_2000features_1000binary_20pca_ngrams\" + str(n_components) + \".pkl\"\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(userclick_pipeline_pcasearch,f)\n",
    "    print(\"Saved the pipeline search using: \"+ str(n_components)+ \" components \" + \"as :\" + filename)\n",
    "    \n",
    "    print(\"Completed pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    print(\"--\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not sensible. We will lock down the **userclick_pipeline_pca20_2000features_nbinary1000** pipeline and continue using it to train new classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and transform the training and validation sets\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Read the pickled balanced training set\n",
    "X_train_balanced = pd.read_pickle(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_train_balanced.pkl\")\n",
    "y_train_balanced = pd.read_pickle(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/y_train_balanced.pkl\")\n",
    "\n",
    "# Load the validation sets\n",
    "X_val1 = pd.read_pickle(\"X_val1.pkl\")\n",
    "y_val1 = pd.read_pickle(\"y_val1.pkl\")\n",
    "\n",
    "# Load the validation sets\n",
    "X_val2 = pd.read_pickle(\"X_val2.pkl\")\n",
    "y_val2 = pd.read_pickle(\"y_val2.pkl\")\n",
    "\n",
    "# Load the established pipeline\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca20_2000features_nbinary1000.pkl\",\"rb\") as f:\n",
    "    userclick_pipeline_pca = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed training set.\n",
      "Transformed validation set 1.\n",
      "Transformed validation set 2.\n"
     ]
    }
   ],
   "source": [
    "X_train_balanced_trans_pca = userclick_pipeline_pca.transform(X_train_balanced)\n",
    "print(\"Transformed training set.\")\n",
    "X_val1_trans_pca = userclick_pipeline_pca.transform(X_val1)\n",
    "print(\"Transformed validation set 1.\")\n",
    "X_val2_trans_pca = userclick_pipeline_pca.transform(X_val2)\n",
    "print(\"Transformed validation set 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed data sets\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_train_balanced_trans_pca.pkl\",\"wb\") as f:\n",
    "    pickle.dump(X_train_balanced_trans_pca,f)\n",
    "    \n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_val1_trans_pca.pkl\",\"wb\") as f:\n",
    "    pickle.dump(X_val1_trans_pca,f)\n",
    "    \n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_val2_trans_pca.pkl\",\"wb\") as f:\n",
    "    pickle.dump(X_val2_trans_pca,f)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training classifiers using the decomposed PCA pipeline\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "### Untuned classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the transformed data sets\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_train_balanced_trans_pca.pkl\",\"rb\") as f:\n",
    "    X_train_balanced_trans_pca = pickle.load(f)\n",
    "y_train_balanced = pd.read_pickle(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/y_train_balanced.pkl\")    \n",
    "\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_val1_trans_pca.pkl\",\"rb\") as f:\n",
    "    X_val1_trans_pca = pickle.load(f)\n",
    "y_val1 = pd.read_pickle(\"y_val1.pkl\")\n",
    "\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_val2_trans_pca.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pca = pickle.load(f)\n",
    "y_val2 = pd.read_pickle(\"y_val2.pkl\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed model fit and it took: 4.166666666666667 minutes.\n",
      "ROC score in the training set: 0.969772394341\n",
      "ROC score in the validation set 1: 0.953305941476\n",
      "ROC score in the validation set 2: 0.951113101673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "clf =  LogisticRegression(C = 1)\n",
    "\n",
    "clf.fit(X_train_balanced_trans_pca,y_train_balanced)\n",
    "\n",
    "process_time = datetime.datetime.now() - start\n",
    "print(\"Completed model fit and it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "probs = clf.predict_proba(X_train_balanced_trans_pca)[:,1]\n",
    "print(\"ROC score in the training set: \" + str(roc_auc_score(y_train_balanced,probs))) \n",
    "\n",
    "probs = clf.predict_proba(X_val1_trans_pca)[:,1]\n",
    "print(\"ROC score in the validation set 1: \" + str(roc_auc_score(y_val1,probs)))\n",
    "\n",
    "probs = clf.predict_proba(X_val2_trans_pca)[:,1]\n",
    "print(\"ROC score in the validation set 2: \" + str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "### Untuned classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the transformed data sets\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_train_balanced_trans_pca.pkl\",\"rb\") as f:\n",
    "    X_train_balanced_trans_pca = pickle.load(f)\n",
    "y_train_balanced = pd.read_pickle(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/y_train_balanced.pkl\")    \n",
    "\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_val1_trans_pca.pkl\",\"rb\") as f:\n",
    "    X_val1_trans_pca = pickle.load(f)\n",
    "y_val1 = pd.read_pickle(\"y_val1.pkl\")\n",
    "\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_val2_trans_pca.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pca = pickle.load(f)\n",
    "y_val2 = pd.read_pickle(\"y_val2.pkl\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear]"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "# Note that we can't get probabilities directly from this LinearSVC function\n",
    "# We need to wrap into Calibrated Classifier \n",
    "# (see: https://stackoverflow.com/questions/35212213/sklearn-how-to-get-decision-probabilities-for-linearsvc-classifier)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lsvcbal = LinearSVC(verbose=10)\n",
    "\n",
    "clf =  CalibratedClassifierCV(base_estimator = lsvcbal,\n",
    "                                  cv = 3, # Also performs cross-validation\n",
    "                                  method= \"sigmoid\") # We use sigmoid function to get probabilities\n",
    "\n",
    "clf.fit(X_train_balanced_trans_pca,y_train_balanced)\n",
    "\n",
    "process_time = datetime.datetime.now() - start\n",
    "print(\"Completed model fit and it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "probs = clf.predict_proba(X_train_balanced_trans_pca)[:,1]\n",
    "print(\"ROC score in the training set: \" + str(roc_auc_score(y_train_balanced,probs))) \n",
    "\n",
    "probs = clf.predict_proba(X_val1_trans_pca)[:,1]\n",
    "print(\"ROC score in the validation set 1: \" + str(roc_auc_score(y_val1,probs)))\n",
    "\n",
    "probs = clf.predict_proba(X_val2_trans_pca)[:,1]\n",
    "print(\"ROC score in the validation set 2: \" + str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadric Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the test set using the new pca pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the established pipeline\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca20_2000features_nbinary1000.pkl\",\"rb\") as f:\n",
    "    userclick_pipeline_pca = pickle.load(f)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "filename = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test.csv\"\n",
    "\n",
    "test_set = pd.read_csv(filename, dtype= \"str\", nrows= 1000000)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"Finished reading first chunk \" + \"so far it took : \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "\n",
    "\n",
    "test_proc_pca = userclick_pipeline_pca.transform(test_set)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"Finished processing first chunk \" + \"so far it took : \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "\n",
    "print(\"Shape of the tensor is : \" + str(test_proc_pca.shape))\n",
    "\n",
    "column_names = test_set.columns\n",
    "\n",
    "skip = (10**6)+1\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"Added chunk : \" + \"1 \" + \"so far it took : \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "\n",
    "for i in range(2,20): \n",
    "    test_set = pd.read_csv(filename, dtype= \"str\",skiprows= skip, nrows= 1000000,names = list(column_names)) \n",
    "    temp_stack = userclick_pipeline_pca.transform(test_set)\n",
    "    test_proc_p12 = vstack([test_proc_pca,temp_stack]) \n",
    "    skip = skip + (10**6)\n",
    "    end = datetime.datetime.now()\n",
    "    process_time = end - start\n",
    "    print(\"Added chunk: \" + str(i) + \" so far it took : \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "    print(\"Shape of the tensor is : \" + str(test_proc_pca.shape))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "\n",
    "# Save a sparse matrix to a file using .npz format\n",
    "# See: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.save_npz.html\n",
    "import scipy.sparse as sp\n",
    "sp.save_npz(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pca.npz\",\n",
    "            test_proc_pca)\n",
    "\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
