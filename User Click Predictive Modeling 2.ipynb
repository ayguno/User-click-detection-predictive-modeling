{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a PCAsearch to find ideal number of components to keep in the pipeline\n",
    "\n",
    "We will use the same pipeline but grid search the number of components. We noticed that the entire balanced training set does not fit into memory, therefore we will need to use a random sample from the training set. for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Read the pickled balanced training set\n",
    "X_train_balanced = pd.read_pickle(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_train_balanced.pkl\")\n",
    "y_train_balanced = pd.read_pickle(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/y_train_balanced.pkl\")\n",
    "\n",
    "#let's try to get a small random sample (~10%) from the class-balanced set to train the pipeline\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(112)\n",
    "rows = random.sample(list(range(0,X_train_balanced.shape[0])),80000)\n",
    "\n",
    "X_train_balanced_sample = X_train_balanced.iloc[rows,]\n",
    "y_train_balanced_sample = y_train_balanced.iloc[rows]\n",
    "\n",
    "\n",
    "# Label text features\n",
    "Text_features = [\"app\",\"device\",\"os\",\"channel\"]\n",
    "\n",
    "##############################################################\n",
    "# Define utility function to parse and process text features\n",
    "##############################################################\n",
    "# Note we avoid lambda functions since they don't pickle when we want to save the pipeline later   \n",
    "def column_text_processer_nolambda(df,text_columns = Text_features):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \"\"\"\"A function that will merge/join all text in a given row to make it ready for tokenization. \n",
    "    - This function should take care of converting missing values to empty strings. \n",
    "    - It should also convert the text to lowercase.\n",
    "    df= pandas dataframe\n",
    "    text_columns = names of the text features in df\n",
    "    \"\"\" \n",
    "    # Select only non-text columns that are in the df\n",
    "    text_data = df[text_columns]\n",
    "    \n",
    "    # Fill the missing values in text_data using empty strings\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    \n",
    "    # Concatenate feature name to each category encoding for each row\n",
    "    # E.g: encoding 3 at device column will read as device3 to make each encoding unique for a given feature\n",
    "    for col_index in list(text_data.columns):\n",
    "        text_data[col_index] = col_index + text_data[col_index].astype(str)\n",
    "    \n",
    "    # Join all the strings in a given row to make a vector\n",
    "    # text_vector = text_data.apply(lambda x: \" \".join(x), axis = 1)\n",
    "    text_vector = []\n",
    "    for index,rows in text_data.iterrows():\n",
    "        text_item = \" \".join(rows).lower()\n",
    "        text_vector.append(text_item)\n",
    "\n",
    "    # return text_vector as pd.Series object to enter the tokenization pipeline\n",
    "    return pd.Series(text_vector)\n",
    "\n",
    "#######################################################################\n",
    "# Define custom processing functions to add the log_total_clicks and \n",
    "# log_total_click_time features, and remove the unwanted base features\n",
    "#######################################################################\n",
    "def column_time_processer(X_train):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert click_time to datetime64 dtype \n",
    "    X_train.click_time = pd.to_datetime(X_train.click_time)\n",
    "\n",
    "    # Calculate the log_total_clicks for each ip and add as a new feature to temp_data\n",
    "    temp_data = pd.DataFrame(np.log(X_train.groupby([\"ip\"]).size()),\n",
    "                                    columns = [\"log_total_clicks\"]).reset_index()\n",
    "\n",
    "\n",
    "    # Calculate the log_total_click_time for each ip and add as a new feature to temp_data\n",
    "    # First define a function to process selected ip group \n",
    "    def get_log_total_click_time(group):\n",
    "        diff = (max(group.click_time) - min(group.click_time)).seconds\n",
    "        return np.log(diff+1)\n",
    "\n",
    "    # Then apply this function to each ip group and extract the total click time per ip group\n",
    "    log_time_frame = pd.DataFrame(X_train.groupby([\"ip\"]).apply(get_log_total_click_time),\n",
    "                                  columns=[\"log_total_click_time\"]).reset_index()\n",
    "\n",
    "    # Then add this new feature to the temp_data\n",
    "    temp_data = pd.merge(temp_data,log_time_frame, how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Combine temp_data with X_train to maintain X_train key order\n",
    "    temp_data = pd.merge(X_train,temp_data,how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Drop features that are not needed\n",
    "    temp_data = temp_data[[\"log_total_clicks\",\"log_total_click_time\"]]\n",
    "\n",
    "    # Return only the numeric features as a tensor to integrate into the numeric feature branch of the pipeline\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# We need to wrap these custom utility functions using FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# FunctionTransformer wrapper of utility functions to parse text and numeric features\n",
    "# Note how we avoid putting any arguments into column_text_processer or column_time_processer\n",
    "#############################################################################\n",
    "get_numeric_data = FunctionTransformer(func = column_time_processer, validate=False) \n",
    "get_text_data = FunctionTransformer(func = column_text_processer_nolambda,validate=False) \n",
    "\n",
    "#############################################################################\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "# #Note this regex will match either a whitespace or a punctuation to tokenize \n",
    "# the string vector on these preferences, in our case we only have white spaces in our text  \n",
    "#############################################################################\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "\n",
    "# Load a validation set to use in the new pca pipeline\n",
    "X_val1 = pd.read_pickle(\"X_val1.pkl\")\n",
    "y_val1 = pd.read_pickle(\"y_val1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our main search loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline search using: 10 components.\n",
      "Completed pipeline fit and transform using 10 components, it took: 2.25 minutes.\n",
      "Completed model fit and it took: 0.0 minutes.\n",
      "ROC score in the training set: 0.865398828872\n",
      "Completed validation set transformation, it took: 0.0 minutes.\n",
      "ROC score in the validation set 1: 0.880240252656\n",
      "Saved the pipeline search using: 10 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca10.pkl\n",
      "Completed pipeline search using: 10 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 600 components.\n",
      "Completed pipeline fit and transform using 600 components, it took: 3.533333333333333 minutes.\n",
      "Completed model fit and it took: 5.316666666666666 minutes.\n",
      "ROC score in the training set: 0.960074287179\n",
      "Completed validation set transformation, it took: 5.316666666666666 minutes.\n",
      "ROC score in the validation set 1: 0.931542083596\n",
      "Saved the pipeline search using: 600 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_pca600.pkl\n",
      "Completed pipeline search using: 600 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 700 components.\n",
      "Completed pipeline fit and transform using 700 components, it took: 4.3 minutes.\n",
      "Completed model fit and it took: 6.666666666666667 minutes.\n",
      "ROC score in the training set: 0.9605818804\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2 # We will use chi-squared as a scoring function to select features for classification\n",
    "from sklearn.metrics import auc\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module since it was saved into working directory as SparseInteractions.py\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "disk_directory = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "component_list = [10,600,700,800]\n",
    "\n",
    "for n_components in component_list:\n",
    "    print(\"Starting pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    \n",
    "    # Generic pipeline to cycle for each component search\n",
    "    userclick_pipeline_pcasearch = Pipeline([\n",
    "\n",
    "        (\"union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [\n",
    "\n",
    "                (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                    (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                    (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                ])), # End of: numeric_subpipeline\n",
    "\n",
    "                (\"text_subpipeline\",Pipeline([\n",
    "                    (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                    (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                 ngram_range = (1,1),\n",
    "                                                 non_negative=True, \n",
    "                                                 norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                    ('dim_red', SelectKBest(chi2,300)) # Step3: use dimension reduction to select 300 best features using chi2 as scoring function\n",
    "                ]))\n",
    "            ]\n",
    "\n",
    "        )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "        # Common steps:\n",
    "\n",
    "        (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "        (\"scaler\",StandardScaler(with_mean=False)), # Standardize the features for a more gaussian distribution. \n",
    "        (\"dim_red\", TruncatedSVD(n_components=n_components))      \n",
    "    ])# End of: userclick_pipeline_pcasearch\n",
    "\n",
    "    # Fit and transform the X_train_balanced_sample set to get the features using new pipeline\n",
    "    start = datetime.datetime.now()\n",
    "    X_train_balanced_sample_trans_pl_pcasearch = userclick_pipeline_pcasearch.fit(X_train_balanced_sample,y_train_balanced_sample).transform(X_train_balanced_sample)\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed pipeline fit and transform using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    # Train the classifier and get estimates\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_balanced_sample_trans_pl_pcasearch,y_train_balanced_sample)\n",
    "\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed model fit and it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "    probs = clf.predict_proba(X_train_balanced_sample_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the training set: \" + str(roc_auc_score(y_train_balanced_sample,probs)))\n",
    "\n",
    "    # Transform the validation set\n",
    "    start = datetime.datetime.now()\n",
    "    X_val1_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_val1)\n",
    "    print(\"Completed validation set transformation, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    probs = clf.predict_proba(X_val1_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the validation set 1: \" + str(roc_auc_score(y_val1,probs)))\n",
    "    \n",
    "    # Save the current pipeline\n",
    "    filename = disk_directory + \"userclick_pipeline_pca\" + str(n_components) + \".pkl\"\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(userclick_pipeline_pcasearch,f)\n",
    "    print(\"Saved the pipeline search using: \"+ str(n_components)+ \" components \" + \"as :\" + filename)\n",
    "    \n",
    "    print(\"Completed pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    print(\"--\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the 700 components, kernel breaks down, therefore this is not feasible to search components in this way. Let's try to modify the pipeline for different strategies in feature selection and dimension reduction.\n",
    "\n",
    "How about simple feature selection at the end of the pipeline instead of PCA, could it be faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline search using: 10 components.\n",
      "Completed pipeline fit using 10 components, it took: 0.0 minutes.\n",
      "Completed pipeline transform using 10 components, it took: 0.8666666666666667 minutes.\n",
      "Completed model fit and it took: 0.0 minutes.\n",
      "ROC score in the training set: 0.811026316688\n",
      "Completed validation set transformation, it took: 0.0 minutes.\n",
      "ROC score in the validation set 1: 0.558237052807\n",
      "Saved the pipeline search using: 10 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_nfeatures10.pkl\n",
      "Completed pipeline search using: 10 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 100 components.\n",
      "Completed pipeline fit using 100 components, it took: 0.0 minutes.\n",
      "Completed pipeline transform using 100 components, it took: 0.7666666666666667 minutes.\n",
      "Completed model fit and it took: 0.016666666666666666 minutes.\n",
      "ROC score in the training set: 0.944834862071\n",
      "Completed validation set transformation, it took: 0.016666666666666666 minutes.\n",
      "ROC score in the validation set 1: 0.888466684363\n",
      "Saved the pipeline search using: 100 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_nfeatures100.pkl\n",
      "Completed pipeline search using: 100 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 600 components.\n",
      "Completed pipeline fit using 600 components, it took: 0.016666666666666666 minutes.\n",
      "Completed pipeline transform using 600 components, it took: 0.8166666666666667 minutes.\n",
      "Completed model fit and it took: 0.06666666666666667 minutes.\n",
      "ROC score in the training set: 0.962187273708\n",
      "Completed validation set transformation, it took: 0.06666666666666667 minutes.\n",
      "ROC score in the validation set 1: 0.922357116847\n",
      "Saved the pipeline search using: 600 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_nfeatures600.pkl\n",
      "Completed pipeline search using: 600 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 800 components.\n",
      "Completed pipeline fit using 800 components, it took: 0.06666666666666667 minutes.\n",
      "Completed pipeline transform using 800 components, it took: 0.7833333333333333 minutes.\n",
      "Completed model fit and it took: 0.08333333333333333 minutes.\n",
      "ROC score in the training set: 0.963342697526\n",
      "Completed validation set transformation, it took: 0.08333333333333333 minutes.\n",
      "ROC score in the validation set 1: 0.922793092699\n",
      "Saved the pipeline search using: 800 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_nfeatures800.pkl\n",
      "Completed pipeline search using: 800 components.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting pipeline search using: 1000 components.\n",
      "Completed pipeline fit using 1000 components, it took: 0.08333333333333333 minutes.\n",
      "Completed pipeline transform using 1000 components, it took: 0.7833333333333333 minutes.\n",
      "Completed model fit and it took: 0.08333333333333333 minutes.\n",
      "ROC score in the training set: 0.964220175802\n",
      "Completed validation set transformation, it took: 0.08333333333333333 minutes.\n",
      "ROC score in the validation set 1: 0.924431806602\n",
      "Saved the pipeline search using: 1000 components as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/userclick_pipeline_nfeatures1000.pkl\n",
      "Completed pipeline search using: 1000 components.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest # We will use default scoring function to select features for classification\n",
    "from sklearn.metrics import auc\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module since it was saved into working directory as SparseInteractions.py\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "disk_directory = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "component_list = [10,100,600,800,1000]\n",
    "\n",
    "for n_components in component_list:\n",
    "    print(\"Starting pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    \n",
    "    # Generic pipeline to cycle for each component search\n",
    "    userclick_pipeline_pcasearch = Pipeline([\n",
    "\n",
    "        (\"union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [\n",
    "\n",
    "                (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                    (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                    (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                ])), # End of: numeric_subpipeline\n",
    "\n",
    "                (\"text_subpipeline\",Pipeline([\n",
    "                    (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                    (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                 ngram_range = (1,1),\n",
    "                                                 non_negative=True, \n",
    "                                                 norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                    ('dim_red', SelectKBest(k = 300)) # Step3: use dimension reduction to select 300 best features using chi2 as scoring function\n",
    "                ]))\n",
    "            ]\n",
    "\n",
    "        )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "        # Common steps:\n",
    "        (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "        (\"scaler\",StandardScaler(with_mean=False)), # Standardize the features for a more gaussian distribution. \n",
    "        (\"dim_red2\", SelectKBest(k = n_components))      \n",
    "    ])# End of: userclick_pipeline_pcasearch\n",
    "\n",
    "    # Fit and transform the X_train_balanced_sample set to get the features using new pipeline\n",
    "    start = datetime.datetime.now()\n",
    "    userclick_pipeline_pcasearch.fit(X_train_balanced_sample,y_train_balanced_sample)\n",
    "    print(\"Completed pipeline fit using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    X_train_balanced_sample_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_train_balanced_sample)\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed pipeline transform using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    # Train the classifier and get estimates\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_balanced_sample_trans_pl_pcasearch,y_train_balanced_sample)\n",
    "\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed model fit and it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "    probs = clf.predict_proba(X_train_balanced_sample_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the training set: \" + str(roc_auc_score(y_train_balanced_sample,probs)))\n",
    "\n",
    "    # Transform the validation set\n",
    "    start = datetime.datetime.now()\n",
    "    X_val1_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_val1)\n",
    "    print(\"Completed validation set transformation, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    probs = clf.predict_proba(X_val1_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the validation set 1: \" + str(roc_auc_score(y_val1,probs)))\n",
    "    \n",
    "    # Save the current pipeline\n",
    "    filename = disk_directory + \"userclick_pipeline_nfeatures\" + str(n_components) + \".pkl\"\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(userclick_pipeline_pcasearch,f)\n",
    "    print(\"Saved the pipeline search using: \"+ str(n_components)+ \" components \" + \"as :\" + filename)\n",
    "    \n",
    "    print(\"Completed pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    print(\"--\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of selection is indeed faster and also fits into memory. However, out-of-the-box performance of the selected features are not as good as the PCA features. How about we perform a feature union of first 20 Principal components and 1000 - 2000 best features selected before decomposition? Beside, let's try to use the entire training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline search using: 20 components.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest # We will use default scoring function to select features for classification\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module since it was saved into working directory as SparseInteractions.py\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "disk_directory = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "component_list = [20,50,500,1000,1200,1500,2000]\n",
    "\n",
    "for n_components in component_list:\n",
    "    print(\"Starting pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    \n",
    "    # Generic pipeline to cycle for each component search\n",
    "    userclick_pipeline_pcasearch = Pipeline([\n",
    "\n",
    "        (\"union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [\n",
    "\n",
    "                (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                    (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                    (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                ])), # End of: numeric_subpipeline\n",
    "\n",
    "                (\"text_subpipeline\",Pipeline([\n",
    "                    (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                    (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                 ngram_range = (1,1),\n",
    "                                                 non_negative=True, \n",
    "                                                 norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                    ('dim_red', SelectKBest(k = 300)) # Step3: use dimension reduction to select 300 best features using chi2 as scoring function\n",
    "                ]))\n",
    "            ]\n",
    "\n",
    "        )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "        # Common steps:\n",
    "        (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "        (\"scaler\",StandardScaler(with_mean=False)), # Standardize the features for a more gaussian distribution. \n",
    "        \n",
    "        # A new feature union\n",
    "        (\"final_union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [ (\"dim_red_feature\", SelectKBest(k = n_components)),\n",
    "                                 (\"dim_red_pca\", TruncatedSVD(n_components = 20)) ]\n",
    "        ))   \n",
    "             \n",
    "    ])# End of: userclick_pipeline_pcasearch\n",
    "\n",
    "    # Fit and transform the X_train_balanced_sample set to get the features using new pipeline\n",
    "    start = datetime.datetime.now()\n",
    "    userclick_pipeline_pcasearch.fit(X_train_balanced,y_train_balanced)\n",
    "    print(\"Completed pipeline fit using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    X_train_balanced_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_train_balanced)\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed pipeline transform using \"+ str(n_components)+ \" components, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    \n",
    "    print(\"The shape of the transformed set is: \" + str(X_train_balanced_trans_pl_pcasearch.shape))\n",
    "    # Train the classifier and get estimates\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_balanced_trans_pl_pcasearch,y_train_balanced)\n",
    "\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed model fit and it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "    probs = clf.predict_proba(X_train_balanced_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the training set: \" + str(roc_auc_score(y_train_balanced,probs)))\n",
    "\n",
    "    # Transform the validation set\n",
    "    start = datetime.datetime.now()\n",
    "    X_val1_trans_pl_pcasearch = userclick_pipeline_pcasearch.transform(X_val1)\n",
    "    process_time = datetime.datetime.now() - start\n",
    "    print(\"Completed validation set transformation, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "    print(\"The shape of the transformed validation set is: \" + str(X_val1_trans_pl_pcasearch.shape))\n",
    "    probs = clf.predict_proba(X_val1_trans_pl_pcasearch)[:,1]\n",
    "    print(\"ROC score in the validation set 1: \" + str(roc_auc_score(y_val1,probs)))\n",
    "    \n",
    "    # Save the current pipeline\n",
    "    filename = disk_directory + \"userclick_pipeline_pca20_nfeatures\" + str(n_components) + \".pkl\"\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(userclick_pipeline_pcasearch,f)\n",
    "    print(\"Saved the pipeline search using: \"+ str(n_components)+ \" components \" + \"as :\" + filename)\n",
    "    \n",
    "    print(\"Completed pipeline search using: \"+ str(n_components) + \" components.\")\n",
    "    print(\"--\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
