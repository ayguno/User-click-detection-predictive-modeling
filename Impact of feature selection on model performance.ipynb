{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring feature selection strategies to monitor model performance\n",
    "\n",
    "In this notebook we are going to try employing different feature selection strategies in an existing pipeline and try to understand its impact on performance of different models.\n",
    "\n",
    "# Streamline model fit and validation\n",
    "\n",
    "In order to compare the performance of different models, let's streamline the transformation, fit and prediction steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our training and validation features and target labels were prepared before:\n",
    "import pickle\n",
    "with open(\"X_val1.pkl\",\"rb\") as f:\n",
    "    X_train = pickle.load(f)   \n",
    "with open(\"y_val1.pkl\",\"rb\") as f:\n",
    "    y_train = pickle.load(f)\n",
    "        \n",
    "with open(\"X_val2.pkl\",\"rb\") as f:\n",
    "    X_val = pickle.load(f)       \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to streamline classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def calculate_validation(X_val_trans,y_val,clf):\n",
    "    probs = clf.predict_proba(X_val_trans)[:,1]\n",
    "    return roc_auc_score(y_val,probs)\n",
    "\n",
    "\n",
    "def streamline_classifiers(fpipeline,nfeatures,X_train,y_train,X_val,y_val):\n",
    "    \"\"\" We will monitor performance of different default classifiers.\"\"\"\n",
    "    print(\">>> streamline_classifiers: started exploring model performance using \" + str(nfeatures) + \" features.\")\n",
    "    # Transform the training and test set using the current pipeline\n",
    "    start = datetime.now()\n",
    "    X_train_trans = fpipeline.fit(X_train,y_train).transform(X_train)\n",
    "    process = datetime.now() - start\n",
    "    print(\" >>> streamline_classifiers: fitted and transformed X_train. It took: \" + str(process.seconds/60) + \" minutes.\")\n",
    "    print(\"Shape of X_train is \" + str(X_train_trans.shape))\n",
    "    start = datetime.now()\n",
    "    X_val_trans = fpipeline.transform(X_val)\n",
    "    process = datetime.now() - start\n",
    "    print(\" >>> streamline_classifiers: transformed X_val. It took: \" + str(process.seconds/60) + \" minutes.\")\n",
    "    print(\"Shape of X_val is \" + str(X_val_trans.shape))\n",
    "    #######################################################################\n",
    "    # Train different classifiers report and calculate the validation score\n",
    "    #######################################################################\n",
    "    # LogisticRegression\n",
    "    clf_dict = {\n",
    "        \"LogisticRegression\": LogisticRegression(),\n",
    "        \"MultinomialNB\":MultinomialNB(),\n",
    "        \"AdaBoostClassifier\":AdaBoostClassifier(),\n",
    "        \"QuadraticDiscriminantAnalysis\":QuadraticDiscriminantAnalysis()\n",
    "    }\n",
    "    \n",
    "    # Container to collect current scores\n",
    "    scores = pd.DataFrame()\n",
    "    scores[\"nfeatures\"] = pd.Series(nfeatures)\n",
    "    \n",
    "    for key,value in clf_dict.items():\n",
    "        start = datetime.now()\n",
    "        \n",
    "        clf = value\n",
    "        clf.fit(X_train_trans,y_train)\n",
    "        val_score =calculate_validation(X_val_trans,y_val,clf)\n",
    "        scores[key] = pd.Series(val_score)\n",
    "        \n",
    "        process = datetime.now() - start\n",
    "        print(\" >>> Completed \" +str(key) + \" classifier with an roc score of \"+str(val_score) + \" it took \" +str(process.seconds/60)+ \" minutes.\" )\n",
    "        print(\"*\" * 80)\n",
    "    # Return a data frame of validation roc scores for a given number of features\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label text features\n",
    "Text_features = [\"app\",\"device\",\"os\",\"channel\"]\n",
    "\n",
    "##############################################################\n",
    "# Define utility function to parse and process text features\n",
    "##############################################################\n",
    "# Note we avoid lambda functions since they don't pickle when we want to save the pipeline later   \n",
    "def column_text_processer_nolambda(df,text_columns = Text_features):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \"\"\"\"A function that will merge/join all text in a given row to make it ready for tokenization. \n",
    "    - This function should take care of converting missing values to empty strings. \n",
    "    - It should also convert the text to lowercase.\n",
    "    df= pandas dataframe\n",
    "    text_columns = names of the text features in df\n",
    "    \"\"\" \n",
    "    # Select only non-text columns that are in the df\n",
    "    text_data = df[text_columns]\n",
    "    \n",
    "    # Fill the missing values in text_data using empty strings\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    \n",
    "    # Concatenate feature name to each category encoding for each row\n",
    "    # E.g: encoding 3 at device column will read as device3 to make each encoding unique for a given feature\n",
    "    for col_index in list(text_data.columns):\n",
    "        text_data[col_index] = col_index + text_data[col_index].astype(str)\n",
    "    \n",
    "    # Join all the strings in a given row to make a vector\n",
    "    # text_vector = text_data.apply(lambda x: \" \".join(x), axis = 1)\n",
    "    text_vector = []\n",
    "    for index,rows in text_data.iterrows():\n",
    "        text_item = \" \".join(rows).lower()\n",
    "        text_vector.append(text_item)\n",
    "\n",
    "    # return text_vector as pd.Series object to enter the tokenization pipeline\n",
    "    return pd.Series(text_vector)\n",
    "\n",
    "#######################################################################\n",
    "# Define custom processing functions to add the log_total_clicks and \n",
    "# log_total_click_time features, and remove the unwanted base features\n",
    "#######################################################################\n",
    "def column_time_processer(X_train):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert click_time to datetime64 dtype \n",
    "    X_train.click_time = pd.to_datetime(X_train.click_time)\n",
    "\n",
    "    # Calculate the log_total_clicks for each ip and add as a new feature to temp_data\n",
    "    temp_data = pd.DataFrame(np.log(X_train.groupby([\"ip\"]).size()),\n",
    "                                    columns = [\"log_total_clicks\"]).reset_index()\n",
    "\n",
    "\n",
    "    # Calculate the log_total_click_time for each ip and add as a new feature to temp_data\n",
    "    # First define a function to process selected ip group \n",
    "    def get_log_total_click_time(group):\n",
    "        diff = (max(group.click_time) - min(group.click_time)).seconds\n",
    "        return np.log(diff+1)\n",
    "\n",
    "    # Then apply this function to each ip group and extract the total click time per ip group\n",
    "    log_time_frame = pd.DataFrame(X_train.groupby([\"ip\"]).apply(get_log_total_click_time),\n",
    "                                  columns=[\"log_total_click_time\"]).reset_index()\n",
    "\n",
    "    # Then add this new feature to the temp_data\n",
    "    temp_data = pd.merge(temp_data,log_time_frame, how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Combine temp_data with X_train to maintain X_train key order\n",
    "    temp_data = pd.merge(X_train,temp_data,how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Drop features that are not needed\n",
    "    temp_data = temp_data[[\"log_total_clicks\",\"log_total_click_time\"]]\n",
    "\n",
    "    # Return only the numeric features as a tensor to integrate into the numeric feature branch of the pipeline\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# We need to wrap these custom utility functions using FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# FunctionTransformer wrapper of utility functions to parse text and numeric features\n",
    "# Note how we avoid putting any arguments into column_text_processer or column_time_processer\n",
    "#############################################################################\n",
    "get_numeric_data = FunctionTransformer(func = column_time_processer, validate=False) \n",
    "get_text_data = FunctionTransformer(func = column_text_processer_nolambda,validate=False) \n",
    "\n",
    "#############################################################################\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "# #Note this regex will match either a whitespace or a punctuation to tokenize \n",
    "# the string vector on these preferences, in our case we only have white spaces in our text  \n",
    "#############################################################################\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> streamline_classifiers: started exploring model performance using SelectFromSVC features.\n",
      " >>> streamline_classifiers: fitted and transformed X_train. It took: 13.366666666666667 minutes.\n",
      "Shape of X_train is (1000000, 2464)\n",
      " >>> streamline_classifiers: transformed X_val. It took: 3.566666666666667 minutes.\n",
      "Shape of X_val is (1000000, 2464)\n",
      " >>> Completed LogisticRegression classifier with an roc score of 0.951678536123 it took 0.18333333333333332 minutes.\n",
      "********************\n",
      " >>> Completed MultinomialNB classifier with an roc score of 0.942329960157 it took 0.0 minutes.\n",
      "********************\n",
      " >>> Completed AdaBoostClassifier classifier with an roc score of 0.931726238424 it took 4.383333333333334 minutes.\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Construct our feature extraction selection pipeline\n",
    "######################################################\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler, Imputer\n",
    "from sklearn.feature_selection import SelectKBest,VarianceThreshold, chi2,SelectFromModel\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "nfeatures = \"SelectFromSVC\"\n",
    "\n",
    "featureselection_pipeline = Pipeline([\n",
    "        (\"union\",FeatureUnion(\n",
    "            # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "            # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "            transformer_list = [\n",
    "\n",
    "                (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                    (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                    (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                ])), # End of: numeric_subpipeline\n",
    "\n",
    "                (\"text_subpipeline\",Pipeline([\n",
    "                    (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                    (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                 ngram_range = (1,1),\n",
    "                                                 non_negative=True, \n",
    "                                                 norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                    ('dim_red', SelectKBest(k=nfeatures) # Step3: use dimension reduction to select best features \n",
    "                ]))\n",
    "            ]\n",
    "\n",
    "        )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "        # Common steps:\n",
    "\n",
    "        (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "        (\"scaler\",MaxAbsScaler()), # Scale the features between 0 and 1.       \n",
    "        ('dim_red2', SelectKBest(k=100))\n",
    "    ])# End of: featureselection_pipeline\n",
    "\n",
    "result = streamline_classifiers(fpipeline=featureselection_pipeline,\n",
    "                       nfeatures = nfeatures,X_train = X_train,y_train = y_train,\n",
    "                       X_val = X_val,y_val = y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nfeatures', 'LogisticRegression', 'MultinomialNB', 'AdaBoostClassifier']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['nfeatures', 'LogisticRegression', 'MultinomialNB', 'AdaBoostClassifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nfeatures</th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>MultinomialNB</th>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nfeatures, LogisticRegression, MultinomialNB, AdaBoostClassifier]\n",
       "Index: []"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Construct our feature extraction selection pipeline\n",
    "######################################################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectKBest,VarianceThreshold, chi2,SelectFromModel\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "nfeatures_list = [10,20,30,50]\n",
    "results = pd.DataFrame(columns=['nfeatures', 'LogisticRegression', 'MultinomialNB', 'AdaBoostClassifier'])\n",
    "\n",
    "for nfeatures in nfeatures_list:\n",
    "  \n",
    "    featureselection_pipeline = Pipeline([\n",
    "            (\"union\",FeatureUnion(\n",
    "                # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "                # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "                transformer_list = [\n",
    "\n",
    "                    (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                        (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                        (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                    ])), # End of: numeric_subpipeline\n",
    "\n",
    "                    (\"text_subpipeline\",Pipeline([\n",
    "                        (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                        (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                     ngram_range = (1,1),\n",
    "                                                     non_negative=True, \n",
    "                                                     norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                        ('dim_red', SelectKBest(k=nfeatures)) # Step3: use dimension reduction to select best features \n",
    "                    ]))\n",
    "                ]\n",
    "\n",
    "            )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "            # Common steps:\n",
    "\n",
    "            (\"int\", SparseInteractions(degree=3)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "            (\"scaler\",MaxAbsScaler()), # Scale the features between 0 and 1.       \n",
    "            ('dim_red2', SelectKBest(k = 100))\n",
    "        ])# End of: featureselection_pipeline\n",
    "\n",
    "    results = pd.concat([results, streamline_classifiers(fpipeline=featureselection_pipeline,\n",
    "                       nfeatures = nfeatures,X_train = X_train,y_train = y_train,\n",
    "                       X_val = X_val,y_val = y_val)], axis = 0)\n",
    "    print(\"--\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nfeatures</th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>MultinomialNB</th>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.888943</td>\n",
       "      <td>0.887109</td>\n",
       "      <td>0.872207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.893488</td>\n",
       "      <td>0.883378</td>\n",
       "      <td>0.893272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.893283</td>\n",
       "      <td>0.883365</td>\n",
       "      <td>0.889234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.893283</td>\n",
       "      <td>0.883365</td>\n",
       "      <td>0.889234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  nfeatures  LogisticRegression  MultinomialNB  AdaBoostClassifier\n",
       "0        10            0.888943       0.887109            0.872207\n",
       "0        20            0.893488       0.883378            0.893272\n",
       "0        30            0.893283       0.883365            0.889234\n",
       "0        50            0.893283       0.883365            0.889234"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> streamline_classifiers: started exploring model performance using 10 features.\n",
      " >>> streamline_classifiers: fitted and transformed X_train. It took: 7.733333333333333 minutes.\n",
      "Shape of X_train is (1000000, 10)\n",
      " >>> streamline_classifiers: transformed X_val. It took: 3.783333333333333 minutes.\n",
      "Shape of X_val is (1000000, 10)\n",
      " >>> Completed LogisticRegression classifier with an roc score of 0.741096318523 it took 0.03333333333333333 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed MultinomialNB classifier with an roc score of 0.477099483832 it took 0.0 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed AdaBoostClassifier classifier with an roc score of 0.741077284969 it took 0.3333333333333333 minutes.\n",
      "********************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">>> streamline_classifiers: started exploring model performance using 20 features.\n",
      " >>> streamline_classifiers: fitted and transformed X_train. It took: 7.683333333333334 minutes.\n",
      "Shape of X_train is (1000000, 20)\n",
      " >>> streamline_classifiers: transformed X_val. It took: 3.7666666666666666 minutes.\n",
      "Shape of X_val is (1000000, 20)\n",
      " >>> Completed LogisticRegression classifier with an roc score of 0.870281384669 it took 0.03333333333333333 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed MultinomialNB classifier with an roc score of 0.869969830511 it took 0.0 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed AdaBoostClassifier classifier with an roc score of 0.868690185486 it took 0.5833333333333334 minutes.\n",
      "********************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">>> streamline_classifiers: started exploring model performance using 50 features.\n",
      " >>> streamline_classifiers: fitted and transformed X_train. It took: 97.51666666666667 minutes.\n",
      "Shape of X_train is (1000000, 50)\n",
      " >>> streamline_classifiers: transformed X_val. It took: 5.166666666666667 minutes.\n",
      "Shape of X_val is (1000000, 50)\n",
      " >>> Completed LogisticRegression classifier with an roc score of 0.885521741062 it took 0.06666666666666667 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed MultinomialNB classifier with an roc score of 0.880457806333 it took 0.0 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed AdaBoostClassifier classifier with an roc score of 0.882335305188 it took 0.9 minutes.\n",
      "********************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">>> streamline_classifiers: started exploring model performance using 100 features.\n",
      " >>> streamline_classifiers: fitted and transformed X_train. It took: 7.85 minutes.\n",
      "Shape of X_train is (1000000, 100)\n",
      " >>> streamline_classifiers: transformed X_val. It took: 3.8 minutes.\n",
      "Shape of X_val is (1000000, 100)\n",
      " >>> Completed LogisticRegression classifier with an roc score of 0.892628466607 it took 0.1 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed MultinomialNB classifier with an roc score of 0.884410713743 it took 0.0 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed AdaBoostClassifier classifier with an roc score of 0.889321968441 it took 1.55 minutes.\n",
      "********************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">>> streamline_classifiers: started exploring model performance using 200 features.\n",
      " >>> streamline_classifiers: fitted and transformed X_train. It took: 7.683333333333334 minutes.\n",
      "Shape of X_train is (1000000, 200)\n",
      " >>> streamline_classifiers: transformed X_val. It took: 3.816666666666667 minutes.\n",
      "Shape of X_val is (1000000, 200)\n",
      " >>> Completed LogisticRegression classifier with an roc score of 0.899963645558 it took 0.1 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed MultinomialNB classifier with an roc score of 0.891626352266 it took 0.0 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed AdaBoostClassifier classifier with an roc score of 0.900374876103 it took 1.55 minutes.\n",
      "********************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">>> streamline_classifiers: started exploring model performance using 500 features.\n",
      " >>> streamline_classifiers: fitted and transformed X_train. It took: 7.833333333333333 minutes.\n",
      "Shape of X_train is (1000000, 500)\n",
      " >>> streamline_classifiers: transformed X_val. It took: 3.8 minutes.\n",
      "Shape of X_val is (1000000, 500)\n",
      " >>> Completed LogisticRegression classifier with an roc score of 0.9043380755 it took 0.1 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed MultinomialNB classifier with an roc score of 0.894316331765 it took 0.0 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed AdaBoostClassifier classifier with an roc score of 0.888937909477 it took 1.55 minutes.\n",
      "********************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">>> streamline_classifiers: started exploring model performance using 1000 features.\n",
      " >>> streamline_classifiers: fitted and transformed X_train. It took: 7.7 minutes.\n",
      "Shape of X_train is (1000000, 1000)\n",
      " >>> streamline_classifiers: transformed X_val. It took: 3.816666666666667 minutes.\n",
      "Shape of X_val is (1000000, 1000)\n",
      " >>> Completed LogisticRegression classifier with an roc score of 0.942774260587 it took 0.18333333333333332 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed MultinomialNB classifier with an roc score of 0.922505674334 it took 0.0 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed AdaBoostClassifier classifier with an roc score of 0.913993121764 it took 3.15 minutes.\n",
      "********************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">>> streamline_classifiers: started exploring model performance using 2000 features.\n",
      " >>> streamline_classifiers: fitted and transformed X_train. It took: 7.683333333333334 minutes.\n",
      "Shape of X_train is (1000000, 2000)\n",
      " >>> streamline_classifiers: transformed X_val. It took: 3.8 minutes.\n",
      "Shape of X_val is (1000000, 2000)\n",
      " >>> Completed LogisticRegression classifier with an roc score of 0.951607556943 it took 0.18333333333333332 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed MultinomialNB classifier with an roc score of 0.943533155922 it took 0.0 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed AdaBoostClassifier classifier with an roc score of 0.931726238424 it took 4.766666666666667 minutes.\n",
      "********************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">>> streamline_classifiers: started exploring model performance using 5000 features.\n",
      " >>> streamline_classifiers: fitted and transformed X_train. It took: 7.716666666666667 minutes.\n",
      "Shape of X_train is (1000000, 5000)\n",
      " >>> streamline_classifiers: transformed X_val. It took: 3.783333333333333 minutes.\n",
      "Shape of X_val is (1000000, 5000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >>> Completed LogisticRegression classifier with an roc score of 0.951570299903 it took 0.18333333333333332 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed MultinomialNB classifier with an roc score of 0.942506764896 it took 0.0 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed AdaBoostClassifier classifier with an roc score of 0.931726238424 it took 4.866666666666666 minutes.\n",
      "********************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Construct our feature extraction selection pipeline\n",
    "######################################################\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler, Imputer\n",
    "from sklearn.feature_selection import SelectKBest,VarianceThreshold, chi2,SelectFromModel\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "nfeatures_list = [10,20,50,100,200,500,1000,2000,5000]\n",
    "results = pd.DataFrame(columns=['nfeatures', 'LogisticRegression', 'MultinomialNB', 'AdaBoostClassifier'])\n",
    "\n",
    "for nfeatures in nfeatures_list:\n",
    "  \n",
    "    featureselection_pipeline = Pipeline([\n",
    "            (\"union\",FeatureUnion(\n",
    "                # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "                # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "                transformer_list = [\n",
    "\n",
    "                    (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                        (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                        (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                    ])), # End of: numeric_subpipeline\n",
    "\n",
    "                    (\"text_subpipeline\",Pipeline([\n",
    "                        (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                        (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                     ngram_range = (1,1),\n",
    "                                                     non_negative=True, \n",
    "                                                     norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                        ('dim_red', SelectKBest(k=500)) # Step3: use dimension reduction to select best features \n",
    "                    ]))\n",
    "                ]\n",
    "\n",
    "            )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "            # Common steps:\n",
    "\n",
    "            (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "            (\"scaler\",MaxAbsScaler()), # Scale the features between 0 and 1.       \n",
    "            ('dim_red2', SelectKBest(k = nfeatures))\n",
    "        ])# End of: featureselection_pipeline\n",
    "\n",
    "    results = pd.concat([results, streamline_classifiers(fpipeline=featureselection_pipeline,\n",
    "                       nfeatures = nfeatures,X_train = X_train,y_train = y_train,\n",
    "                       X_val = X_val,y_val = y_val)], axis = 0)\n",
    "    print(\"--\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nfeatures</th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>MultinomialNB</th>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.741096</td>\n",
       "      <td>0.477099</td>\n",
       "      <td>0.741077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.870281</td>\n",
       "      <td>0.869970</td>\n",
       "      <td>0.868690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.885522</td>\n",
       "      <td>0.880458</td>\n",
       "      <td>0.882335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.892628</td>\n",
       "      <td>0.884411</td>\n",
       "      <td>0.889322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.899964</td>\n",
       "      <td>0.891626</td>\n",
       "      <td>0.900375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>0.904338</td>\n",
       "      <td>0.894316</td>\n",
       "      <td>0.888938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.942774</td>\n",
       "      <td>0.922506</td>\n",
       "      <td>0.913993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.951608</td>\n",
       "      <td>0.943533</td>\n",
       "      <td>0.931726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.951570</td>\n",
       "      <td>0.942507</td>\n",
       "      <td>0.931726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  nfeatures  LogisticRegression  MultinomialNB  AdaBoostClassifier\n",
       "0        10            0.741096       0.477099            0.741077\n",
       "0        20            0.870281       0.869970            0.868690\n",
       "0        50            0.885522       0.880458            0.882335\n",
       "0       100            0.892628       0.884411            0.889322\n",
       "0       200            0.899964       0.891626            0.900375\n",
       "0       500            0.904338       0.894316            0.888938\n",
       "0      1000            0.942774       0.922506            0.913993\n",
       "0      2000            0.951608       0.943533            0.931726\n",
       "0      5000            0.951570       0.942507            0.931726"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJsAAAJuCAYAAAAXeAONAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlNAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2c666b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "plt.plot('nfeatures','LogisticRegression', data=results, label = \"LogisticRegression\", marker = 'o')\n",
    "plt.plot('nfeatures','MultinomialNB', data=results, label = \"MultinomialNB\", marker = 'o')\n",
    "plt.plot('nfeatures','AdaBoostClassifier', data=results, label = \"AdaBoostClassifier\", marker = 'o')\n",
    "plt.xlabel(\"Number of features\",fontsize=20)\n",
    "plt.ylabel(\"Validation ROC score\",fontsize=20)\n",
    "plt.title(\"Impact of number of features on model performance\",fontsize=25)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like adding more features beyond 2000 features have minimum impact on model performance. For this data set, it is seems to keep only the 100 best features before hyperparameter optimization.\n",
    "\n",
    "Let's see what we can do by using class-balanced training set we prepared earlier, and just keep 100 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> streamline_classifiers: started exploring model performance using 100 features.\n",
      " >>> streamline_classifiers: fitted and transformed X_train. It took: 8.666666666666666 minutes.\n",
      "Shape of X_train is (835222, 100)\n",
      " >>> streamline_classifiers: transformed X_val. It took: 4.283333333333333 minutes.\n",
      "Shape of X_val is (1000000, 100)\n",
      " >>> Completed LogisticRegression classifier with an roc score of 0.930089232869 it took 0.21666666666666667 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed MultinomialNB classifier with an roc score of 0.909096172642 it took 0.0 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed AdaBoostClassifier classifier with an roc score of 0.905051130916 it took 2.566666666666667 minutes.\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_train_balanced.pkl\",\"rb\") as f:\n",
    "    X_train_balanced = pickle.load(f) \n",
    "\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/y_train_balanced.pkl\", \"rb\") as f:\n",
    "    y_train_balanced = pickle.load(f) \n",
    "nfeatures = 100\n",
    "featureselection_pipeline = Pipeline([\n",
    "            (\"union\",FeatureUnion(\n",
    "                # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "                # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "                transformer_list = [\n",
    "\n",
    "                    (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                        (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                        (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                    ])), # End of: numeric_subpipeline\n",
    "\n",
    "                    (\"text_subpipeline\",Pipeline([\n",
    "                        (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                        (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                     ngram_range = (1,1),\n",
    "                                                     non_negative=True, \n",
    "                                                     norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                        ('dim_red', SelectKBest(k=500)) # Step3: use dimension reduction to select best features \n",
    "                    ]))\n",
    "                ]\n",
    "\n",
    "            )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "            # Common steps:\n",
    "\n",
    "            (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "            (\"scaler\",MaxAbsScaler()), # Scale the features between 0 and 1.       \n",
    "            ('dim_red2', SelectKBest(k = 100))\n",
    "        ])# End of: featureselection_pipeline\n",
    "\n",
    "results_balanced = streamline_classifiers(fpipeline=featureselection_pipeline,\n",
    "                       nfeatures = nfeatures,X_train = X_train_balanced,y_train = y_train_balanced,\n",
    "                       X_val = X_val,y_val = y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like having 100 features extracted from a class-balanced training set increases the model performance. This is a good point to start hyperparameter tuning. \n",
    "\n",
    "As a last step, let's also look at the impact of changing standardization approach to standardscaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> streamline_classifiers: started exploring model performance using 100 features.\n",
      " >>> streamline_classifiers: fitted and transformed X_train. It took: 8.866666666666667 minutes.\n",
      "Shape of X_train is (835222, 100)\n",
      " >>> streamline_classifiers: transformed X_val. It took: 4.6 minutes.\n",
      "Shape of X_val is (1000000, 100)\n",
      " >>> Completed LogisticRegression classifier with an roc score of 0.930163951143 it took 0.31666666666666665 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed MultinomialNB classifier with an roc score of 0.908413806202 it took 0.0 minutes.\n",
      "********************************************************************************\n",
      " >>> Completed AdaBoostClassifier classifier with an roc score of 0.905051130916 it took 2.5166666666666666 minutes.\n",
      "********************************************************************************\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-6ba0f3d9a4dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m results_balanced = streamline_classifiers(fpipeline=featureselection_pipeline,\n\u001b[1;32m     43\u001b[0m                        \u001b[0mnfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_balanced\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_balanced\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                        X_val = X_val,y_val = y_val)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-101-c78adb5e2b2d>\u001b[0m in \u001b[0;36mstreamline_classifiers\u001b[0;34m(fpipeline, nfeatures, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_trans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcalculate_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_trans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/discriminant_analysis.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0mTarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mintegers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \"\"\"\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[0;32m--> 431\u001b[0;31m                                       force_all_finite)\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[1;32m    276\u001b[0m                         \u001b[0;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_train_balanced.pkl\",\"rb\") as f:\n",
    "    X_train_balanced = pickle.load(f) \n",
    "\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/y_train_balanced.pkl\", \"rb\") as f:\n",
    "    y_train_balanced = pickle.load(f) \n",
    "nfeatures = 100\n",
    "featureselection_pipeline = Pipeline([\n",
    "            (\"union\",FeatureUnion(\n",
    "                # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "                # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "                transformer_list = [\n",
    "\n",
    "                    (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                        (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                        (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                    ])), # End of: numeric_subpipeline\n",
    "\n",
    "                    (\"text_subpipeline\",Pipeline([\n",
    "                        (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                        (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                     ngram_range = (1,1),\n",
    "                                                     non_negative=True, \n",
    "                                                     norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                        ('dim_red', SelectKBest(k=500)) # Step3: use dimension reduction to select best features \n",
    "                    ]))\n",
    "                ]\n",
    "\n",
    "            )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "            # Common steps:\n",
    "\n",
    "            (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "            (\"scaler\",StandardScaler(with_mean = False)), # Scale the features between 0 and 1.       \n",
    "            ('dim_red2', SelectKBest(k = 100))\n",
    "        ])# End of: featureselection_pipeline\n",
    "\n",
    "results_balanced = streamline_classifiers(fpipeline=featureselection_pipeline,\n",
    "                       nfeatures = nfeatures,X_train = X_train_balanced,y_train = y_train_balanced,\n",
    "                       X_val = X_val,y_val = y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing scaler has almost no impact. Let's keep MaxAbscaler. Let's transform class-balanced training set and validation sets using this simplified pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_train_balanced.pkl\",\"rb\") as f:\n",
    "    X_train_balanced = pickle.load(f) \n",
    "\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/y_train_balanced.pkl\", \"rb\") as f:\n",
    "    y_train_balanced = pickle.load(f)\n",
    "    \n",
    "# Our validation features and target labels were prepared before:\n",
    "import pickle\n",
    "with open(\"X_val1.pkl\",\"rb\") as f:\n",
    "    X_val1 = pickle.load(f)   \n",
    "with open(\"y_val1.pkl\",\"rb\") as f:\n",
    "    y_val1 = pickle.load(f)\n",
    "        \n",
    "with open(\"X_val2.pkl\",\"rb\") as f:\n",
    "    X_val2 = pickle.load(f)       \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2 = pickle.load(f)      \n",
    "\n",
    "featureselection_pipeline = Pipeline([\n",
    "            (\"union\",FeatureUnion(\n",
    "                # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "                # is the name of the transformer within the FeatureUnion\n",
    "\n",
    "                transformer_list = [\n",
    "\n",
    "                    (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                        (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                        (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "                    ])), # End of: numeric_subpipeline\n",
    "\n",
    "                    (\"text_subpipeline\",Pipeline([\n",
    "                        (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                        (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                                     ngram_range = (1,1),\n",
    "                                                     non_negative=True, \n",
    "                                                     norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                        ('dim_red', SelectKBest(k=500)) # Step3: use dimension reduction to select best features \n",
    "                    ]))\n",
    "                ]\n",
    "\n",
    "            )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "\n",
    "            # Common steps:\n",
    "\n",
    "            (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "            (\"scaler\",MaxAbsScaler()), # Scale the features between 0 and 1.       \n",
    "            ('dim_red2', SelectKBest(k = 100))\n",
    "        ])# End of: featureselection_pipeline\n",
    "\n",
    "print(\"Started training pipeline.\")\n",
    "# Fit the pipeline using the training set\n",
    "featureselection_pipeline.fit(X_train_balanced,y_train_balanced)\n",
    "print(\"Trained pipeline.\")\n",
    "\n",
    "# Save the trained pipeline as 'featureselection_pipeline100.pkl'\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/featureselection_pipeline100.pkl\",\"wb\") as f:\n",
    "    pickle.dump(featureselection_pipeline,f)\n",
    "print(\"Saved pipeline.\")\n",
    "\n",
    "# Transform training set\n",
    "X_train_balanced_trans_100 = featureselection_pipeline.transform(X_train_balanced)\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_train_balanced_trans_100.pkl\",\"wb\") as f:\n",
    "    pickle.dump(X_train_balanced_trans_100,f)\n",
    "print(\"Saved X_train_balanced_trans_100.\" + str(X_train_balanced_trans_100.shape))\n",
    "\n",
    "# Transform X_val1 set\n",
    "X_val1_trans_100 = featureselection_pipeline.transform(X_val1)\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_val1_trans_100.pkl\",\"wb\") as f:\n",
    "    pickle.dump(X_val1_trans_100,f)\n",
    "print(\"Saved X_val1_trans_100.\" + str(X_val1_trans_100.shape))\n",
    "\n",
    "# Transform X_val2 set\n",
    "X_val2_trans_100 = featureselection_pipeline.transform(X_val2)\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_val2_trans_100.pkl\",\"wb\") as f:\n",
    "    pickle.dump(X_val2_trans_100,f)\n",
    "print(\"Saved X_val2_trans_100.\" + str(X_val2_trans_100.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
