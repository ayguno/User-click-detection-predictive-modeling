{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Through the initial analysis and exploration of the train_sample set (100K observations) we developed some initial expectations about the data. Here we are going to build a pipeline in order to effectively process data sets for modeling.\n",
    "\n",
    "# Planing for the Pipeline\n",
    "\n",
    "1. We will prepare 3 data sets from the training set. The training set provided is 200 million samples, we don't have computational power to use all of this data at the moment. Instead, we will extract 3 data sets each contain ~1 million observations. We will refer these sets as:\n",
    "\n",
    "    - training_set (1:100,000th rows of the original training set)\n",
    "    - validation_set1 (100,001:200,000th rows of the original training set)\n",
    "    - validation_set2 (200,001:300,000th rows of the original training set)\n",
    "\n",
    "2. Build the feature extraction and selection pipeline using the training set:\n",
    "\n",
    "    - Using the insights we obtained from data exploration, the following features will be used to create dummy variables: device, app, os and channel. We will perform this by converting these features to string, tokenization and selecting 300 best features.\n",
    "    - We will write custom processing functions to add the log_total_clicks and log_total_click_time features, and remove the unwanted base features\n",
    "    \n",
    "3. We will prepare the remainder of the pipeline to incorporate interaction terms and perform scaling and standardization.    \n",
    "\n",
    "## Prepare training and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training_set\n",
      "Finished validation_set1\n",
      "Finished validation_set2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "training_set = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/train.csv\",\n",
    "                           nrows=1000000,\n",
    "                           dtype = \"str\")\n",
    "print(\"Finished training_set\")\n",
    "validation_set1 = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/train.csv\",\n",
    "                           skiprows = 1000000,names = list(training_set.columns),\n",
    "                           nrows=1000000,\n",
    "                           dtype = \"str\")\n",
    "print(\"Finished validation_set1\")\n",
    "validation_set2 = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/train.csv\",\n",
    "                           skiprows = 2000000,names = list(training_set.columns),\n",
    "                           nrows=1000000,\n",
    "                           dtype = \"str\")\n",
    "print(\"Finished validation_set2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121848</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>105</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2698</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>259</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5729</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>237</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122891</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>280</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105433</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>245</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip app device  os channel           click_time attributed_time  \\\n",
       "0  121848  24      1  19     105  2017-11-06 16:21:51             NaN   \n",
       "1    2698  25      1  30     259  2017-11-06 16:21:51             NaN   \n",
       "2    5729   2      1  19     237  2017-11-06 16:21:51             NaN   \n",
       "3  122891   3      1  35     280  2017-11-06 16:21:51             NaN   \n",
       "4  105433  15      2  25     245  2017-11-06 16:21:51             NaN   \n",
       "\n",
       "  is_attributed  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 8 columns):\n",
      "ip                 1000000 non-null object\n",
      "app                1000000 non-null object\n",
      "device             1000000 non-null object\n",
      "os                 1000000 non-null object\n",
      "channel            1000000 non-null object\n",
      "click_time         1000000 non-null object\n",
      "attributed_time    1693 non-null object\n",
      "is_attributed      1000000 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 61.0+ MB\n"
     ]
    }
   ],
   "source": [
    "training_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote training_set to disk\n",
      "Wrote validation_set1 to disk\n",
      "Wrote validation_set2 to disk\n"
     ]
    }
   ],
   "source": [
    "# Let's save them for future easier individual loading\n",
    "training_set.to_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/training_set.csv\")\n",
    "print(\"Wrote training_set to disk\")\n",
    "\n",
    "validation_set1.to_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/validation_set1.csv\")\n",
    "print(\"Wrote validation_set1 to disk\")\n",
    "\n",
    "validation_set2.to_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/validation_set2.csv\")\n",
    "print(\"Wrote validation_set2 to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/training_set.csv\",\n",
    "                          index_col = 0, dtype = \"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83230</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:32:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17357</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:33:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35810</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:34:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45745</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>478</td>\n",
       "      <td>2017-11-06 14:34:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161007</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:35:08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip app device  os channel           click_time attributed_time  \\\n",
       "0   83230   3      1  13     379  2017-11-06 14:32:21             NaN   \n",
       "1   17357   3      1  19     379  2017-11-06 14:33:34             NaN   \n",
       "2   35810   3      1  13     379  2017-11-06 14:34:12             NaN   \n",
       "3   45745  14      1  13     478  2017-11-06 14:34:52             NaN   \n",
       "4  161007   3      1  13     379  2017-11-06 14:35:08             NaN   \n",
       "\n",
       "  is_attributed  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ip',\n",
       " 'app',\n",
       " 'device',\n",
       " 'os',\n",
       " 'channel',\n",
       " 'click_time',\n",
       " 'attributed_time',\n",
       " 'is_attributed']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(training_set.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate target labels from feature matrix \n",
    "\n",
    "We will seperate target labels from features for each of these data sets and pickle them for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "X_train = training_set.drop([\"is_attributed\",\"attributed_time\"], axis = 1)\n",
    "y_train = pd.to_numeric(training_set.is_attributed) \n",
    "\n",
    "X_train.to_pickle(\"X_train.pkl\")\n",
    "y_train.to_pickle(\"y_train.pkl\")\n",
    "\n",
    "X_val1 = validation_set1.drop([\"is_attributed\",\"attributed_time\"], axis = 1)\n",
    "y_val1 = pd.to_numeric(validation_set1.is_attributed) \n",
    "\n",
    "X_val1.to_pickle(\"X_val1.pkl\")\n",
    "y_val1.to_pickle(\"y_val1.pkl\")\n",
    "\n",
    "X_val2 = validation_set2.drop([\"is_attributed\",\"attributed_time\"], axis = 1)\n",
    "y_val2 = pd.to_numeric(validation_set2.is_attributed) \n",
    "\n",
    "X_val2.to_pickle(\"X_val2.pkl\")\n",
    "y_val2.to_pickle(\"y_val2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " '.Rhistory',\n",
       " 'app_dummy.rds',\n",
       " 'channel_dummy.rds',\n",
       " 'device_dummy.rds',\n",
       " 'os_dummy.rds',\n",
       " 'test_processed.csv',\n",
       " 'train_sample.csv',\n",
       " 'User-click-detection-predictive-modeling.ipynb',\n",
       " 'UserClickDetectionPredictiveModeling.Rmd',\n",
       " 'X_train.pkl',\n",
       " 'X_val1.pkl',\n",
       " 'X_val2.pkl',\n",
       " 'y_train.pkl',\n",
       " 'y_val1.pkl',\n",
       " 'y_val2.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the feature extraction and selection pipeline using the training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Read the pickled training set\n",
    "X_train = pd.read_pickle(\"X_train.pkl\")\n",
    "y_train = pd.read_pickle(\"y_train.pkl\")\n",
    "\n",
    "# Label text features\n",
    "Text_features = [\"app\",\"device\",\"os\",\"channel\"]\n",
    "\n",
    "##############################################################\n",
    "# Define utility function to parse and process text features\n",
    "##############################################################\n",
    "# Note we avoid lambda functions since they don't pickle when we want to save the pipeline later   \n",
    "def column_text_processer_nolambda(df,text_columns = Text_features):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \"\"\"\"A function that will merge/join all text in a given row to make it ready for tokenization. \n",
    "    - This function should take care of converting missing values to empty strings. \n",
    "    - It should also convert the text to lowercase.\n",
    "    df= pandas dataframe\n",
    "    text_columns = names of the text features in df\n",
    "    \"\"\" \n",
    "    # Select only non-text columns that are in the df\n",
    "    text_data = df[text_columns]\n",
    "    \n",
    "    # Fill the missing values in text_data using empty strings\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    \n",
    "    # Concatenate feature name to each category encoding for each row\n",
    "    # E.g: encoding 3 at device column will read as device3 to make each encoding unique for a given feature\n",
    "    for col_index in list(text_data.columns):\n",
    "        text_data[col_index] = col_index + text_data[col_index].astype(str)\n",
    "    \n",
    "    # Join all the strings in a given row to make a vector\n",
    "    # text_vector = text_data.apply(lambda x: \" \".join(x), axis = 1)\n",
    "    text_vector = []\n",
    "    for index,rows in text_data.iterrows():\n",
    "        text_item = \" \".join(rows).lower()\n",
    "        text_vector.append(text_item)\n",
    "\n",
    "    # return text_vector as pd.Series object to enter the tokenization pipeline\n",
    "    return pd.Series(text_vector)\n",
    "\n",
    "#######################################################################\n",
    "# Define custom processing functions to add the log_total_clicks and \n",
    "# log_total_click_time features, and remove the unwanted base features\n",
    "#######################################################################\n",
    "def column_time_processer(X_train):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert click_time to datetime64 dtype \n",
    "    X_train.click_time = pd.to_datetime(X_train.click_time)\n",
    "\n",
    "    # Calculate the log_total_clicks for each ip and add as a new feature to temp_data\n",
    "    temp_data = pd.DataFrame(np.log(X_train.groupby([\"ip\"]).size()),\n",
    "                                    columns = [\"log_total_clicks\"]).reset_index()\n",
    "\n",
    "\n",
    "    # Calculate the log_total_click_time for each ip and add as a new feature to temp_data\n",
    "    # First define a function to process selected ip group \n",
    "    def get_log_total_click_time(group):\n",
    "        diff = (max(group.click_time) - min(group.click_time)).seconds\n",
    "        return np.log(diff+1)\n",
    "\n",
    "    # Then apply this function to each ip group and extract the total click time per ip group\n",
    "    log_time_frame = pd.DataFrame(X_train.groupby([\"ip\"]).apply(get_log_total_click_time),\n",
    "                                  columns=[\"log_total_click_time\"]).reset_index()\n",
    "\n",
    "    # Then add this new feature to the temp_data\n",
    "    temp_data = pd.merge(temp_data,log_time_frame, how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Combine temp_data with X_train to maintain X_train key order\n",
    "    temp_data = pd.merge(X_train,temp_data,how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Drop features that are not needed\n",
    "    temp_data = temp_data[[\"log_total_clicks\",\"log_total_click_time\"]]\n",
    "\n",
    "    # Return only the numeric features as a tensor to integrate into the numeric feature branch of the pipeline\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# We need to wrap these custom utility functions using FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# FunctionTransformer wrapper of utility functions to parse text and numeric features\n",
    "# Note how we avoid putting any arguments into column_text_processer or column_time_processer\n",
    "#############################################################################\n",
    "get_numeric_data = FunctionTransformer(func = column_time_processer, validate=False) \n",
    "get_text_data = FunctionTransformer(func = column_text_processer_nolambda,validate=False) \n",
    "\n",
    "#############################################################################\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "# #Note this regex will match either a whitespace or a punctuation to tokenize \n",
    "# the string vector on these preferences, in our case we only have white spaces in our text  \n",
    "#############################################################################\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "\n",
    "###############################################\n",
    "# Construct our feature extraction pipeline\n",
    "###############################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, chi2 # We will use chi-squared as a scoring function to select features for classification\n",
    "from sklearn.metrics import auc\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module since it was saved into working directory as SparseInteractions.py\n",
    "\n",
    "userclick_pipeline1 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion(\n",
    "        # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "        # is the name of the transformer within the FeatureUnion\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "            ])), # End of: numeric_subpipeline\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                             ngram_range = (1,1),\n",
    "                                             non_negative=True, \n",
    "                                             norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                ('dim_red', SelectKBest(chi2,300)) # Step3: use dimension reduction to select 300 best features using chi2 as scoring function\n",
    "            ]))\n",
    "        ]\n",
    "        \n",
    "    )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "    \n",
    "    # Common steps:\n",
    "            \n",
    "    (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "    (\"scaler\",MaxAbsScaler()) # Scale the features between 0 and 1.       \n",
    "            \n",
    "])# End of: userclick_pipeline1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop the userclick_pipeline1 by using the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 3.3333333333333335 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "userclick_pipeline1.fit(X_train,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained our pipeline using the training set, we will pickle it and store for reuse. This will ensure the consistency every time we want to process a data set, and we will extract the same set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle and store the userclick_pipeline1\n",
    "import pickle\n",
    "with open(\"userclick_pipeline1.pkl\",\"wb\") as f:\n",
    "    pickle.dump(userclick_pipeline1,f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the features in the training set using the established pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the userclick_pipeline1 to work with\n",
    "import pickle\n",
    "with open(\"userclick_pipeline1.pkl\",\"rb\") as f:\n",
    "    userclick_pipeline1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 3.0833333333333335 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# Transform the training set features\n",
    "X_train_trans_pl1 = userclick_pipeline1.transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 45753)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_trans_pl1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csc.csc_matrix"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_trans_pl1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pickle and save this transformed version of the features from the training set. We spent about 3 minutes by training the pipeline and an additional 3 minutes for transforming the features.\n",
    "\n",
    "In the future, we will only use this pipeline with .transform method to process any datasets we would like to use in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed version of training set features\n",
    "import pickle\n",
    "with open(\"X_train_trans_pl1.pkl\",\"wb\") as f:\n",
    "    pickle.dump(X_train_trans_pl1,f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Regularized Linear Model for predicting true events\n",
    "\n",
    "We will start linear and train a Ridge classifier by setting the regularization parameter alpha to 0.5. We will see the untuned model performance first, then try to optimize the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 1.3333333333333333 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Read the transformed features and target labels from the training set\n",
    "import pickle\n",
    "with open(\"X_train_trans_pl1.pkl\",\"rb\") as f:\n",
    "    X_train_trans_pl1 = pickle.load(f)\n",
    "with open(\"y_train.pkl\",\"rb\") as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# Instantiate a Ridge classifier with a medium alpha \n",
    "Ridge = RidgeClassifier(alpha=0.5, random_state= 321)\n",
    "\n",
    "# Train the model\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "Ridge.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 0.016666666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Predict class labels using training set\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# Predict class probabilities\n",
    "# Note that there is no predict_proba on RidgeClassifier\n",
    "# So we use the trick in https://stackoverflow.com/questions/22538080/scikit-learn-ridge-classifier-extracting-class-probabilities \n",
    "\n",
    "d = Ridge.decision_function(X= X_train_trans_pl1) # Predict confidence scores for samples\n",
    "probs = np.exp(d) / np.sum(np.exp(d)) # Use softmax to convert them probabilities between 0 and 1\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94980780103952844"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate ROC score between the predicted probability and the observed target\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train,probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even this naive attempt gave us an ROC score of 0.949, Next, we will try to perform hyperparameter optimization to see where we can get further: \n",
    "\n",
    "## Hyperparameter optimization using Ridge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n",
      "[CV] alpha=0.86 ......................................................\n",
      "[CV] alpha=0.86 ......................................................\n",
      "[CV] alpha=0.25 ......................................................\n",
      "[CV] ............. alpha=0.86, score=0.8914263110278222, total=  37.6s\n",
      "[CV] alpha=0.25 ......................................................\n",
      "[CV] ............. alpha=0.86, score=0.8956513669854133, total= 1.1min\n",
      "[CV] alpha=0.45 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:  1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. alpha=0.25, score=0.884190883324053, total= 1.4min\n",
      "[CV] alpha=0.45 ......................................................\n",
      "[CV] ............. alpha=0.25, score=0.8914015886647204, total= 1.3min\n",
      "[CV] alpha=0.32 ......................................................\n",
      "[CV] ............. alpha=0.45, score=0.8886544167323791, total=  53.1s\n",
      "[CV] alpha=0.32 ......................................................\n",
      "[CV] ............. alpha=0.45, score=0.8941574912061789, total=  55.7s\n",
      "[CV] alpha=0.2 .......................................................\n",
      "[CV] ............. alpha=0.32, score=0.8875157179170297, total= 1.2min\n",
      "[CV] alpha=0.2 .......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:  3.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.32, score=0.8924351290802932, total= 1.2min\n",
      "[CV] alpha=0.06 ......................................................\n",
      "[CV] ............... alpha=0.2, score=0.882164720700341, total= 1.8min\n",
      "[CV] alpha=0.06 ......................................................\n",
      "[CV] .............. alpha=0.2, score=0.8905415407269656, total= 1.8min\n",
      "[CV] alpha=0.15 ......................................................\n",
      "[CV] ............. alpha=0.06, score=0.8786576036594884, total= 2.8min\n",
      "[CV] alpha=0.15 ......................................................\n",
      "[CV] .............. alpha=0.15, score=0.883062963076619, total= 1.8min\n",
      "[CV] alpha=0.82 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:  6.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.06, score=0.8759978611806225, total= 2.7min\n",
      "[CV] alpha=0.82 ......................................................\n",
      "[CV] ............. alpha=0.82, score=0.8913547259152571, total=  33.9s\n",
      "[CV] alpha=0.08 ......................................................\n",
      "[CV] ............. alpha=0.15, score=0.8884136760769671, total= 1.7min\n",
      "[CV] alpha=0.08 ......................................................\n",
      "[CV] ............. alpha=0.82, score=0.8954961726441188, total=  55.6s\n",
      "[CV] alpha=0.38 ......................................................\n",
      "[CV] ............. alpha=0.38, score=0.8886718559207138, total=  57.1s\n",
      "[CV] alpha=0.38 ......................................................\n",
      "[CV] .............. alpha=0.08, score=0.880409659046294, total= 2.2min\n",
      "[CV] alpha=0.69 ......................................................\n",
      "[CV] ............. alpha=0.38, score=0.8924743918088495, total= 1.0min\n",
      "[CV] alpha=0.69 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:  9.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.08, score=0.8830280871683156, total= 2.3min\n",
      "[CV] alpha=0.48 ......................................................\n",
      "[CV] ............. alpha=0.69, score=0.8903844868681919, total=  38.9s\n",
      "[CV] alpha=0.48 ......................................................\n",
      "[CV] .............. alpha=0.69, score=0.894370026181325, total=  40.2s\n",
      "[CV] alpha=0.95 ......................................................\n",
      "[CV] ............. alpha=0.48, score=0.8886348535251752, total=  48.8s\n",
      "[CV] alpha=0.95 ......................................................\n",
      "[CV] ............. alpha=0.48, score=0.8942835960807918, total=  50.9s\n",
      "[CV] alpha=0.42 ......................................................\n",
      "[CV] ............. alpha=0.95, score=0.8912950972786214, total=  29.2s\n",
      "[CV] alpha=0.42 ......................................................\n",
      "[CV] .............. alpha=0.42, score=0.888656706320648, total=  54.5s\n",
      "[CV] alpha=0.67 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed: 12.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.42, score=0.8933858578180848, total=  58.5s\n",
      "[CV] alpha=0.67 ......................................................\n",
      "[CV] ............. alpha=0.95, score=0.8976710720537423, total= 1.3min\n",
      "[CV] alpha=0.74 ......................................................\n",
      "[CV] ............. alpha=0.67, score=0.8904195284489833, total=  38.5s\n",
      "[CV] alpha=0.74 ......................................................\n",
      "[CV] ............. alpha=0.67, score=0.8943406596495505, total=  42.0s\n",
      "[CV] alpha=0.7 .......................................................\n",
      "[CV] ............. alpha=0.74, score=0.8902902991673008, total=  36.1s\n",
      "[CV] alpha=0.7 .......................................................\n",
      "[CV] ............. alpha=0.74, score=0.8951258526506547, total=  43.6s\n",
      "[CV] alpha=0.44 ......................................................\n",
      "[CV] .............. alpha=0.7, score=0.8904273622468626, total=  38.9s\n",
      "[CV] alpha=0.44 ......................................................\n",
      "[CV] .............. alpha=0.7, score=0.8944044414547937, total=  41.6s\n",
      "[CV] alpha=0.51 ......................................................\n",
      "[CV] ............. alpha=0.44, score=0.8886681708189335, total=  53.0s\n",
      "[CV] alpha=0.51 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed: 14.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.51, score=0.8885397528273091, total=  47.1s\n",
      "[CV] alpha=0.53 ......................................................\n",
      "[CV] ............. alpha=0.44, score=0.8940703814673404, total=  55.5s\n",
      "[CV] alpha=0.53 ......................................................\n",
      "[CV] ............. alpha=0.51, score=0.8943254518544484, total=  51.8s\n",
      "[CV] ............. alpha=0.53, score=0.8885548291037833, total=  47.6s\n",
      "[CV] ............. alpha=0.53, score=0.8943041770442346, total=  49.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  40 out of  40 | elapsed: 15.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done  40 out of  40 | elapsed: 15.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 16.15 minutes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "params_space = {\"alpha\": np.arange(0,1,0.01)}\n",
    "RidgeSearch = RandomizedSearchCV(Ridge,cv = 2,verbose=10,\n",
    "                                 n_iter=20,\n",
    "                                 n_jobs=3,\n",
    "                                 param_distributions=params_space,\n",
    "                                 random_state= 321,\n",
    "                                 scoring= \"roc_auc\")\n",
    "\n",
    "RidgeSearch.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89448307829020712"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RidgeSearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.95000000000000007}"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RidgeSearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC score is:0.949325548082\n",
      "It took: 1.05 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Use 'alpha': 0.95 to re-fit the Ridge classifier and calculate the performance\n",
    "# Read the transformed features and target labels from the training set\n",
    "import pickle\n",
    "with open(\"X_train_trans_pl1.pkl\",\"rb\") as f:\n",
    "    X_train_trans_pl1 = pickle.load(f)\n",
    "with open(\"y_train.pkl\",\"rb\") as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# Instantiate a Ridge classifier with a medium alpha \n",
    "Ridge = RidgeClassifier(alpha=0.95, random_state= 321)\n",
    "\n",
    "# Train the model\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "Ridge.fit(X_train_trans_pl1,y_train)\n",
    "d = Ridge.decision_function(X= X_train_trans_pl1) # Predict confidence scores for samples\n",
    "probs = np.exp(d) / np.sum(np.exp(d)) # Use softmax to convert them probabilities between 0 and 1\n",
    "\n",
    "# Calculate ROC score between the predicted probability and the observed target\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print( \"The ROC score is:\" + str(roc_auc_score(y_train,probs)))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could not improve the performance of the Ridge model using this approach. Let's save this model and continue to explore other types of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"Ridge_classifier.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Ridge, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the transformed features and target labels from the training set\n",
    "import pickle\n",
    "import numpy as np\n",
    "with open(\"X_train_trans_pl1.pkl\",\"rb\") as f:\n",
    "    X_train_trans_pl1 = pickle.load(f)\n",
    "with open(\"y_train.pkl\",\"rb\") as f:\n",
    "    y_train = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 0.016666666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probability predictions\n",
    "probs = mnb.predict_proba(X_train_trans_pl1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   6.32335454e-15],\n",
       "       [  1.00000000e+00,   7.52020576e-15],\n",
       "       [  1.00000000e+00,   1.22347330e-14],\n",
       "       ..., \n",
       "       [  1.00000000e+00,   1.74420279e-12],\n",
       "       [  1.00000000e+00,   5.90993405e-16],\n",
       "       [  1.00000000e+00,   5.74730378e-13]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs # We need the second column which is the probabilities for class label: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = probs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB roc score is: 0.946744947288\n"
     ]
    }
   ],
   "source": [
    "# Calculate the roc score for the training set\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"NB roc score is: \" + str(roc_auc_score(y_train,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our untuned classifier, which has similar performance to Ridge. Can we try to tune it to perform better? Looking into description, we find the following parameters that can be tuned:\n",
    "\n",
    "Parameters\n",
    "\n",
    " |  alpha : float, optional (default=1.0)\n",
    " |      Additive (Laplace/Lidstone) smoothing parameter\n",
    " |      (0 for no smoothing).\n",
    "\n",
    "\n",
    " |  fit_prior : boolean, optional (default=True)\n",
    " |      Whether to learn class prior probabilities or not.\n",
    " |      If false, a uniform prior will be used.\n",
    "\n",
    "\n",
    " |  class_prior : array-like, size (n_classes,), optional (default=None)\n",
    " |      Prior probabilities of the classes. If specified the priors are not\n",
    " |      adjusted according to the data.\n",
    " \n",
    "\n",
    "- We can make search across the alpha (0-1). \n",
    "- We will leave fit_prior = True\n",
    "- We have some idea about the probability of being in class 1, which is (sum(y_train)/len(y_train)) (0.00169) in our training set. Why don't we use this information in our hyperparameter search and see if it makes any difference.\n",
    "\n",
    "Since the classifier trained very fast, we can perform exhaustive GridSearch with 3-fold CV.\n",
    "\n",
    "## Hyperparameter optimization for Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 22 candidates, totalling 66 fits\n",
      "[CV] alpha=0.0, class_prior=None, fit_prior=True .....................\n",
      "[CV] alpha=0.0, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] alpha=0.0, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=None, fit_prior=True, score=0.8959370258094743, total=   1.6s\n",
      "[CV] alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=None, fit_prior=True, score=0.8868648185207342, total=   1.9s\n",
      "[CV] alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    5.2s\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=None, fit_prior=True, score=0.8857097885869714, total=   2.0s\n",
      "[CV] alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.8959370204907351, total=   1.7s\n",
      "[CV] alpha=0.1, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.886864834505243, total=   1.7s\n",
      "[CV] alpha=0.1, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.8857097859228865, total=   1.6s\n",
      "[CV] alpha=0.1, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.1, class_prior=None, fit_prior=True, score=0.9435606570419294, total=   1.5s\n",
      "[CV] alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    9.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.1, class_prior=None, fit_prior=True, score=0.9327180517680261, total=   1.7s\n",
      "[CV] alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.1, class_prior=None, fit_prior=True, score=0.9505139967923142, total=   1.5s\n",
      "[CV] alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9435606517231905, total=   1.5s\n",
      "[CV] alpha=0.2, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9327180624243653, total=   1.6s\n",
      "[CV] alpha=0.2, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9505139914641447, total=   1.7s\n",
      "[CV] alpha=0.2, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   13.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.2, class_prior=None, fit_prior=True, score=0.9444671084783707, total=   1.6s\n",
      "[CV] alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.2, class_prior=None, fit_prior=True, score=0.9326124714247604, total=   1.6s\n",
      "[CV] alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.2, class_prior=None, fit_prior=True, score=0.9520073601628835, total=   1.5s\n",
      "[CV] alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9444671270939577, total=   1.4s\n",
      "[CV] alpha=0.3, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9326125300346252, total=   1.7s\n",
      "[CV] alpha=0.3, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9520073814755616, total=   1.7s\n",
      "[CV] alpha=0.3, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.3, class_prior=None, fit_prior=True, score=0.9446672473134183, total=   1.6s\n",
      "[CV] alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   18.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.3, class_prior=None, fit_prior=True, score=0.931901677625326, total=   1.5s\n",
      "[CV] alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.3, class_prior=None, fit_prior=True, score=0.9524465345361612, total=   1.6s\n",
      "[CV] alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9446671994447662, total=   1.8s\n",
      "[CV] alpha=0.4, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9319016589767328, total=   1.7s\n",
      "[CV] alpha=0.4, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9524465345361612, total=   1.6s\n",
      "[CV] alpha=0.4, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.4, class_prior=None, fit_prior=True, score=0.9445732731709474, total=   1.4s\n",
      "[CV] alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.4, class_prior=None, fit_prior=True, score=0.9310744313527055, total=   1.4s\n",
      "[CV] alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   24.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.4, class_prior=None, fit_prior=True, score=0.9524166168642662, total=   1.4s\n",
      "[CV] alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9445732731709476, total=   1.4s\n",
      "[CV] alpha=0.5, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9310744313527057, total=   1.4s\n",
      "[CV] alpha=0.5, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9524165928875032, total=   1.5s\n",
      "[CV] alpha=0.5, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.5, class_prior=None, fit_prior=True, score=0.9442627013551642, total=   1.5s\n",
      "[CV] alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.5, class_prior=None, fit_prior=True, score=0.9302755229497157, total=   1.5s\n",
      "[CV] alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.5, class_prior=None, fit_prior=True, score=0.9521864532611158, total=   1.4s\n",
      "[CV] alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9442626960364251, total=   1.5s\n",
      "[CV] alpha=0.6, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9302755122933766, total=   1.6s\n",
      "[CV] alpha=0.6, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:   31.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9521864452688615, total=   1.5s\n",
      "[CV] alpha=0.6, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.6, class_prior=None, fit_prior=True, score=0.9438514218731604, total=   1.6s\n",
      "[CV] alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.6, class_prior=None, fit_prior=True, score=0.9294995310038618, total=   1.7s\n",
      "[CV] alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.6, class_prior=None, fit_prior=True, score=0.9518519294655212, total=   1.4s\n",
      "[CV] alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9438514218731604, total=   1.5s\n",
      "[CV] alpha=0.7, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9294995310038618, total=   1.6s\n",
      "[CV] alpha=0.7, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9518519294655213, total=   1.5s\n",
      "[CV] alpha=0.7, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.7, class_prior=None, fit_prior=True, score=0.9433635658444083, total=   1.6s\n",
      "[CV] alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.7, class_prior=None, fit_prior=True, score=0.9287724996077935, total=   1.5s\n",
      "[CV] alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   38.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.7, class_prior=None, fit_prior=True, score=0.9514674114552534, total=   1.6s\n",
      "[CV] alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9433635685037778, total=   1.6s\n",
      "[CV] alpha=0.8, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9287725022718782, total=   1.6s\n",
      "[CV] alpha=0.8, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9514674061270838, total=   1.8s\n",
      "[CV] alpha=0.8, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.8, class_prior=None, fit_prior=True, score=0.942827681601234, total=   2.0s\n",
      "[CV] alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.8, class_prior=None, fit_prior=True, score=0.9280890526384574, total=   2.1s\n",
      "[CV] alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.8, class_prior=None, fit_prior=True, score=0.9510515265109788, total=   1.8s\n",
      "[CV] alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9428276789418646, total=   1.6s\n",
      "[CV] alpha=0.9, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9280890286616944, total=   1.6s\n",
      "[CV] alpha=0.9, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9510515265109788, total=   1.6s\n",
      "[CV] alpha=0.9, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.9, class_prior=None, fit_prior=True, score=0.9422726206440957, total=   1.7s\n",
      "[CV] alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:   48.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.9, class_prior=None, fit_prior=True, score=0.9274512973869017, total=   1.7s\n",
      "[CV] alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.9, class_prior=None, fit_prior=True, score=0.9506482853129924, total=   1.5s\n",
      "[CV] alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9422726206440957, total=   1.9s\n",
      "[CV] alpha=1.0, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9274512973869015, total=   2.1s\n",
      "[CV] alpha=1.0, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9506482853129923, total=   2.2s\n",
      "[CV] alpha=1.0, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=1.0, class_prior=None, fit_prior=True, score=0.9417146237429526, total=   2.3s\n",
      "[CV] alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=1.0, class_prior=None, fit_prior=True, score=0.926872215938179, total=   2.0s\n",
      "[CV] alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=1.0, class_prior=None, fit_prior=True, score=0.9502528152502612, total=   1.7s\n",
      "[CV] alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9417146264023222, total=   1.6s\n",
      "[CV]  alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9268722079459248, total=   1.7s\n",
      "[CV]  alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9502528205784307, total=   1.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  66 out of  66 | elapsed:   58.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 1.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "params_space = {\n",
    "    \"alpha\":np.arange(0,1.1,0.1),\n",
    "    \"fit_prior\":[True],\n",
    "    \"class_prior\":[None,[1-0.00169,0.00169]]\n",
    "}\n",
    "\n",
    "MNBsearch = GridSearchCV(mnb,\n",
    "                         param_grid= params_space,\n",
    "                         scoring=\"roc_auc\",\n",
    "                         cv =3, n_jobs=3,verbose=10)\n",
    "\n",
    "MNBsearch.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.20000000000000001,\n",
       " 'class_prior': [0.99831, 0.00169],\n",
       " 'fit_prior': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNBsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94302901430616237"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNBsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95332437154555072"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "probs = MNBsearch.best_estimator_.predict_proba(X_train_trans_pl1)[:,1]\n",
    "roc_auc_score(y_train,probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the hyperparameter search improved MNB model performance. We will store the best estimator we obtained for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"mnb.pkl\",\"wb\") as f:\n",
    "    pickle.dump(MNBsearch.best_estimator_,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Support Vector Machine Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the transformed features and target labels from the training set\n",
    "import pickle\n",
    "import numpy as np\n",
    "with open(\"X_train_trans_pl1.pkl\",\"rb\") as f:\n",
    "    X_train_trans_pl1 = pickle.load(f)\n",
    "with open(\"y_train.pkl\",\"rb\") as f:\n",
    "    y_train = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear]It took: 2.25 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "# Note that we can't get probabilities directly from this LinearSVC function\n",
    "# We need to wrap into Calibrated Classifier \n",
    "# (see: https://stackoverflow.com/questions/35212213/sklearn-how-to-get-decision-probabilities-for-linearsvc-classifier)\n",
    "\n",
    "lsvc = LinearSVC(verbose=10)\n",
    "\n",
    "cal_lsvc = CalibratedClassifierCV(base_estimator = lsvc,\n",
    "                                  cv = 3, # Also performs cross-validation\n",
    "                                  method= \"sigmoid\") # We use sigmoid function to get probabilities\n",
    "\n",
    "cal_lsvc.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = cal_lsvc.predict_proba(X_train_trans_pl1)[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95099179905081954"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate ROC score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train,probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good start for an untuned classifier, let's try to perform hyperparameter search to see if we can improve this performance.\n",
    "\n",
    "## Hyperparameter tuning for SVC classifier\n",
    "\n",
    "We need to understand what SVC parameters we can tune in the context of calibrated classifier wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "      verbose=10),\n",
       " 'base_estimator__C': 1.0,\n",
       " 'base_estimator__class_weight': None,\n",
       " 'base_estimator__dual': True,\n",
       " 'base_estimator__fit_intercept': True,\n",
       " 'base_estimator__intercept_scaling': 1,\n",
       " 'base_estimator__loss': 'squared_hinge',\n",
       " 'base_estimator__max_iter': 1000,\n",
       " 'base_estimator__multi_class': 'ovr',\n",
       " 'base_estimator__penalty': 'l2',\n",
       " 'base_estimator__random_state': None,\n",
       " 'base_estimator__tol': 0.0001,\n",
       " 'base_estimator__verbose': 10,\n",
       " 'cv': 3,\n",
       " 'method': 'sigmoid'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_lsvc.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Parameters\n",
    " \n",
    " |  penalty : string, 'l1' or 'l2' (default='l2')\n",
    " |      Specifies the norm used in the penalization. The 'l2'\n",
    " |      penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
    " |      vectors that are sparse.\n",
    " |  \n",
    " \n",
    " \n",
    " |  loss : string, 'hinge' or 'squared_hinge' (default='squared_hinge')\n",
    " |      Specifies the loss function. 'hinge' is the standard SVM loss\n",
    " |      (used e.g. by the SVC class) while 'squared_hinge' is the\n",
    " |      square of the hinge loss.\n",
    " \n",
    " \n",
    " |  dual : bool, (default=True)\n",
    " |      Select the algorithm to either solve the dual or primal\n",
    " |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
    " \n",
    " \n",
    " |  tol : float, optional (default=1e-4)\n",
    " |      Tolerance for stopping criteria.\n",
    " \n",
    " \n",
    " |  C : float, optional (default=1.0)\n",
    " |      Penalty parameter C of the error term.\n",
    " \n",
    " \n",
    " |  multi_class : string, 'ovr' or 'crammer_singer' (default='ovr')\n",
    " |      Determines the multi-class strategy if `y` contains more than\n",
    " |      two classes.\n",
    " |      ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
    " |      ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
    " |      While `crammer_singer` is interesting from a theoretical perspective\n",
    " |      as it is consistent, it is seldom used in practice as it rarely leads\n",
    " |      to better accuracy and is more expensive to compute.\n",
    " |      If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
    " |      will be ignored.\n",
    " \n",
    " \n",
    " |  fit_intercept : boolean, optional (default=True)\n",
    " |      Whether to calculate the intercept for this model. If set\n",
    " |      to false, no intercept will be used in calculations\n",
    " |      (i.e. data is expected to be already centered).\n",
    " \n",
    " \n",
    " |  intercept_scaling : float, optional (default=1)\n",
    " |      When self.fit_intercept is True, instance vector x becomes\n",
    " |      ``[x, self.intercept_scaling]``,\n",
    " |      i.e. a \"synthetic\" feature with constant value equals to\n",
    " |      intercept_scaling is appended to the instance vector.\n",
    " |      The intercept becomes intercept_scaling * synthetic feature weight\n",
    " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    " |      as all other features.\n",
    " |      To lessen the effect of regularization on synthetic feature weight\n",
    " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
    " \n",
    " \n",
    " |  class_weight : {dict, 'balanced'}, optional\n",
    " |      Set the parameter C of class i to ``class_weight[i]*C`` for\n",
    " |      SVC. If not given, all classes are supposed to have\n",
    " |      weight one.\n",
    " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
    " |      weights inversely proportional to class frequencies in the input data\n",
    " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
    " \n",
    " \n",
    " |  verbose : int, (default=0)\n",
    " |      Enable verbose output. Note that this setting takes advantage of a\n",
    " |      per-process runtime setting in liblinear that, if enabled, may not work\n",
    " |      properly in a multithreaded context.\n",
    " \n",
    " \n",
    " |  random_state : int, RandomState instance or None, optional (default=None)\n",
    " |      The seed of the pseudo random number generator to use when shuffling\n",
    " |      the data.  If int, random_state is the seed used by the random number\n",
    " |      generator; If RandomState instance, random_state is the random number\n",
    " |      generator; If None, the random number generator is the RandomState\n",
    " |      instance used by `np.random`.\n",
    " \n",
    " \n",
    " |  max_iter : int, (default=1000)\n",
    " |      The maximum number of iterations to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.21208098657e+17 \n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.21208098657e+17 \n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.21208098657e+17 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.21208098657e+17, score=0.8840566440686211, total=22.3min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.00778542013e+14 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.21208098657e+17, score=0.8177943609594873, total=22.5min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.00778542013e+14 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed: 22.6min\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.21208098657e+17, score=0.8700200891436635, total=22.6min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.00778542013e+14 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.00778542013e+14, score=0.8676939739830596, total=40.0min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=11334486647.2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.00778542013e+14, score=0.8831522714858911, total=41.2min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=11334486647.2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.00778542013e+14, score=0.8876377920585509, total=42.3min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=11334486647.2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=11334486647.2, score=0.8613303863263513, total=25.2min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=3.51965492688e+25 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed: 87.9min\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=11334486647.2, score=0.8810403140175892, total=24.8min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=3.51965492688e+25 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=11334486647.2, score=0.8418134881077068, total=24.7min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=3.51965492688e+25 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=3.51965492688e+25, score=0.8735556797658436, total=12.9min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=5.29541857683e+23 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=3.51965492688e+25, score=0.882629551122085, total=15.9min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=5.29541857683e+23 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=3.51965492688e+25, score=0.883657409654119, total=18.2min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=5.29541857683e+23 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed: 106.6min\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=5.29541857683e+23, score=0.8835153339382337, total=15.5min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=314704.081426 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=5.29541857683e+23, score=0.8729862955856605, total=12.9min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=314704.081426 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=5.29541857683e+23, score=0.8886630011417841, total=18.2min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=314704.081426 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=314704.081426, score=0.8644759524778105, total=10.0min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=6.69835040938e+15 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=314704.081426, score=0.869363875594573, total= 9.9min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=6.69835040938e+15 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=314704.081426, score=0.8276504835452378, total=10.4min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=6.69835040938e+15 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=6.69835040938e+15, score=0.8703198586002563, total=12.4min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.9591531793e+19 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed: 140.5min\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=6.69835040938e+15, score=0.8839019726354164, total=11.9min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.9591531793e+19 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=6.69835040938e+15, score=0.8127863398265177, total=23.0min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.9591531793e+19 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.9591531793e+19, score=0.8759856507864273, total=21.5min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=9.47617652913e+27 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.9591531793e+19, score=0.8800628027079634, total=21.1min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=9.47617652913e+27 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.9591531793e+19, score=0.8740074286192056, total=10.4min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=9.47617652913e+27 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=9.47617652913e+27, score=0.8870179021594196, total=14.9min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.07177346254 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=9.47617652913e+27, score=0.8828418273198472, total=15.8min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.07177346254 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=9.47617652913e+27, score=0.8754417665277526, total=13.4min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.07177346254 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.07177346254, score=0.8965921880965714, total=358.8min\n",
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.07177346254, score=0.9036990529221292, total=359.4min\n",
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.07177346254, score=0.8892588556681781, total=359.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  30 out of  30 | elapsed: 538.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear]It took: 553.1833333333333 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "lsvc = LinearSVC(verbose=10)\n",
    "\n",
    "cal_lsvc = CalibratedClassifierCV(base_estimator = lsvc,\n",
    "                                  cv = 3, # Also performs cross-validation if needed\n",
    "                                  method= \"sigmoid\") # We use sigmoid function to get probabilities\n",
    "\n",
    "params_space = {\n",
    "    \"base_estimator__penalty\":['l2'],\n",
    "    \"base_estimator__dual\":[False,True],\n",
    "    \"base_estimator__C\":np.logspace(0.1,100,base = 2, num=100)   \n",
    "}\n",
    "\n",
    "CAL_LSVC_search = RandomizedSearchCV(cal_lsvc,\n",
    "                                     param_distributions= params_space,\n",
    "                                     n_jobs=3, cv = 3, \n",
    "                                     n_iter = 10,verbose=10,\n",
    "                                     scoring=\"roc_auc\"  )\n",
    "\n",
    "CAL_LSVC_search.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator__C': 1.0717734625362931,\n",
       " 'base_estimator__dual': False,\n",
       " 'base_estimator__penalty': 'l2'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAL_LSVC_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8965166989711153"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAL_LSVC_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9509676015575883"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = CAL_LSVC_search.best_estimator_.predict_proba(X_train_trans_pl1)[:,1]\n",
    "roc_auc_score(y_train,probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"svc.pkl\",\"wb\") as f:\n",
    "    pickle.dump(CAL_LSVC_search.best_estimator_,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on an early submission\n",
    "\n",
    "Let's try to work on an early submission using the models we have in our hand.\n",
    "\n",
    "## Processing test data set using the pipeline locked down\n",
    "\n",
    "We need to first process the test data set using the same pipeline we trained to consistently extract the same features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label text features\n",
    "Text_features = [\"app\",\"device\",\"os\",\"channel\"]\n",
    "\n",
    "##############################################################\n",
    "# Define utility function to parse and process text features\n",
    "##############################################################\n",
    "# Note we avoid lambda functions since they don't pickle when we want to save the pipeline later   \n",
    "def column_text_processer_nolambda(df,text_columns = Text_features):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \"\"\"\"A function that will merge/join all text in a given row to make it ready for tokenization. \n",
    "    - This function should take care of converting missing values to empty strings. \n",
    "    - It should also convert the text to lowercase.\n",
    "    df= pandas dataframe\n",
    "    text_columns = names of the text features in df\n",
    "    \"\"\" \n",
    "    # Select only non-text columns that are in the df\n",
    "    text_data = df[text_columns]\n",
    "    \n",
    "    # Fill the missing values in text_data using empty strings\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    \n",
    "    # Concatenate feature name to each category encoding for each row\n",
    "    # E.g: encoding 3 at device column will read as device3 to make each encoding unique for a given feature\n",
    "    for col_index in list(text_data.columns):\n",
    "        text_data[col_index] = col_index + text_data[col_index].astype(str)\n",
    "    \n",
    "    # Join all the strings in a given row to make a vector\n",
    "    # text_vector = text_data.apply(lambda x: \" \".join(x), axis = 1)\n",
    "    text_vector = []\n",
    "    for index,rows in text_data.iterrows():\n",
    "        text_item = \" \".join(rows).lower()\n",
    "        text_vector.append(text_item)\n",
    "\n",
    "    # return text_vector as pd.Series object to enter the tokenization pipeline\n",
    "    return pd.Series(text_vector)\n",
    "\n",
    "#######################################################################\n",
    "# Define custom processing functions to add the log_total_clicks and \n",
    "# log_total_click_time features, and remove the unwanted base features\n",
    "#######################################################################\n",
    "def column_time_processer(X_train):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert click_time to datetime64 dtype \n",
    "    X_train.click_time = pd.to_datetime(X_train.click_time)\n",
    "\n",
    "    # Calculate the log_total_clicks for each ip and add as a new feature to temp_data\n",
    "    temp_data = pd.DataFrame(np.log(X_train.groupby([\"ip\"]).size()),\n",
    "                                    columns = [\"log_total_clicks\"]).reset_index()\n",
    "\n",
    "\n",
    "    # Calculate the log_total_click_time for each ip and add as a new feature to temp_data\n",
    "    # First define a function to process selected ip group \n",
    "    def get_log_total_click_time(group):\n",
    "        diff = (max(group.click_time) - min(group.click_time)).seconds\n",
    "        return np.log(diff+1)\n",
    "\n",
    "    # Then apply this function to each ip group and extract the total click time per ip group\n",
    "    log_time_frame = pd.DataFrame(X_train.groupby([\"ip\"]).apply(get_log_total_click_time),\n",
    "                                  columns=[\"log_total_click_time\"]).reset_index()\n",
    "\n",
    "    # Then add this new feature to the temp_data\n",
    "    temp_data = pd.merge(temp_data,log_time_frame, how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Combine temp_data with X_train to maintain X_train key order\n",
    "    temp_data = pd.merge(X_train,temp_data,how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Drop features that are not needed\n",
    "    temp_data = temp_data[[\"log_total_clicks\",\"log_total_click_time\"]]\n",
    "\n",
    "    # Return only the numeric features as a tensor to integrate into the numeric feature branch of the pipeline\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# We need to wrap these custom utility functions using FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# FunctionTransformer wrapper of utility functions to parse text and numeric features\n",
    "# Note how we avoid putting any arguments into column_text_processer or column_time_processer\n",
    "#############################################################################\n",
    "get_numeric_data = FunctionTransformer(func = column_time_processer, validate=False) \n",
    "get_text_data = FunctionTransformer(func = column_text_processer_nolambda,validate=False) \n",
    "\n",
    "#############################################################################\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "# #Note this regex will match either a whitespace or a punctuation to tokenize \n",
    "# the string vector on these preferences, in our case we only have white spaces in our text  \n",
    "#############################################################################\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
    "\n",
    "# Read the pipeline\n",
    "import pickle\n",
    "with open('userclick_pipeline1.pkl',\"rb\") as f: \n",
    "    userclick_pipeline1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this processing is computationally intense. We will try to perform this in chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(790469, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_set = pd.read_csv(\"test.csv\", dtype= \"str\",skiprows= 18 * (10**6) , nrows= 1000000)\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset has 18790469 samples. In order to avoid congesting the available memory, we will load the data in chunks of 1 million data points, this will give 18 chunks. As they ae loaded, we will process the chunks using the pipeline we trained to extract the same features. Recall that the sklearn pipeline returns a sparse matrix. We will aggregate the processed chunks of the test data set using the .vstack (\"vertical stack\") method of scipy.sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading first chunk so far it took : 0.03333333333333333 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing first chunk so far it took : 2.85 minutes.\n",
      "Shape of the tensor is : (1000000, 45753)\n",
      "Added chunk : 1 so far it took : 2.85 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 2 so far it took : 6.133333333333334 minutes.\n",
      "Shape of the tensor is : (2000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 3 so far it took : 10.55 minutes.\n",
      "Shape of the tensor is : (3000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 4 so far it took : 13.966666666666667 minutes.\n",
      "Shape of the tensor is : (4000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 5 so far it took : 17.25 minutes.\n",
      "Shape of the tensor is : (5000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 6 so far it took : 20.333333333333332 minutes.\n",
      "Shape of the tensor is : (6000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 7 so far it took : 23.55 minutes.\n",
      "Shape of the tensor is : (7000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 8 so far it took : 27.216666666666665 minutes.\n",
      "Shape of the tensor is : (8000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 9 so far it took : 30.716666666666665 minutes.\n",
      "Shape of the tensor is : (9000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 10 so far it took : 34.1 minutes.\n",
      "Shape of the tensor is : (10000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 11 so far it took : 37.81666666666667 minutes.\n",
      "Shape of the tensor is : (11000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 12 so far it took : 41.7 minutes.\n",
      "Shape of the tensor is : (12000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 13 so far it took : 45.61666666666667 minutes.\n",
      "Shape of the tensor is : (13000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 14 so far it took : 49.483333333333334 minutes.\n",
      "Shape of the tensor is : (14000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 15 so far it took : 53.666666666666664 minutes.\n",
      "Shape of the tensor is : (15000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 16 so far it took : 57.833333333333336 minutes.\n",
      "Shape of the tensor is : (16000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 17 so far it took : 62.15 minutes.\n",
      "Shape of the tensor is : (17000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 18 so far it took : 66.55 minutes.\n",
      "Shape of the tensor is : (18000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 19 so far it took : 70.73333333333333 minutes.\n",
      "Shape of the tensor is : (18790469, 45753)\n",
      "It took: 70.75 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "filename = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test.csv\"\n",
    "\n",
    "test_set = pd.read_csv(filename, dtype= \"str\", nrows= 1000000)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"Finished reading first chunk \" + \"so far it took : \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "\n",
    "\n",
    "test_proc_p11 = userclick_pipeline1.transform(test_set)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"Finished processing first chunk \" + \"so far it took : \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "\n",
    "print(\"Shape of the tensor is : \" + str(test_proc_p11.shape))\n",
    "\n",
    "column_names = test_set.columns\n",
    "\n",
    "skip = (10**6)+1\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"Added chunk : \" + \"1 \" + \"so far it took : \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "\n",
    "for i in range(2,20): \n",
    "    test_set = pd.read_csv(filename, dtype= \"str\",skiprows= skip, nrows= 1000000,names = list(column_names)) \n",
    "    temp_stack = userclick_pipeline1.transform(test_set)\n",
    "    test_proc_p11 = vstack([test_proc_p11,temp_stack]) \n",
    "    skip = skip + (10**6)\n",
    "    end = datetime.datetime.now()\n",
    "    process_time = end - start\n",
    "    print(\"Added chunk: \" + str(i) + \" so far it took : \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "    print(\"Shape of the tensor is : \" + str(test_proc_p11.shape))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# Note that writing files over 4GB through pickle stuck to a bug:\n",
    "# https://stackoverflow.com/questions/31468117/python-3-can-pickle-handle-byte-objects-larger-than-4gb\n",
    "# We need to write the sparse matrix in chunks,\n",
    "# break the bytes object into chunks of size 2**31 - 1 to get it in or out of the file.\n",
    "# Since the file is not feasible to fit into internal disk we need to save into an external space\n",
    "\n",
    "import pickle\n",
    "import os.path\n",
    "\n",
    "file_path = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl1.pkl\"\n",
    "n_bytes = 2**31\n",
    "max_bytes = 2**31 - 1\n",
    "\n",
    "\n",
    "## write in chunks\n",
    "bytes_out = pickle.dumps(test_proc_p11)\n",
    "with open(file_path, 'wb') as f_out:\n",
    "    for idx in range(0, n_bytes, max_bytes):\n",
    "        f_out.write(bytes_out[idx:idx+max_bytes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally saved the processed test data set into the external disk. Let's try to make predictions using the models we prepared so far:\n",
    "\n",
    "### Reading the large sparse matrix in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "pickle data was truncated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0ff267156c76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mbytes_in\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mf_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtest_proc_p11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m: pickle data was truncated"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl1.pkl\"\n",
    "n_bytes = 2**31\n",
    "max_bytes = 2**31 - 1\n",
    "\n",
    "## read in chunks\n",
    "bytes_in = bytearray(0)\n",
    "input_size = os.path.getsize(file_path)\n",
    "with open(file_path, 'rb') as f_in:\n",
    "    for idx in range(0, input_size, max_bytes):\n",
    "        bytes_in += f_in.read(max_bytes)\n",
    "test_proc_p11 = pickle.loads(bytes_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting this error and it looks like the reason is not clear. We need to find a different solution to save and load the processed test set sparse martix.\n",
    "\n",
    "### Saving and reading the large sparse matrix using scipy.sparse.save_npz and .load_npz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<18790469x45753 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 272396216 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_proc_p11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a sparse matrix to a file using .npz format\n",
    "# See: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.save_npz.html\n",
    "import scipy.sparse as sp\n",
    "sp.save_npz(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl1.npz\",\n",
    "            test_proc_p11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sparse matrix \n",
    "import scipy.sparse as sp\n",
    "test_proc_p11 = sp.load_npz(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl1.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is much better way of saving the sparse matrix. The resulting file is significantly smaller compared to pickle (~700MB v.s. 4GB). \n",
    "\n",
    "Let's use the processed test set and make predictions using the models we prepared so far to establish some benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions using the Ridge, Naive Bayes and SVC classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " '.Rhistory',\n",
       " '__pycache__',\n",
       " 'app_dummy.rds',\n",
       " 'channel_dummy.rds',\n",
       " 'device_dummy.rds',\n",
       " 'mnb.pkl',\n",
       " 'os_dummy.rds',\n",
       " 'Ridge_classifier.pkl',\n",
       " 'sample_submission.csv',\n",
       " 'SparseInteractions.py',\n",
       " 'svc.pkl',\n",
       " 'test.csv',\n",
       " 'test_processed.csv',\n",
       " 'train_sample.csv',\n",
       " 'User-click-detection-predictive-modeling.ipynb',\n",
       " 'userclick_pipeline1.pkl',\n",
       " 'UserClickDetectionPredictiveModeling.Rmd',\n",
       " 'X_train.pkl',\n",
       " 'X_train_trans_pl1.pkl',\n",
       " 'X_val1.pkl',\n",
       " 'X_val2.pkl',\n",
       " 'y_train.pkl',\n",
       " 'y_val1.pkl',\n",
       " 'y_val2.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sparse matrix.\n",
      "Loaded model objects.\n",
      "Completed Ridge predictions, it took: 1.7166666666666666 minutes.\n",
      "Completed MNB predictions, it took: 2.1 minutes.\n",
      "Completed SVC predictions, it took: 6.05 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Load the sparse matrix \n",
    "import scipy.sparse as sp\n",
    "test_proc_p11 = sp.load_npz(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl1.npz\")\n",
    "\n",
    "print(\"Loaded sparse matrix.\")\n",
    "\n",
    "# Load the model objects\n",
    "import pickle\n",
    "with open('Ridge_classifier.pkl',\"rb\") as f:\n",
    "    Ridge_classifier = pickle.load(f)\n",
    "\n",
    "import pickle\n",
    "with open('mnb.pkl',\"rb\") as f:\n",
    "    mnb = pickle.load(f)\n",
    "    \n",
    "import pickle\n",
    "with open('svc.pkl',\"rb\") as f:\n",
    "    svc = pickle.load(f) \n",
    "\n",
    "print(\"Loaded model objects.\") \n",
    "\n",
    "# Collect predictions\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# Ridge\n",
    "d = Ridge_classifier.decision_function(X= test_proc_p11) # Predict confidence scores for samples\n",
    "probs_Ridge = np.exp(d) / np.sum(np.exp(d)) # Use softmax to convert them probabilities between 0 and 1\n",
    "end = datetime.datetime.now()\n",
    "process_time = end-start\n",
    "print(\"Completed Ridge predictions, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "# mnb\n",
    "probs_mnb = mnb.predict_proba(test_proc_p11)[:,1]\n",
    "end1 = datetime.datetime.now()\n",
    "process_time = end1-end\n",
    "print(\"Completed MNB predictions, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "# SVC\n",
    "probs_svc = svc.predict_proba(test_proc_p11)[:,1]\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = end2-end1\n",
    "print(\"Completed SVC predictions, it took: \" + str((process_time.seconds)/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18790469,)\n",
      "(18790469,)\n",
      "(18790469,)\n"
     ]
    }
   ],
   "source": [
    "print(probs_Ridge.shape)\n",
    "print(probs_mnb.shape)\n",
    "print(probs_svc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A custom function to prepare submission files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the click_id from test set to disk to reuse\n",
    "import pandas as pd\n",
    "click_id = pd.read_csv(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test.csv\", \n",
    "                       dtype = \"str\").click_id.astype('int64')\n",
    "click_id.to_hdf(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/click_id.h5\",\n",
    "               \"click_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "click_id = pd.read_hdf(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/click_id.h5\")\n",
    "def prepare_submission(predictions,filename = \"new_submission\", click_id = click_id):\n",
    "    \"\"\"predictions: a list containing the predicted probabilities in the test set. \"\"\"\n",
    "    is_attributed = pd.Series(predictions)\n",
    "    submission_frame = pd.DataFrame()\n",
    "    submission_frame[\"click_id\"] = click_id\n",
    "    submission_frame[\"is_attributed\"] = is_attributed.apply(lambda x: format(x,\".9f\"))  # Reformat the probabilities upto the 9th decimal point\n",
    "    filename = filename + \".csv\"\n",
    "    submission_frame.to_csv(filename,index = False)\n",
    "    print(\"File saved as :\" + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare these 3 predictions for submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the same time built a dataframe with the predictions we keep accumulating so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_frames = pd.DataFrame()\n",
    "prediction_frames[\"click_id\"] = click_id\n",
    "prediction_frames[\"probs_Ridge\"] = probs_Ridge\n",
    "prediction_frames[\"probs_mnb\"] = probs_mnb\n",
    "prediction_frames[\"probs_svc\"] = probs_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>click_id</th>\n",
       "      <th>probs_Ridge</th>\n",
       "      <th>probs_mnb</th>\n",
       "      <th>probs_svc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.279898e-08</td>\n",
       "      <td>1.263286e-07</td>\n",
       "      <td>0.000795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.273787e-08</td>\n",
       "      <td>2.544532e-08</td>\n",
       "      <td>0.000837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.268285e-08</td>\n",
       "      <td>5.561252e-11</td>\n",
       "      <td>0.000602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5.270626e-08</td>\n",
       "      <td>4.261830e-11</td>\n",
       "      <td>0.001191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.275368e-08</td>\n",
       "      <td>4.717778e-11</td>\n",
       "      <td>0.001255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   click_id   probs_Ridge     probs_mnb  probs_svc\n",
       "0         0  5.279898e-08  1.263286e-07   0.000795\n",
       "1         1  5.273787e-08  2.544532e-08   0.000837\n",
       "2         2  5.268285e-08  5.561252e-11   0.000602\n",
       "3         3  5.270626e-08  4.261830e-11   0.001191\n",
       "4         4  5.275368e-08  4.717778e-11   0.001255"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_frames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18790469 entries, 0 to 18790468\n",
      "Data columns (total 4 columns):\n",
      "click_id       int64\n",
      "probs_Ridge    float64\n",
      "probs_mnb      float64\n",
      "probs_svc      float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 716.8 MB\n"
     ]
    }
   ],
   "source": [
    "prediction_frames.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>click_id</th>\n",
       "      <th>probs_Ridge</th>\n",
       "      <th>probs_mnb</th>\n",
       "      <th>probs_svc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18790464</th>\n",
       "      <td>18790464</td>\n",
       "      <td>5.366131e-08</td>\n",
       "      <td>1.249719e-05</td>\n",
       "      <td>0.001435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18790465</th>\n",
       "      <td>18790465</td>\n",
       "      <td>5.269257e-08</td>\n",
       "      <td>2.442719e-14</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18790466</th>\n",
       "      <td>18790467</td>\n",
       "      <td>5.267693e-08</td>\n",
       "      <td>2.790520e-10</td>\n",
       "      <td>0.000945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18790467</th>\n",
       "      <td>18790466</td>\n",
       "      <td>5.283781e-08</td>\n",
       "      <td>2.501097e-07</td>\n",
       "      <td>0.001181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18790468</th>\n",
       "      <td>18790468</td>\n",
       "      <td>5.241018e-08</td>\n",
       "      <td>2.759973e-17</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          click_id   probs_Ridge     probs_mnb  probs_svc\n",
       "18790464  18790464  5.366131e-08  1.249719e-05   0.001435\n",
       "18790465  18790465  5.269257e-08  2.442719e-14   0.000321\n",
       "18790466  18790467  5.267693e-08  2.790520e-10   0.000945\n",
       "18790467  18790466  5.283781e-08  2.501097e-07   0.001181\n",
       "18790468  18790468  5.241018e-08  2.759973e-17   0.000040"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_frames.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_frames.to_hdf(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/prediction_frames.h5\",\n",
    "                         'prediction_frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/Ridge_submission.csv\n",
      "File saved as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/MNB_submission.csv\n",
      "File saved as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/SVC_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Need to repeat these submission preparations!!\n",
    "import pandas as pd\n",
    "prediction_frames = pd.read_hdf(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/prediction_frames.h5\")\n",
    "\n",
    "prepare_submission(predictions= prediction_frames.probs_Ridge, \n",
    "                   filename= \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/Ridge_submission\")\n",
    "prepare_submission(predictions= prediction_frames.probs_mnb, \n",
    "                   filename= \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/MNB_submission\")\n",
    "prepare_submission(predictions= prediction_frames.probs_svc, \n",
    "                   filename= \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/SVC_submission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These preliminary submissions provided us some benchmark. The best model was MNB classifier, which scored 0.9425 ROC-AUC using the test data set. SVC and Ridge classifiers gave a score of 0.88. Most likely we won't use these classifiers in the future ensemble models.\n",
    "\n",
    "# Testing Bayesian Optimization to tune MNB classifier\n",
    "\n",
    "It looks like MNB model has the best performance amongst the 3 classifiers we have trained. Let's try Bayesian Optimization to see if we can further improve this classifier by performing this type of hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# Read the transformed features and target labels from the training set\n",
    "import pickle\n",
    "with open(\"X_train_trans_pl1.pkl\",\"rb\") as f:\n",
    "    X_train_trans_pl1 = pickle.load(f)\n",
    "with open(\"y_train.pkl\",\"rb\") as f:\n",
    "    y_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MultinomialNB in module sklearn.naive_bayes:\n",
      "\n",
      "class MultinomialNB(BaseDiscreteNB)\n",
      " |  Naive Bayes classifier for multinomial models\n",
      " |  \n",
      " |  The multinomial Naive Bayes classifier is suitable for classification with\n",
      " |  discrete features (e.g., word counts for text classification). The\n",
      " |  multinomial distribution normally requires integer feature counts. However,\n",
      " |  in practice, fractional counts such as tf-idf may also work.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  alpha : float, optional (default=1.0)\n",
      " |      Additive (Laplace/Lidstone) smoothing parameter\n",
      " |      (0 for no smoothing).\n",
      " |  \n",
      " |  fit_prior : boolean, optional (default=True)\n",
      " |      Whether to learn class prior probabilities or not.\n",
      " |      If false, a uniform prior will be used.\n",
      " |  \n",
      " |  class_prior : array-like, size (n_classes,), optional (default=None)\n",
      " |      Prior probabilities of the classes. If specified the priors are not\n",
      " |      adjusted according to the data.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_log_prior_ : array, shape (n_classes, )\n",
      " |      Smoothed empirical log probability for each class.\n",
      " |  \n",
      " |  intercept_ : property\n",
      " |      Mirrors ``class_log_prior_`` for interpreting MultinomialNB\n",
      " |      as a linear model.\n",
      " |  \n",
      " |  feature_log_prob_ : array, shape (n_classes, n_features)\n",
      " |      Empirical log probability of features\n",
      " |      given a class, ``P(x_i|y)``.\n",
      " |  \n",
      " |  coef_ : property\n",
      " |      Mirrors ``feature_log_prob_`` for interpreting MultinomialNB\n",
      " |      as a linear model.\n",
      " |  \n",
      " |  class_count_ : array, shape (n_classes,)\n",
      " |      Number of samples encountered for each class during fitting. This\n",
      " |      value is weighted by the sample weight when provided.\n",
      " |  \n",
      " |  feature_count_ : array, shape (n_classes, n_features)\n",
      " |      Number of samples encountered for each (class, feature)\n",
      " |      during fitting. This value is weighted by the sample weight when\n",
      " |      provided.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.random.randint(5, size=(6, 100))\n",
      " |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      " |  >>> from sklearn.naive_bayes import MultinomialNB\n",
      " |  >>> clf = MultinomialNB()\n",
      " |  >>> clf.fit(X, y)\n",
      " |  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      " |  >>> print(clf.predict(X[2:3]))\n",
      " |  [3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  For the rationale behind the names `coef_` and `intercept_`, i.e.\n",
      " |  naive Bayes as a linear classifier, see J. Rennie et al. (2003),\n",
      " |  Tackling the poor assumptions of naive Bayes text classifiers, ICML.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
      " |  Information Retrieval. Cambridge University Press, pp. 234-265.\n",
      " |  http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MultinomialNB\n",
      " |      BaseDiscreteNB\n",
      " |      BaseNB\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, alpha=1.0, fit_prior=True, class_prior=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDiscreteNB:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Naive Bayes classifier according to X, y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], (default=None)\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Incremental fit on a batch of samples.\n",
      " |      \n",
      " |      This method is expected to be called several times consecutively\n",
      " |      on different chunks of a dataset so as to implement out-of-core\n",
      " |      or online learning.\n",
      " |      \n",
      " |      This is especially useful when the whole dataset is too big to fit in\n",
      " |      memory at once.\n",
      " |      \n",
      " |      This method has some performance overhead hence it is better to call\n",
      " |      partial_fit on chunks of data that are as large as possible\n",
      " |      (as long as fitting in the memory budget) to hide the overhead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      classes : array-like, shape = [n_classes] (default=None)\n",
      " |          List of all the classes that can possibly appear in the y vector.\n",
      " |      \n",
      " |          Must be provided at the first call to partial_fit, can be omitted\n",
      " |          in subsequent calls.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] (default=None)\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseDiscreteNB:\n",
      " |  \n",
      " |  coef_\n",
      " |  \n",
      " |  intercept_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseNB:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape = [n_samples]\n",
      " |          Predicted target values for X\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log-probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the log-probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MultinomialNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our earlier best paremeters were:\n",
    "\n",
    "{'alpha': 0.20000000000000001,\n",
    " 'class_prior': [0.99831, 0.00169],\n",
    " 'fit_prior': True}\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return MEAN cross validated 'roc_auc' score from Support Vector Machine Classifier\n",
    "# Note that paameters we will optimize are called as generic arguments\n",
    "\n",
    "def mnbcv(alpha):\n",
    "    val = cross_val_score(MultinomialNB(alpha = alpha, fit_prior= True, class_prior= None),\n",
    "                         X_train_trans_pl1,y_train, 'roc_auc', cv=5, n_jobs = 3).mean()\n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |     alpha | \n",
      "    1 | 00m06s | \u001b[35m   0.94277\u001b[0m | \u001b[32m   0.2375\u001b[0m | \n",
      "    2 | 00m06s | \u001b[35m   0.94286\u001b[0m | \u001b[32m   0.2640\u001b[0m | \n",
      "    3 | 00m06s | \u001b[35m   0.94291\u001b[0m | \u001b[32m   0.2950\u001b[0m | \n",
      "    4 | 00m06s |    0.94264 |    0.2076 | \n",
      "    5 | 00m06s |    0.94288 |    0.2777 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |     alpha | \n",
      "    6 | 00m12s |    0.94283 |    0.2541 | \n",
      "    7 | 00m13s | \u001b[35m   0.94291\u001b[0m | \u001b[32m   0.2957\u001b[0m | \n",
      "    8 | 00m13s |    0.94288 |    0.2776 | \n",
      "    9 | 00m10s |    0.94290 |    0.2858 | \n",
      "   10 | 00m13s |    0.94287 |    0.2679 | \n",
      "   11 | 00m14s |    0.94277 |    0.2371 | \n",
      "   12 | 00m14s |    0.94285 |    0.2589 | \n",
      "   13 | 00m14s |    0.94281 |    0.2484 | \n",
      "   14 | 00m08s |    0.94262 |    0.2046 | \n",
      "   15 | 00m08s |    0.94290 |    0.2899 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "\n",
    "mnbBO = BayesianOptimization(f = mnbcv, \n",
    "                             pbounds =  {'alpha': (0.2, 0.3)},\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "\n",
    "mnbBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa=3, **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': {'params': [{'alpha': 0.25405897371886416},\n",
       "   {'alpha': 0.2957218967374855},\n",
       "   {'alpha': 0.27762561466946939},\n",
       "   {'alpha': 0.28580178477869017},\n",
       "   {'alpha': 0.26787478048201852},\n",
       "   {'alpha': 0.23709997691568072},\n",
       "   {'alpha': 0.25885144513692321},\n",
       "   {'alpha': 0.24839723916628981},\n",
       "   {'alpha': 0.20455275849005611},\n",
       "   {'alpha': 0.28992123720651092}],\n",
       "  'values': [0.9428309561856455,\n",
       "   0.94291343915763548,\n",
       "   0.94288420657109628,\n",
       "   0.94289622895547076,\n",
       "   0.94286742251853073,\n",
       "   0.94277156078197832,\n",
       "   0.94284550808716239,\n",
       "   0.9428131702186654,\n",
       "   0.94262378824705217,\n",
       "   0.94290411500060434]},\n",
       " 'max': {'max_params': {'alpha': 0.2957218967374855},\n",
       "  'max_val': 0.94291343915763548}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnbBO.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb roc score after bayesian optimization: 0.95235789271\n"
     ]
    }
   ],
   "source": [
    "# We re-fit the mnb using the optimized parameters\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "mnb_bopt = MultinomialNB(alpha=0.29572, class_prior= None, fit_prior= True)\n",
    "mnb_bopt.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "probs = mnb_bopt.predict_proba(X_train_trans_pl1)[:,1]\n",
    "\n",
    "print(\"mnb roc score after bayesian optimization: \" + str(roc_auc_score(y_train,probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the bayesian optimized model\n",
    "import pickle\n",
    "with open(\"mnb_bopt.pkl\",\"wb\") as f:\n",
    "    pickle.dump(mnb_bopt,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed test sparse matrix.\n",
      "Loaded model object.\n",
      "Completed MNB predictions, it took: 3.0833333333333335 minutes.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-8463bbe122f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m prepare_submission(predictions= probs_mnb_bopt, \n\u001b[0;32m---> 31\u001b[0;31m                    filename= \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/MNB_Bayesian_OPT_submission\")\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-b10e69050aae>\u001b[0m in \u001b[0;36mprepare_submission\u001b[0;34m(predictions, filename, click_id)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msubmission_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msubmission_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"click_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclick_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msubmission_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"is_attributed\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_attributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\".9f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reformat the probabilities upto the 9th decimal point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msubmission_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "# let's perform a prediction using the test set\n",
    "# Load the sparse matrix \n",
    "import scipy.sparse as sp\n",
    "test_proc_p11 = sp.load_npz(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl1.npz\")\n",
    "\n",
    "print(\"Loaded processed test sparse matrix.\")\n",
    "\n",
    "# Load the model object\n",
    "import pickle\n",
    "with open('mnb_bopt.pkl',\"rb\") as f:\n",
    "    mnb_bopt = pickle.load(f)\n",
    "\n",
    "print(\"Loaded model object.\") \n",
    "\n",
    "# Collect predictions\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "start = datetime.datetime.now()\n",
    "\n",
    "\n",
    "# mnb_bopt\n",
    "probs_mnb_bopt = mnb_bopt.predict_proba(test_proc_p11)[:,1]\n",
    "end1 = datetime.datetime.now()\n",
    "process_time = end1-start\n",
    "print(\"Completed MNB predictions, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/MNB_Bayesian_OPT_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare the submission file\n",
    "\n",
    "prepare_submission(predictions= probs_mnb_bopt, \n",
    "                   filename= \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/MNB_Bayesian_OPT_submission\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
