{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Through the initial analysis and exploration of the train_sample set (100K observations) we developed some initial expectations about the data. Here we are going to build a pipeline in order to effectively process data sets for modeling.\n",
    "\n",
    "# Planing for the Pipeline\n",
    "\n",
    "1. We will prepare 3 data sets from the training set. The training set provided is 200 million samples, we don't have computational power to use all of this data at the moment. Instead, we will extract 3 data sets each contain ~1 million observations. We will refer these sets as:\n",
    "\n",
    "    - training_set (1:100,000th rows of the original training set)\n",
    "    - validation_set1 (100,001:200,000th rows of the original training set)\n",
    "    - validation_set2 (200,001:300,000th rows of the original training set)\n",
    "\n",
    "2. Build the feature extraction and selection pipeline using the training set:\n",
    "\n",
    "    - Using the insights we obtained from data exploration, the following features will be used to create dummy variables: device, app, os and channel. We will perform this by converting these features to string, tokenization and selecting 300 best features.\n",
    "    - We will write custom processing functions to add the log_total_clicks and log_total_click_time features, and remove the unwanted base features\n",
    "    \n",
    "3. We will prepare the remainder of the pipeline to incorporate interaction terms and perform scaling and standardization.    \n",
    "\n",
    "## Prepare training and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training_set\n",
      "Finished validation_set1\n",
      "Finished validation_set2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "training_set = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/train.csv\",\n",
    "                           nrows=1000000,\n",
    "                           dtype = \"str\")\n",
    "print(\"Finished training_set\")\n",
    "validation_set1 = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/train.csv\",\n",
    "                           skiprows = 1000000,names = list(training_set.columns),\n",
    "                           nrows=1000000,\n",
    "                           dtype = \"str\")\n",
    "print(\"Finished validation_set1\")\n",
    "validation_set2 = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/train.csv\",\n",
    "                           skiprows = 2000000,names = list(training_set.columns),\n",
    "                           nrows=1000000,\n",
    "                           dtype = \"str\")\n",
    "print(\"Finished validation_set2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121848</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>105</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2698</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>259</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5729</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>237</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122891</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>280</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105433</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>245</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip app device  os channel           click_time attributed_time  \\\n",
       "0  121848  24      1  19     105  2017-11-06 16:21:51             NaN   \n",
       "1    2698  25      1  30     259  2017-11-06 16:21:51             NaN   \n",
       "2    5729   2      1  19     237  2017-11-06 16:21:51             NaN   \n",
       "3  122891   3      1  35     280  2017-11-06 16:21:51             NaN   \n",
       "4  105433  15      2  25     245  2017-11-06 16:21:51             NaN   \n",
       "\n",
       "  is_attributed  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 8 columns):\n",
      "ip                 1000000 non-null object\n",
      "app                1000000 non-null object\n",
      "device             1000000 non-null object\n",
      "os                 1000000 non-null object\n",
      "channel            1000000 non-null object\n",
      "click_time         1000000 non-null object\n",
      "attributed_time    1693 non-null object\n",
      "is_attributed      1000000 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 61.0+ MB\n"
     ]
    }
   ],
   "source": [
    "training_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote training_set to disk\n",
      "Wrote validation_set1 to disk\n",
      "Wrote validation_set2 to disk\n"
     ]
    }
   ],
   "source": [
    "# Let's save them for future easier individual loading\n",
    "training_set.to_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/training_set.csv\")\n",
    "print(\"Wrote training_set to disk\")\n",
    "\n",
    "validation_set1.to_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/validation_set1.csv\")\n",
    "print(\"Wrote validation_set1 to disk\")\n",
    "\n",
    "validation_set2.to_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/validation_set2.csv\")\n",
    "print(\"Wrote validation_set2 to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/training_set.csv\",\n",
    "                          index_col = 0, dtype = \"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83230</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:32:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17357</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:33:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35810</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:34:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45745</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>478</td>\n",
       "      <td>2017-11-06 14:34:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161007</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:35:08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip app device  os channel           click_time attributed_time  \\\n",
       "0   83230   3      1  13     379  2017-11-06 14:32:21             NaN   \n",
       "1   17357   3      1  19     379  2017-11-06 14:33:34             NaN   \n",
       "2   35810   3      1  13     379  2017-11-06 14:34:12             NaN   \n",
       "3   45745  14      1  13     478  2017-11-06 14:34:52             NaN   \n",
       "4  161007   3      1  13     379  2017-11-06 14:35:08             NaN   \n",
       "\n",
       "  is_attributed  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ip',\n",
       " 'app',\n",
       " 'device',\n",
       " 'os',\n",
       " 'channel',\n",
       " 'click_time',\n",
       " 'attributed_time',\n",
       " 'is_attributed']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(training_set.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate target labels from feature matrix \n",
    "\n",
    "We will seperate target labels from features for each of these data sets and pickle them for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "X_train = training_set.drop([\"is_attributed\",\"attributed_time\"], axis = 1)\n",
    "y_train = pd.to_numeric(training_set.is_attributed) \n",
    "\n",
    "X_train.to_pickle(\"X_train.pkl\")\n",
    "y_train.to_pickle(\"y_train.pkl\")\n",
    "\n",
    "X_val1 = validation_set1.drop([\"is_attributed\",\"attributed_time\"], axis = 1)\n",
    "y_val1 = pd.to_numeric(validation_set1.is_attributed) \n",
    "\n",
    "X_val1.to_pickle(\"X_val1.pkl\")\n",
    "y_val1.to_pickle(\"y_val1.pkl\")\n",
    "\n",
    "X_val2 = validation_set2.drop([\"is_attributed\",\"attributed_time\"], axis = 1)\n",
    "y_val2 = pd.to_numeric(validation_set2.is_attributed) \n",
    "\n",
    "X_val2.to_pickle(\"X_val2.pkl\")\n",
    "y_val2.to_pickle(\"y_val2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " '.Rhistory',\n",
       " 'app_dummy.rds',\n",
       " 'channel_dummy.rds',\n",
       " 'device_dummy.rds',\n",
       " 'os_dummy.rds',\n",
       " 'test_processed.csv',\n",
       " 'train_sample.csv',\n",
       " 'User-click-detection-predictive-modeling.ipynb',\n",
       " 'UserClickDetectionPredictiveModeling.Rmd',\n",
       " 'X_train.pkl',\n",
       " 'X_val1.pkl',\n",
       " 'X_val2.pkl',\n",
       " 'y_train.pkl',\n",
       " 'y_val1.pkl',\n",
       " 'y_val2.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the feature extraction and selection pipeline using the training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Read the pickled training set\n",
    "X_train = pd.read_pickle(\"X_train.pkl\")\n",
    "y_train = pd.read_pickle(\"y_train.pkl\")\n",
    "\n",
    "# Label text features\n",
    "Text_features = [\"app\",\"device\",\"os\",\"channel\"]\n",
    "\n",
    "##############################################################\n",
    "# Define utility function to parse and process text features\n",
    "##############################################################\n",
    "# Note we avoid lambda functions since they don't pickle when we want to save the pipeline later   \n",
    "def column_text_processer_nolambda(df,text_columns = Text_features):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \"\"\"\"A function that will merge/join all text in a given row to make it ready for tokenization. \n",
    "    - This function should take care of converting missing values to empty strings. \n",
    "    - It should also convert the text to lowercase.\n",
    "    df= pandas dataframe\n",
    "    text_columns = names of the text features in df\n",
    "    \"\"\" \n",
    "    # Select only non-text columns that are in the df\n",
    "    text_data = df[text_columns]\n",
    "    \n",
    "    # Fill the missing values in text_data using empty strings\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    \n",
    "    # Concatenate feature name to each category encoding for each row\n",
    "    # E.g: encoding 3 at device column will read as device3 to make each encoding unique for a given feature\n",
    "    for col_index in list(text_data.columns):\n",
    "        text_data[col_index] = col_index + text_data[col_index].astype(str)\n",
    "    \n",
    "    # Join all the strings in a given row to make a vector\n",
    "    # text_vector = text_data.apply(lambda x: \" \".join(x), axis = 1)\n",
    "    text_vector = []\n",
    "    for index,rows in text_data.iterrows():\n",
    "        text_item = \" \".join(rows).lower()\n",
    "        text_vector.append(text_item)\n",
    "\n",
    "    # return text_vector as pd.Series object to enter the tokenization pipeline\n",
    "    return pd.Series(text_vector)\n",
    "\n",
    "#######################################################################\n",
    "# Define custom processing functions to add the log_total_clicks and \n",
    "# log_total_click_time features, and remove the unwanted base features\n",
    "#######################################################################\n",
    "def column_time_processer(X_train):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert click_time to datetime64 dtype \n",
    "    X_train.click_time = pd.to_datetime(X_train.click_time)\n",
    "\n",
    "    # Calculate the log_total_clicks for each ip and add as a new feature to temp_data\n",
    "    temp_data = pd.DataFrame(np.log(X_train.groupby([\"ip\"]).size()),\n",
    "                                    columns = [\"log_total_clicks\"]).reset_index()\n",
    "\n",
    "\n",
    "    # Calculate the log_total_click_time for each ip and add as a new feature to temp_data\n",
    "    # First define a function to process selected ip group \n",
    "    def get_log_total_click_time(group):\n",
    "        diff = (max(group.click_time) - min(group.click_time)).seconds\n",
    "        return np.log(diff+1)\n",
    "\n",
    "    # Then apply this function to each ip group and extract the total click time per ip group\n",
    "    log_time_frame = pd.DataFrame(X_train.groupby([\"ip\"]).apply(get_log_total_click_time),\n",
    "                                  columns=[\"log_total_click_time\"]).reset_index()\n",
    "\n",
    "    # Then add this new feature to the temp_data\n",
    "    temp_data = pd.merge(temp_data,log_time_frame, how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Combine temp_data with X_train to maintain X_train key order\n",
    "    temp_data = pd.merge(X_train,temp_data,how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Drop features that are not needed\n",
    "    temp_data = temp_data[[\"log_total_clicks\",\"log_total_click_time\"]]\n",
    "\n",
    "    # Return only the numeric features as a tensor to integrate into the numeric feature branch of the pipeline\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# We need to wrap these custom utility functions using FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# FunctionTransformer wrapper of utility functions to parse text and numeric features\n",
    "# Note how we avoid putting any arguments into column_text_processer or column_time_processer\n",
    "#############################################################################\n",
    "get_numeric_data = FunctionTransformer(func = column_time_processer, validate=False) \n",
    "get_text_data = FunctionTransformer(func = column_text_processer_nolambda,validate=False) \n",
    "\n",
    "#############################################################################\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "# #Note this regex will match either a whitespace or a punctuation to tokenize \n",
    "# the string vector on these preferences, in our case we only have white spaces in our text  \n",
    "#############################################################################\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "\n",
    "###############################################\n",
    "# Construct our feature extraction pipeline\n",
    "###############################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, chi2 # We will use chi-squared as a scoring function to select features for classification\n",
    "from sklearn.metrics import auc\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module since it was saved into working directory as SparseInteractions.py\n",
    "\n",
    "userclick_pipeline1 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion(\n",
    "        # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "        # is the name of the transformer within the FeatureUnion\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "            ])), # End of: numeric_subpipeline\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                             ngram_range = (1,1),\n",
    "                                             non_negative=True, \n",
    "                                             norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                ('dim_red', SelectKBest(chi2,300)) # Step3: use dimension reduction to select 300 best features using chi2 as scoring function\n",
    "            ]))\n",
    "        ]\n",
    "        \n",
    "    )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "    \n",
    "    # Common steps:\n",
    "            \n",
    "    (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "    (\"scaler\",MaxAbsScaler()) # Scale the features between 0 and 1.       \n",
    "            \n",
    "])# End of: userclick_pipeline1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop the userclick_pipeline1 by using the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 3.3333333333333335 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "userclick_pipeline1.fit(X_train,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained our pipeline using the training set, we will pickle it and store for reuse. This will ensure the consistency every time we want to process a data set, and we will extract the same set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle and store the userclick_pipeline1\n",
    "import pickle\n",
    "with open(\"userclick_pipeline1.pkl\",\"wb\") as f:\n",
    "    pickle.dump(userclick_pipeline1,f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the features in the training set using the established pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the userclick_pipeline1 to work with\n",
    "import pickle\n",
    "with open(\"userclick_pipeline1.pkl\",\"rb\") as f:\n",
    "    userclick_pipeline1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 3.0833333333333335 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# Transform the training set features\n",
    "X_train_trans_pl1 = userclick_pipeline1.transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 45753)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_trans_pl1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csc.csc_matrix"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_trans_pl1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pickle and save this transformed version of the features from the training set. We spent about 3 minutes by training the pipeline and an additional 3 minutes for transforming the features.\n",
    "\n",
    "In the future, we will only use this pipeline with .transform method to process any datasets we would like to use in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed version of training set features\n",
    "import pickle\n",
    "with open(\"X_train_trans_pl1.pkl\",\"wb\") as f:\n",
    "    pickle.dump(X_train_trans_pl1,f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Regularized Linear Model for predicting true events\n",
    "\n",
    "We will start linear and train a Ridge classifier by setting the regularization parameter alpha to 0.5. We will see the untuned model performance first, then try to optimize the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 1.3333333333333333 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Read the transformed features and target labels from the training set\n",
    "import pickle\n",
    "with open(\"X_train_trans_pl1.pkl\",\"rb\") as f:\n",
    "    X_train_trans_pl1 = pickle.load(f)\n",
    "with open(\"y_train.pkl\",\"rb\") as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# Instantiate a Ridge classifier with a medium alpha \n",
    "Ridge = RidgeClassifier(alpha=0.5, random_state= 321)\n",
    "\n",
    "# Train the model\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "Ridge.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 0.016666666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Predict class labels using training set\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# Predict class probabilities\n",
    "# Note that there is no predict_proba on RidgeClassifier\n",
    "# So we use the trick in https://stackoverflow.com/questions/22538080/scikit-learn-ridge-classifier-extracting-class-probabilities \n",
    "\n",
    "d = Ridge.decision_function(X= X_train_trans_pl1) # Predict confidence scores for samples\n",
    "probs = np.exp(d) / np.sum(np.exp(d)) # Use softmax to convert them probabilities between 0 and 1\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94980780103952844"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate ROC score between the predicted probability and the observed target\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train,probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even this naive attempt gave us an ROC score of 0.949, Next, we will try to perform hyperparameter optimization to see where we can get further: \n",
    "\n",
    "## Hyperparameter optimization using Ridge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n",
      "[CV] alpha=0.86 ......................................................\n",
      "[CV] alpha=0.86 ......................................................\n",
      "[CV] alpha=0.25 ......................................................\n",
      "[CV] ............. alpha=0.86, score=0.8914263110278222, total=  37.6s\n",
      "[CV] alpha=0.25 ......................................................\n",
      "[CV] ............. alpha=0.86, score=0.8956513669854133, total= 1.1min\n",
      "[CV] alpha=0.45 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:  1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. alpha=0.25, score=0.884190883324053, total= 1.4min\n",
      "[CV] alpha=0.45 ......................................................\n",
      "[CV] ............. alpha=0.25, score=0.8914015886647204, total= 1.3min\n",
      "[CV] alpha=0.32 ......................................................\n",
      "[CV] ............. alpha=0.45, score=0.8886544167323791, total=  53.1s\n",
      "[CV] alpha=0.32 ......................................................\n",
      "[CV] ............. alpha=0.45, score=0.8941574912061789, total=  55.7s\n",
      "[CV] alpha=0.2 .......................................................\n",
      "[CV] ............. alpha=0.32, score=0.8875157179170297, total= 1.2min\n",
      "[CV] alpha=0.2 .......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:  3.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.32, score=0.8924351290802932, total= 1.2min\n",
      "[CV] alpha=0.06 ......................................................\n",
      "[CV] ............... alpha=0.2, score=0.882164720700341, total= 1.8min\n",
      "[CV] alpha=0.06 ......................................................\n",
      "[CV] .............. alpha=0.2, score=0.8905415407269656, total= 1.8min\n",
      "[CV] alpha=0.15 ......................................................\n",
      "[CV] ............. alpha=0.06, score=0.8786576036594884, total= 2.8min\n",
      "[CV] alpha=0.15 ......................................................\n",
      "[CV] .............. alpha=0.15, score=0.883062963076619, total= 1.8min\n",
      "[CV] alpha=0.82 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:  6.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.06, score=0.8759978611806225, total= 2.7min\n",
      "[CV] alpha=0.82 ......................................................\n",
      "[CV] ............. alpha=0.82, score=0.8913547259152571, total=  33.9s\n",
      "[CV] alpha=0.08 ......................................................\n",
      "[CV] ............. alpha=0.15, score=0.8884136760769671, total= 1.7min\n",
      "[CV] alpha=0.08 ......................................................\n",
      "[CV] ............. alpha=0.82, score=0.8954961726441188, total=  55.6s\n",
      "[CV] alpha=0.38 ......................................................\n",
      "[CV] ............. alpha=0.38, score=0.8886718559207138, total=  57.1s\n",
      "[CV] alpha=0.38 ......................................................\n",
      "[CV] .............. alpha=0.08, score=0.880409659046294, total= 2.2min\n",
      "[CV] alpha=0.69 ......................................................\n",
      "[CV] ............. alpha=0.38, score=0.8924743918088495, total= 1.0min\n",
      "[CV] alpha=0.69 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:  9.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.08, score=0.8830280871683156, total= 2.3min\n",
      "[CV] alpha=0.48 ......................................................\n",
      "[CV] ............. alpha=0.69, score=0.8903844868681919, total=  38.9s\n",
      "[CV] alpha=0.48 ......................................................\n",
      "[CV] .............. alpha=0.69, score=0.894370026181325, total=  40.2s\n",
      "[CV] alpha=0.95 ......................................................\n",
      "[CV] ............. alpha=0.48, score=0.8886348535251752, total=  48.8s\n",
      "[CV] alpha=0.95 ......................................................\n",
      "[CV] ............. alpha=0.48, score=0.8942835960807918, total=  50.9s\n",
      "[CV] alpha=0.42 ......................................................\n",
      "[CV] ............. alpha=0.95, score=0.8912950972786214, total=  29.2s\n",
      "[CV] alpha=0.42 ......................................................\n",
      "[CV] .............. alpha=0.42, score=0.888656706320648, total=  54.5s\n",
      "[CV] alpha=0.67 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed: 12.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.42, score=0.8933858578180848, total=  58.5s\n",
      "[CV] alpha=0.67 ......................................................\n",
      "[CV] ............. alpha=0.95, score=0.8976710720537423, total= 1.3min\n",
      "[CV] alpha=0.74 ......................................................\n",
      "[CV] ............. alpha=0.67, score=0.8904195284489833, total=  38.5s\n",
      "[CV] alpha=0.74 ......................................................\n",
      "[CV] ............. alpha=0.67, score=0.8943406596495505, total=  42.0s\n",
      "[CV] alpha=0.7 .......................................................\n",
      "[CV] ............. alpha=0.74, score=0.8902902991673008, total=  36.1s\n",
      "[CV] alpha=0.7 .......................................................\n",
      "[CV] ............. alpha=0.74, score=0.8951258526506547, total=  43.6s\n",
      "[CV] alpha=0.44 ......................................................\n",
      "[CV] .............. alpha=0.7, score=0.8904273622468626, total=  38.9s\n",
      "[CV] alpha=0.44 ......................................................\n",
      "[CV] .............. alpha=0.7, score=0.8944044414547937, total=  41.6s\n",
      "[CV] alpha=0.51 ......................................................\n",
      "[CV] ............. alpha=0.44, score=0.8886681708189335, total=  53.0s\n",
      "[CV] alpha=0.51 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed: 14.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.51, score=0.8885397528273091, total=  47.1s\n",
      "[CV] alpha=0.53 ......................................................\n",
      "[CV] ............. alpha=0.44, score=0.8940703814673404, total=  55.5s\n",
      "[CV] alpha=0.53 ......................................................\n",
      "[CV] ............. alpha=0.51, score=0.8943254518544484, total=  51.8s\n",
      "[CV] ............. alpha=0.53, score=0.8885548291037833, total=  47.6s\n",
      "[CV] ............. alpha=0.53, score=0.8943041770442346, total=  49.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  40 out of  40 | elapsed: 15.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done  40 out of  40 | elapsed: 15.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 16.15 minutes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "params_space = {\"alpha\": np.arange(0,1,0.01)}\n",
    "RidgeSearch = RandomizedSearchCV(Ridge,cv = 2,verbose=10,\n",
    "                                 n_iter=20,\n",
    "                                 n_jobs=3,\n",
    "                                 param_distributions=params_space,\n",
    "                                 random_state= 321,\n",
    "                                 scoring= \"roc_auc\")\n",
    "\n",
    "RidgeSearch.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89448307829020712"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RidgeSearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.95000000000000007}"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RidgeSearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC score is:0.949325548082\n",
      "It took: 1.05 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Use 'alpha': 0.95 to re-fit the Ridge classifier and calculate the performance\n",
    "# Read the transformed features and target labels from the training set\n",
    "import pickle\n",
    "with open(\"X_train_trans_pl1.pkl\",\"rb\") as f:\n",
    "    X_train_trans_pl1 = pickle.load(f)\n",
    "with open(\"y_train.pkl\",\"rb\") as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# Instantiate a Ridge classifier with a medium alpha \n",
    "Ridge = RidgeClassifier(alpha=0.95, random_state= 321)\n",
    "\n",
    "# Train the model\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "Ridge.fit(X_train_trans_pl1,y_train)\n",
    "d = Ridge.decision_function(X= X_train_trans_pl1) # Predict confidence scores for samples\n",
    "probs = np.exp(d) / np.sum(np.exp(d)) # Use softmax to convert them probabilities between 0 and 1\n",
    "\n",
    "# Calculate ROC score between the predicted probability and the observed target\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print( \"The ROC score is:\" + str(roc_auc_score(y_train,probs)))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could not improve the performance of the Ridge model using this approach. Let's save this model and continue to explore other types of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"Ridge_classifier.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Ridge, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the transformed features and target labels from the training set\n",
    "import pickle\n",
    "import numpy as np\n",
    "with open(\"X_train_trans_pl1.pkl\",\"rb\") as f:\n",
    "    X_train_trans_pl1 = pickle.load(f)\n",
    "with open(\"y_train.pkl\",\"rb\") as f:\n",
    "    y_train = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 0.016666666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probability predictions\n",
    "probs = mnb.predict_proba(X_train_trans_pl1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   6.32335454e-15],\n",
       "       [  1.00000000e+00,   7.52020576e-15],\n",
       "       [  1.00000000e+00,   1.22347330e-14],\n",
       "       ..., \n",
       "       [  1.00000000e+00,   1.74420279e-12],\n",
       "       [  1.00000000e+00,   5.90993405e-16],\n",
       "       [  1.00000000e+00,   5.74730378e-13]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs # We need the second column which is the probabilities for class label: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = probs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB roc score is: 0.946744947288\n"
     ]
    }
   ],
   "source": [
    "# Calculate the roc score for the training set\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"NB roc score is: \" + str(roc_auc_score(y_train,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our untuned classifier, which has similar performance to Ridge. Can we try to tune it to perform better? Looking into description, we find the following parameters that can be tuned:\n",
    "\n",
    "Parameters\n",
    "\n",
    " |  alpha : float, optional (default=1.0)\n",
    " |      Additive (Laplace/Lidstone) smoothing parameter\n",
    " |      (0 for no smoothing).\n",
    "\n",
    "\n",
    " |  fit_prior : boolean, optional (default=True)\n",
    " |      Whether to learn class prior probabilities or not.\n",
    " |      If false, a uniform prior will be used.\n",
    "\n",
    "\n",
    " |  class_prior : array-like, size (n_classes,), optional (default=None)\n",
    " |      Prior probabilities of the classes. If specified the priors are not\n",
    " |      adjusted according to the data.\n",
    " \n",
    "\n",
    "- We can make search across the alpha (0-1). \n",
    "- We will leave fit_prior = True\n",
    "- We have some idea about the probability of being in class 1, which is (sum(y_train)/len(y_train)) (0.00169) in our training set. Why don't we use this information in our hyperparameter search and see if it makes any difference.\n",
    "\n",
    "Since the classifier trained very fast, we can perform exhaustive GridSearch with 3-fold CV.\n",
    "\n",
    "## Hyperparameter optimization for Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 22 candidates, totalling 66 fits\n",
      "[CV] alpha=0.0, class_prior=None, fit_prior=True .....................\n",
      "[CV] alpha=0.0, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] alpha=0.0, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=None, fit_prior=True, score=0.8959370258094743, total=   1.6s\n",
      "[CV] alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=None, fit_prior=True, score=0.8868648185207342, total=   1.9s\n",
      "[CV] alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    5.2s\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=None, fit_prior=True, score=0.8857097885869714, total=   2.0s\n",
      "[CV] alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.8959370204907351, total=   1.7s\n",
      "[CV] alpha=0.1, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.886864834505243, total=   1.7s\n",
      "[CV] alpha=0.1, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.8857097859228865, total=   1.6s\n",
      "[CV] alpha=0.1, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.1, class_prior=None, fit_prior=True, score=0.9435606570419294, total=   1.5s\n",
      "[CV] alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    9.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.1, class_prior=None, fit_prior=True, score=0.9327180517680261, total=   1.7s\n",
      "[CV] alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.1, class_prior=None, fit_prior=True, score=0.9505139967923142, total=   1.5s\n",
      "[CV] alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9435606517231905, total=   1.5s\n",
      "[CV] alpha=0.2, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9327180624243653, total=   1.6s\n",
      "[CV] alpha=0.2, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9505139914641447, total=   1.7s\n",
      "[CV] alpha=0.2, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   13.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.2, class_prior=None, fit_prior=True, score=0.9444671084783707, total=   1.6s\n",
      "[CV] alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.2, class_prior=None, fit_prior=True, score=0.9326124714247604, total=   1.6s\n",
      "[CV] alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.2, class_prior=None, fit_prior=True, score=0.9520073601628835, total=   1.5s\n",
      "[CV] alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9444671270939577, total=   1.4s\n",
      "[CV] alpha=0.3, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9326125300346252, total=   1.7s\n",
      "[CV] alpha=0.3, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9520073814755616, total=   1.7s\n",
      "[CV] alpha=0.3, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.3, class_prior=None, fit_prior=True, score=0.9446672473134183, total=   1.6s\n",
      "[CV] alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   18.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.3, class_prior=None, fit_prior=True, score=0.931901677625326, total=   1.5s\n",
      "[CV] alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.3, class_prior=None, fit_prior=True, score=0.9524465345361612, total=   1.6s\n",
      "[CV] alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9446671994447662, total=   1.8s\n",
      "[CV] alpha=0.4, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9319016589767328, total=   1.7s\n",
      "[CV] alpha=0.4, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9524465345361612, total=   1.6s\n",
      "[CV] alpha=0.4, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.4, class_prior=None, fit_prior=True, score=0.9445732731709474, total=   1.4s\n",
      "[CV] alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.4, class_prior=None, fit_prior=True, score=0.9310744313527055, total=   1.4s\n",
      "[CV] alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   24.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.4, class_prior=None, fit_prior=True, score=0.9524166168642662, total=   1.4s\n",
      "[CV] alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9445732731709476, total=   1.4s\n",
      "[CV] alpha=0.5, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9310744313527057, total=   1.4s\n",
      "[CV] alpha=0.5, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9524165928875032, total=   1.5s\n",
      "[CV] alpha=0.5, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.5, class_prior=None, fit_prior=True, score=0.9442627013551642, total=   1.5s\n",
      "[CV] alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.5, class_prior=None, fit_prior=True, score=0.9302755229497157, total=   1.5s\n",
      "[CV] alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.5, class_prior=None, fit_prior=True, score=0.9521864532611158, total=   1.4s\n",
      "[CV] alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9442626960364251, total=   1.5s\n",
      "[CV] alpha=0.6, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9302755122933766, total=   1.6s\n",
      "[CV] alpha=0.6, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:   31.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9521864452688615, total=   1.5s\n",
      "[CV] alpha=0.6, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.6, class_prior=None, fit_prior=True, score=0.9438514218731604, total=   1.6s\n",
      "[CV] alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.6, class_prior=None, fit_prior=True, score=0.9294995310038618, total=   1.7s\n",
      "[CV] alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.6, class_prior=None, fit_prior=True, score=0.9518519294655212, total=   1.4s\n",
      "[CV] alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9438514218731604, total=   1.5s\n",
      "[CV] alpha=0.7, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9294995310038618, total=   1.6s\n",
      "[CV] alpha=0.7, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9518519294655213, total=   1.5s\n",
      "[CV] alpha=0.7, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.7, class_prior=None, fit_prior=True, score=0.9433635658444083, total=   1.6s\n",
      "[CV] alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.7, class_prior=None, fit_prior=True, score=0.9287724996077935, total=   1.5s\n",
      "[CV] alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   38.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.7, class_prior=None, fit_prior=True, score=0.9514674114552534, total=   1.6s\n",
      "[CV] alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9433635685037778, total=   1.6s\n",
      "[CV] alpha=0.8, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9287725022718782, total=   1.6s\n",
      "[CV] alpha=0.8, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9514674061270838, total=   1.8s\n",
      "[CV] alpha=0.8, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.8, class_prior=None, fit_prior=True, score=0.942827681601234, total=   2.0s\n",
      "[CV] alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.8, class_prior=None, fit_prior=True, score=0.9280890526384574, total=   2.1s\n",
      "[CV] alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.8, class_prior=None, fit_prior=True, score=0.9510515265109788, total=   1.8s\n",
      "[CV] alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9428276789418646, total=   1.6s\n",
      "[CV] alpha=0.9, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9280890286616944, total=   1.6s\n",
      "[CV] alpha=0.9, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9510515265109788, total=   1.6s\n",
      "[CV] alpha=0.9, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.9, class_prior=None, fit_prior=True, score=0.9422726206440957, total=   1.7s\n",
      "[CV] alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:   48.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.9, class_prior=None, fit_prior=True, score=0.9274512973869017, total=   1.7s\n",
      "[CV] alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.9, class_prior=None, fit_prior=True, score=0.9506482853129924, total=   1.5s\n",
      "[CV] alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9422726206440957, total=   1.9s\n",
      "[CV] alpha=1.0, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9274512973869015, total=   2.1s\n",
      "[CV] alpha=1.0, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9506482853129923, total=   2.2s\n",
      "[CV] alpha=1.0, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=1.0, class_prior=None, fit_prior=True, score=0.9417146237429526, total=   2.3s\n",
      "[CV] alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=1.0, class_prior=None, fit_prior=True, score=0.926872215938179, total=   2.0s\n",
      "[CV] alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=1.0, class_prior=None, fit_prior=True, score=0.9502528152502612, total=   1.7s\n",
      "[CV] alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9417146264023222, total=   1.6s\n",
      "[CV]  alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9268722079459248, total=   1.7s\n",
      "[CV]  alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9502528205784307, total=   1.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  66 out of  66 | elapsed:   58.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 1.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "params_space = {\n",
    "    \"alpha\":np.arange(0,1.1,0.1),\n",
    "    \"fit_prior\":[True],\n",
    "    \"class_prior\":[None,[1-0.00169,0.00169]]\n",
    "}\n",
    "\n",
    "MNBsearch = GridSearchCV(mnb,\n",
    "                         param_grid= params_space,\n",
    "                         scoring=\"roc_auc\",\n",
    "                         cv =3, n_jobs=3,verbose=10)\n",
    "\n",
    "MNBsearch.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.20000000000000001,\n",
       " 'class_prior': [0.99831, 0.00169],\n",
       " 'fit_prior': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNBsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94302901430616237"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNBsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95332437154555072"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "probs = MNBsearch.best_estimator_.predict_proba(X_train_trans_pl1)[:,1]\n",
    "roc_auc_score(y_train,probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the hyperparameter search improved MNB model performance. We will store the best estimator we obtained for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"mnb.pkl\",\"wb\") as f:\n",
    "    pickle.dump(MNBsearch.best_estimator_,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Support Vector Machine Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the transformed features and target labels from the training set\n",
    "import pickle\n",
    "import numpy as np\n",
    "with open(\"X_train_trans_pl1.pkl\",\"rb\") as f:\n",
    "    X_train_trans_pl1 = pickle.load(f)\n",
    "with open(\"y_train.pkl\",\"rb\") as f:\n",
    "    y_train = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear]It took: 2.25 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "# Note that we can't get probabilities directly from this LinearSVC function\n",
    "# We need to wrap into Calibrated Classifier \n",
    "# (see: https://stackoverflow.com/questions/35212213/sklearn-how-to-get-decision-probabilities-for-linearsvc-classifier)\n",
    "\n",
    "lsvc = LinearSVC(verbose=10)\n",
    "\n",
    "cal_lsvc = CalibratedClassifierCV(base_estimator = lsvc,\n",
    "                                  cv = 3, # Also performs cross-validation\n",
    "                                  method= \"sigmoid\") # We use sigmoid function to get probabilities\n",
    "\n",
    "cal_lsvc.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = cal_lsvc.predict_proba(X_train_trans_pl1)[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95099179905081954"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate ROC score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train,probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good start for an untuned classifier, let's try to perform hyperparameter search to see if we can improve this performance.\n",
    "\n",
    "## Hyperparameter tuning for SVC classifier\n",
    "\n",
    "We need to understand what SVC parameters we can tune in the context of calibrated classifier wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "      verbose=10),\n",
       " 'base_estimator__C': 1.0,\n",
       " 'base_estimator__class_weight': None,\n",
       " 'base_estimator__dual': True,\n",
       " 'base_estimator__fit_intercept': True,\n",
       " 'base_estimator__intercept_scaling': 1,\n",
       " 'base_estimator__loss': 'squared_hinge',\n",
       " 'base_estimator__max_iter': 1000,\n",
       " 'base_estimator__multi_class': 'ovr',\n",
       " 'base_estimator__penalty': 'l2',\n",
       " 'base_estimator__random_state': None,\n",
       " 'base_estimator__tol': 0.0001,\n",
       " 'base_estimator__verbose': 10,\n",
       " 'cv': 3,\n",
       " 'method': 'sigmoid'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_lsvc.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Parameters\n",
    " \n",
    " |  penalty : string, 'l1' or 'l2' (default='l2')\n",
    " |      Specifies the norm used in the penalization. The 'l2'\n",
    " |      penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
    " |      vectors that are sparse.\n",
    " |  \n",
    " \n",
    " \n",
    " |  loss : string, 'hinge' or 'squared_hinge' (default='squared_hinge')\n",
    " |      Specifies the loss function. 'hinge' is the standard SVM loss\n",
    " |      (used e.g. by the SVC class) while 'squared_hinge' is the\n",
    " |      square of the hinge loss.\n",
    " \n",
    " \n",
    " |  dual : bool, (default=True)\n",
    " |      Select the algorithm to either solve the dual or primal\n",
    " |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
    " \n",
    " \n",
    " |  tol : float, optional (default=1e-4)\n",
    " |      Tolerance for stopping criteria.\n",
    " \n",
    " \n",
    " |  C : float, optional (default=1.0)\n",
    " |      Penalty parameter C of the error term.\n",
    " \n",
    " \n",
    " |  multi_class : string, 'ovr' or 'crammer_singer' (default='ovr')\n",
    " |      Determines the multi-class strategy if `y` contains more than\n",
    " |      two classes.\n",
    " |      ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
    " |      ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
    " |      While `crammer_singer` is interesting from a theoretical perspective\n",
    " |      as it is consistent, it is seldom used in practice as it rarely leads\n",
    " |      to better accuracy and is more expensive to compute.\n",
    " |      If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
    " |      will be ignored.\n",
    " \n",
    " \n",
    " |  fit_intercept : boolean, optional (default=True)\n",
    " |      Whether to calculate the intercept for this model. If set\n",
    " |      to false, no intercept will be used in calculations\n",
    " |      (i.e. data is expected to be already centered).\n",
    " \n",
    " \n",
    " |  intercept_scaling : float, optional (default=1)\n",
    " |      When self.fit_intercept is True, instance vector x becomes\n",
    " |      ``[x, self.intercept_scaling]``,\n",
    " |      i.e. a \"synthetic\" feature with constant value equals to\n",
    " |      intercept_scaling is appended to the instance vector.\n",
    " |      The intercept becomes intercept_scaling * synthetic feature weight\n",
    " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    " |      as all other features.\n",
    " |      To lessen the effect of regularization on synthetic feature weight\n",
    " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
    " \n",
    " \n",
    " |  class_weight : {dict, 'balanced'}, optional\n",
    " |      Set the parameter C of class i to ``class_weight[i]*C`` for\n",
    " |      SVC. If not given, all classes are supposed to have\n",
    " |      weight one.\n",
    " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
    " |      weights inversely proportional to class frequencies in the input data\n",
    " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
    " \n",
    " \n",
    " |  verbose : int, (default=0)\n",
    " |      Enable verbose output. Note that this setting takes advantage of a\n",
    " |      per-process runtime setting in liblinear that, if enabled, may not work\n",
    " |      properly in a multithreaded context.\n",
    " \n",
    " \n",
    " |  random_state : int, RandomState instance or None, optional (default=None)\n",
    " |      The seed of the pseudo random number generator to use when shuffling\n",
    " |      the data.  If int, random_state is the seed used by the random number\n",
    " |      generator; If RandomState instance, random_state is the random number\n",
    " |      generator; If None, the random number generator is the RandomState\n",
    " |      instance used by `np.random`.\n",
    " \n",
    " \n",
    " |  max_iter : int, (default=1000)\n",
    " |      The maximum number of iterations to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.21208098657e+17 \n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.21208098657e+17 \n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.21208098657e+17 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.21208098657e+17, score=0.8840566440686211, total=22.3min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.00778542013e+14 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.21208098657e+17, score=0.8177943609594873, total=22.5min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.00778542013e+14 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed: 22.6min\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.21208098657e+17, score=0.8700200891436635, total=22.6min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.00778542013e+14 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.00778542013e+14, score=0.8676939739830596, total=40.0min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=11334486647.2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.00778542013e+14, score=0.8831522714858911, total=41.2min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=11334486647.2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.00778542013e+14, score=0.8876377920585509, total=42.3min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=11334486647.2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=11334486647.2, score=0.8613303863263513, total=25.2min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=3.51965492688e+25 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed: 87.9min\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=11334486647.2, score=0.8810403140175892, total=24.8min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=3.51965492688e+25 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=11334486647.2, score=0.8418134881077068, total=24.7min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=3.51965492688e+25 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=3.51965492688e+25, score=0.8735556797658436, total=12.9min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=5.29541857683e+23 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=3.51965492688e+25, score=0.882629551122085, total=15.9min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=5.29541857683e+23 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=3.51965492688e+25, score=0.883657409654119, total=18.2min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=5.29541857683e+23 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed: 106.6min\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=5.29541857683e+23, score=0.8835153339382337, total=15.5min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=314704.081426 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=5.29541857683e+23, score=0.8729862955856605, total=12.9min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=314704.081426 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=5.29541857683e+23, score=0.8886630011417841, total=18.2min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=314704.081426 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=314704.081426, score=0.8644759524778105, total=10.0min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=6.69835040938e+15 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=314704.081426, score=0.869363875594573, total= 9.9min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=6.69835040938e+15 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=314704.081426, score=0.8276504835452378, total=10.4min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=6.69835040938e+15 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=6.69835040938e+15, score=0.8703198586002563, total=12.4min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.9591531793e+19 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed: 140.5min\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=6.69835040938e+15, score=0.8839019726354164, total=11.9min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.9591531793e+19 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=6.69835040938e+15, score=0.8127863398265177, total=23.0min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.9591531793e+19 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.9591531793e+19, score=0.8759856507864273, total=21.5min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=9.47617652913e+27 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.9591531793e+19, score=0.8800628027079634, total=21.1min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=9.47617652913e+27 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=True, base_estimator__C=2.9591531793e+19, score=0.8740074286192056, total=10.4min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=9.47617652913e+27 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=9.47617652913e+27, score=0.8870179021594196, total=14.9min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.07177346254 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=9.47617652913e+27, score=0.8828418273198472, total=15.8min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.07177346254 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=9.47617652913e+27, score=0.8754417665277526, total=13.4min\n",
      "[CV] base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.07177346254 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.07177346254, score=0.8965921880965714, total=358.8min\n",
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.07177346254, score=0.9036990529221292, total=359.4min\n",
      "[LibLinear][LibLinear][LibLinear][CV]  base_estimator__penalty=l2, base_estimator__dual=False, base_estimator__C=1.07177346254, score=0.8892588556681781, total=359.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  30 out of  30 | elapsed: 538.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear]It took: 553.1833333333333 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "lsvc = LinearSVC(verbose=10)\n",
    "\n",
    "cal_lsvc = CalibratedClassifierCV(base_estimator = lsvc,\n",
    "                                  cv = 3, # Also performs cross-validation if needed\n",
    "                                  method= \"sigmoid\") # We use sigmoid function to get probabilities\n",
    "\n",
    "params_space = {\n",
    "    \"base_estimator__penalty\":['l2'],\n",
    "    \"base_estimator__dual\":[False,True],\n",
    "    \"base_estimator__C\":np.logspace(0.1,100,base = 2, num=100)   \n",
    "}\n",
    "\n",
    "CAL_LSVC_search = RandomizedSearchCV(cal_lsvc,\n",
    "                                     param_distributions= params_space,\n",
    "                                     n_jobs=3, cv = 3, \n",
    "                                     n_iter = 10,verbose=10,\n",
    "                                     scoring=\"roc_auc\"  )\n",
    "\n",
    "CAL_LSVC_search.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator__C': 1.0717734625362931,\n",
       " 'base_estimator__dual': False,\n",
       " 'base_estimator__penalty': 'l2'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAL_LSVC_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8965166989711153"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAL_LSVC_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9509676015575883"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = CAL_LSVC_search.best_estimator_.predict_proba(X_train_trans_pl1)[:,1]\n",
    "roc_auc_score(y_train,probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"svc.pkl\",\"wb\") as f:\n",
    "    pickle.dump(CAL_LSVC_search.best_estimator_,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on an early submission\n",
    "\n",
    "Let's try to work on an early submission using the models we have in our hand.\n",
    "\n",
    "## Processing test data set using the pipeline locked down\n",
    "\n",
    "We need to first process the test data set using the same pipeline we trained to consistently extract the same features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '.git',\n",
       " '.ipynb_checkpoints',\n",
       " '.Rhistory',\n",
       " '__pycache__',\n",
       " 'app_dummy.rds',\n",
       " 'channel_dummy.rds',\n",
       " 'device_dummy.rds',\n",
       " 'mnb.pkl',\n",
       " 'os_dummy.rds',\n",
       " 'Ridge_classifier.pkl',\n",
       " 'sample_submission.csv',\n",
       " 'SparseInteractions.py',\n",
       " 'svc.pkl',\n",
       " 'test_processed.csv',\n",
       " 'train_sample.csv',\n",
       " 'User-click-detection-predictive-modeling.ipynb',\n",
       " 'userclick_pipeline1.pkl',\n",
       " 'UserClickDetectionPredictiveModeling.Rmd',\n",
       " 'X_train.pkl',\n",
       " 'X_train_trans_pl1.pkl',\n",
       " 'X_val1.pkl',\n",
       " 'X_val2.pkl',\n",
       " 'y_train.pkl',\n",
       " 'y_val1.pkl',\n",
       " 'y_val2.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk : 1 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 2 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 3 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 4 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 5 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 6 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 7 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 8 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 9 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 10 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 11 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 12 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 13 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 14 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 15 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 16 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 17 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 18 so far it took : 0.06666666666666667 minutes.\n",
      "Added chunk : 19 so far it took : 0.06666666666666667 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "filename = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test.csv\"\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "test_set = pd.read_csv(filename, dtype= \"str\", nrows= 1000000)\n",
    "column_names = test_set.columns\n",
    "skip = (10**6)+1\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"Added chunk : \" + \"1 \" + \"so far it took : \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "\n",
    "for i in range(2,20):\n",
    "    temp = pd.read_csv(filename, dtype= \"str\",skiprows= skip,nrows= 1000000,names = list(column_names))\n",
    "    test_set = pd.concat([test_set,temp], axis=0)\n",
    "    end = datetime.datetime.now()\n",
    "    print(\"Added chunk : \" + str(i) + \" so far it took : \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "    skip += 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18790469, 7)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '.git',\n",
       " '.ipynb_checkpoints',\n",
       " '.Rhistory',\n",
       " '__pycache__',\n",
       " 'app_dummy.rds',\n",
       " 'channel_dummy.rds',\n",
       " 'device_dummy.rds',\n",
       " 'mnb.pkl',\n",
       " 'os_dummy.rds',\n",
       " 'Ridge_classifier.pkl',\n",
       " 'sample_submission.csv',\n",
       " 'SparseInteractions.py',\n",
       " 'svc.pkl',\n",
       " 'test.csv',\n",
       " 'test_processed.csv',\n",
       " 'train_sample.csv',\n",
       " 'User-click-detection-predictive-modeling.ipynb',\n",
       " 'userclick_pipeline1.pkl',\n",
       " 'UserClickDetectionPredictiveModeling.Rmd',\n",
       " 'X_train.pkl',\n",
       " 'X_train_trans_pl1.pkl',\n",
       " 'X_val1.pkl',\n",
       " 'X_val2.pkl',\n",
       " 'y_train.pkl',\n",
       " 'y_val1.pkl',\n",
       " 'y_val2.pkl']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission.csv\", dtype= \"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '2', '3', ..., '18790467', '18790466', '18790468'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.click_id[1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEQCAYAAAC3JB/WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJXcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e755048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(sample_submission.click_id.values.astype(\"int64\"),test_set.click_id[1:].values.astype(\"int64\"))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.all(sample_submission.click_id.values.astype(\"int64\") == test_set.click_id.values.astype(\"int64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(sample_submission.click_id.head(1000001).values == test_set.click_id.head(1000001).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000007    1000006\n",
       "1000008    1000008\n",
       "1000009    1000009\n",
       "1000010    1000010\n",
       "1000011    1000011\n",
       "Name: click_id, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.click_id[:1000012].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     1000006\n",
       "8     1000008\n",
       "9     1000009\n",
       "10    1000010\n",
       "11    1000011\n",
       "Name: click_id, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.click_id[:1000012].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  9,  8, 10, 11, 12, 13, 14, 15, 16, 20,\n",
       "       19, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       53, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 71, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.click_id[0:].values.astype(\"int64\")[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label text features\n",
    "Text_features = [\"app\",\"device\",\"os\",\"channel\"]\n",
    "\n",
    "##############################################################\n",
    "# Define utility function to parse and process text features\n",
    "##############################################################\n",
    "# Note we avoid lambda functions since they don't pickle when we want to save the pipeline later   \n",
    "def column_text_processer_nolambda(df,text_columns = Text_features):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \"\"\"\"A function that will merge/join all text in a given row to make it ready for tokenization. \n",
    "    - This function should take care of converting missing values to empty strings. \n",
    "    - It should also convert the text to lowercase.\n",
    "    df= pandas dataframe\n",
    "    text_columns = names of the text features in df\n",
    "    \"\"\" \n",
    "    # Select only non-text columns that are in the df\n",
    "    text_data = df[text_columns]\n",
    "    \n",
    "    # Fill the missing values in text_data using empty strings\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    \n",
    "    # Concatenate feature name to each category encoding for each row\n",
    "    # E.g: encoding 3 at device column will read as device3 to make each encoding unique for a given feature\n",
    "    for col_index in list(text_data.columns):\n",
    "        text_data[col_index] = col_index + text_data[col_index].astype(str)\n",
    "    \n",
    "    # Join all the strings in a given row to make a vector\n",
    "    # text_vector = text_data.apply(lambda x: \" \".join(x), axis = 1)\n",
    "    text_vector = []\n",
    "    for index,rows in text_data.iterrows():\n",
    "        text_item = \" \".join(rows).lower()\n",
    "        text_vector.append(text_item)\n",
    "\n",
    "    # return text_vector as pd.Series object to enter the tokenization pipeline\n",
    "    return pd.Series(text_vector)\n",
    "\n",
    "#######################################################################\n",
    "# Define custom processing functions to add the log_total_clicks and \n",
    "# log_total_click_time features, and remove the unwanted base features\n",
    "#######################################################################\n",
    "def column_time_processer(X_train):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert click_time to datetime64 dtype \n",
    "    X_train.click_time = pd.to_datetime(X_train.click_time)\n",
    "\n",
    "    # Calculate the log_total_clicks for each ip and add as a new feature to temp_data\n",
    "    temp_data = pd.DataFrame(np.log(X_train.groupby([\"ip\"]).size()),\n",
    "                                    columns = [\"log_total_clicks\"]).reset_index()\n",
    "\n",
    "\n",
    "    # Calculate the log_total_click_time for each ip and add as a new feature to temp_data\n",
    "    # First define a function to process selected ip group \n",
    "    def get_log_total_click_time(group):\n",
    "        diff = (max(group.click_time) - min(group.click_time)).seconds\n",
    "        return np.log(diff+1)\n",
    "\n",
    "    # Then apply this function to each ip group and extract the total click time per ip group\n",
    "    log_time_frame = pd.DataFrame(X_train.groupby([\"ip\"]).apply(get_log_total_click_time),\n",
    "                                  columns=[\"log_total_click_time\"]).reset_index()\n",
    "\n",
    "    # Then add this new feature to the temp_data\n",
    "    temp_data = pd.merge(temp_data,log_time_frame, how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Combine temp_data with X_train to maintain X_train key order\n",
    "    temp_data = pd.merge(X_train,temp_data,how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Drop features that are not needed\n",
    "    temp_data = temp_data[[\"log_total_clicks\",\"log_total_click_time\"]]\n",
    "\n",
    "    # Return only the numeric features as a tensor to integrate into the numeric feature branch of the pipeline\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# We need to wrap these custom utility functions using FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# FunctionTransformer wrapper of utility functions to parse text and numeric features\n",
    "# Note how we avoid putting any arguments into column_text_processer or column_time_processer\n",
    "#############################################################################\n",
    "get_numeric_data = FunctionTransformer(func = column_time_processer, validate=False) \n",
    "get_text_data = FunctionTransformer(func = column_text_processer_nolambda,validate=False) \n",
    "\n",
    "#############################################################################\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "# #Note this regex will match either a whitespace or a punctuation to tokenize \n",
    "# the string vector on these preferences, in our case we only have white spaces in our text  \n",
    "#############################################################################\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
    "\n",
    "# Read the pipeline\n",
    "import pickle\n",
    "with open('userclick_pipeline1.pkl',\"rb\") as f: \n",
    "    userclick_pipeline1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this processing is computationally intense. We will try to perform this in chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(790469, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_set = pd.read_csv(\"test.csv\", dtype= \"str\",skiprows= 18 * (10**6) , nrows= 1000000)\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset has 18790469 samples. In order to avoid congesting the available memory, we will load the data in chunks of 1 million data points, this will give 18 chunks. As they ae loaded, we will process the chunks using the pipeline we trained to extract the same features. Recall that the sklearn pipeline returns a sparse matrix. We will aggregate the processed chunks of the test data set using the .vstack (\"vertical stack\") method of scipy.sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading first chunk so far it took : 0.03333333333333333 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing first chunk so far it took : 2.85 minutes.\n",
      "Shape of the tensor is : (1000000, 45753)\n",
      "Added chunk : 1 so far it took : 2.85 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 2 so far it took : 6.133333333333334 minutes.\n",
      "Shape of the tensor is : (2000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 3 so far it took : 10.55 minutes.\n",
      "Shape of the tensor is : (3000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 4 so far it took : 13.966666666666667 minutes.\n",
      "Shape of the tensor is : (4000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 5 so far it took : 17.25 minutes.\n",
      "Shape of the tensor is : (5000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 6 so far it took : 20.333333333333332 minutes.\n",
      "Shape of the tensor is : (6000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 7 so far it took : 23.55 minutes.\n",
      "Shape of the tensor is : (7000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 8 so far it took : 27.216666666666665 minutes.\n",
      "Shape of the tensor is : (8000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 9 so far it took : 30.716666666666665 minutes.\n",
      "Shape of the tensor is : (9000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 10 so far it took : 34.1 minutes.\n",
      "Shape of the tensor is : (10000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 11 so far it took : 37.81666666666667 minutes.\n",
      "Shape of the tensor is : (11000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 12 so far it took : 41.7 minutes.\n",
      "Shape of the tensor is : (12000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 13 so far it took : 45.61666666666667 minutes.\n",
      "Shape of the tensor is : (13000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 14 so far it took : 49.483333333333334 minutes.\n",
      "Shape of the tensor is : (14000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 15 so far it took : 53.666666666666664 minutes.\n",
      "Shape of the tensor is : (15000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 16 so far it took : 57.833333333333336 minutes.\n",
      "Shape of the tensor is : (16000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 17 so far it took : 62.15 minutes.\n",
      "Shape of the tensor is : (17000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 18 so far it took : 66.55 minutes.\n",
      "Shape of the tensor is : (18000000, 45753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk: 19 so far it took : 70.73333333333333 minutes.\n",
      "Shape of the tensor is : (18790469, 45753)\n",
      "It took: 70.75 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "filename = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test.csv\"\n",
    "\n",
    "test_set = pd.read_csv(filename, dtype= \"str\", nrows= 1000000)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"Finished reading first chunk \" + \"so far it took : \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "\n",
    "\n",
    "test_proc_p11 = userclick_pipeline1.transform(test_set)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"Finished processing first chunk \" + \"so far it took : \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "\n",
    "print(\"Shape of the tensor is : \" + str(test_proc_p11.shape))\n",
    "\n",
    "column_names = test_set.columns\n",
    "\n",
    "skip = (10**6)+1\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"Added chunk : \" + \"1 \" + \"so far it took : \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "\n",
    "for i in range(2,20): \n",
    "    test_set = pd.read_csv(filename, dtype= \"str\",skiprows= skip, nrows= 1000000,names = list(column_names)) \n",
    "    temp_stack = userclick_pipeline1.transform(test_set)\n",
    "    test_proc_p11 = vstack([test_proc_p11,temp_stack]) \n",
    "    skip = skip + (10**6)\n",
    "    end = datetime.datetime.now()\n",
    "    process_time = end - start\n",
    "    print(\"Added chunk: \" + str(i) + \" so far it took : \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "    print(\"Shape of the tensor is : \" + str(test_proc_p11.shape))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# Note that writing files over 4GB through pickle stuck to a bug:\n",
    "# https://stackoverflow.com/questions/31468117/python-3-can-pickle-handle-byte-objects-larger-than-4gb\n",
    "# We need to write the sparse matrix in chunks,\n",
    "# break the bytes object into chunks of size 2**31 - 1 to get it in or out of the file.\n",
    "# Since the file is not feasible to fit into internal disk we need to save into an external space\n",
    "\n",
    "import pickle\n",
    "import os.path\n",
    "\n",
    "file_path = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl1.pkl\"\n",
    "n_bytes = 2**31\n",
    "max_bytes = 2**31 - 1\n",
    "\n",
    "\n",
    "## write in chunks\n",
    "bytes_out = pickle.dumps(test_proc_p11)\n",
    "with open(file_path, 'wb') as f_out:\n",
    "    for idx in range(0, n_bytes, max_bytes):\n",
    "        f_out.write(bytes_out[idx:idx+max_bytes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally saved the processed test data set into the external disk. Let's try to make predictions using the models we prepared so far:\n",
    "\n",
    "### Reading the large sparse matrix in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "pickle data was truncated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0ff267156c76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mbytes_in\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mf_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtest_proc_p11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m: pickle data was truncated"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl1.pkl\"\n",
    "n_bytes = 2**31\n",
    "max_bytes = 2**31 - 1\n",
    "\n",
    "## read in chunks\n",
    "bytes_in = bytearray(0)\n",
    "input_size = os.path.getsize(file_path)\n",
    "with open(file_path, 'rb') as f_in:\n",
    "    for idx in range(0, input_size, max_bytes):\n",
    "        bytes_in += f_in.read(max_bytes)\n",
    "test_proc_p11 = pickle.loads(bytes_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting this error and it looks like the reason is not clear. We need to find a different solution to save and load the processed test set sparse martix.\n",
    "\n",
    "### Saving and reading the large sparse matrix using scipy.sparse.save_npz and .load_npz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<18790469x45753 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 272396216 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_proc_p11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a sparse matrix to a file using .npz format\n",
    "# See: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.save_npz.html\n",
    "import scipy.sparse as sp\n",
    "sp.save_npz(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl1.npz\",\n",
    "            test_proc_p11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sparse matrix \n",
    "import scipy.sparse as sp\n",
    "test_proc_p11 = sp.load_npz(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl1.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is much better way of saving the sparse matrix. The resulting file is significantly smaller compared to pickle (~700MB v.s. 4GB). \n",
    "\n",
    "Let's use the processed test set and make predictions using the models we prepared so far to establish some benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions using the Ridge, Naive Bayes and SVC classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " '.Rhistory',\n",
       " '__pycache__',\n",
       " 'app_dummy.rds',\n",
       " 'channel_dummy.rds',\n",
       " 'device_dummy.rds',\n",
       " 'mnb.pkl',\n",
       " 'os_dummy.rds',\n",
       " 'Ridge_classifier.pkl',\n",
       " 'sample_submission.csv',\n",
       " 'SparseInteractions.py',\n",
       " 'svc.pkl',\n",
       " 'test.csv',\n",
       " 'test_processed.csv',\n",
       " 'train_sample.csv',\n",
       " 'User-click-detection-predictive-modeling.ipynb',\n",
       " 'userclick_pipeline1.pkl',\n",
       " 'UserClickDetectionPredictiveModeling.Rmd',\n",
       " 'X_train.pkl',\n",
       " 'X_train_trans_pl1.pkl',\n",
       " 'X_val1.pkl',\n",
       " 'X_val2.pkl',\n",
       " 'y_train.pkl',\n",
       " 'y_val1.pkl',\n",
       " 'y_val2.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model objects\n",
    "import pickle\n",
    "with open('Ridge_classifier.pkl',\"rb\") as f:\n",
    "    Ridge_classifier = pickle.load(f)\n",
    "\n",
    "import pickle\n",
    "with open('mnb.pkl',\"rb\") as f:\n",
    "    mnb = pickle.load(f)\n",
    "    \n",
    "import pickle\n",
    "with open('svc.pkl',\"rb\") as f:\n",
    "    svc = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sparse matrix.\n",
      "Loaded model objects.\n",
      "Completed Ridge predictions, it took: 1.9333333333333333 minutes.\n",
      "Completed MNB predictions, it took: 2.15 minutes.\n",
      "Completed SVC predictions, it took: 7.083333333333333 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Load the sparse matrix \n",
    "import scipy.sparse as sp\n",
    "test_proc_p11 = sp.load_npz(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl1.npz\")\n",
    "\n",
    "print(\"Loaded sparse matrix.\")\n",
    "\n",
    "# Load the model objects\n",
    "import pickle\n",
    "with open('Ridge_classifier.pkl',\"rb\") as f:\n",
    "    Ridge_classifier = pickle.load(f)\n",
    "\n",
    "import pickle\n",
    "with open('mnb.pkl',\"rb\") as f:\n",
    "    mnb = pickle.load(f)\n",
    "    \n",
    "import pickle\n",
    "with open('svc.pkl',\"rb\") as f:\n",
    "    svc = pickle.load(f) \n",
    "\n",
    "print(\"Loaded model objects.\")    \n",
    "    \n",
    "# Collect predictions\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# Ridge\n",
    "d = Ridge_classifier.decision_function(X= test_proc_p11) # Predict confidence scores for samples\n",
    "probs_Ridge = np.exp(d) / np.sum(np.exp(d)) # Use softmax to convert them probabilities between 0 and 1\n",
    "end = datetime.datetime.now()\n",
    "process_time = end-start\n",
    "print(\"Completed Ridge predictions, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "# mnb\n",
    "probs_mnb = mnb.predict_proba(test_proc_p11)[:,1]\n",
    "end1 = datetime.datetime.now()\n",
    "process_time = end1-end\n",
    "print(\"Completed MNB predictions, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "# SVC\n",
    "probs_svc = svc.predict_proba(test_proc_p11)[:,1]\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = end2-end1\n",
    "print(\"Completed SVC predictions, it took: \" + str((process_time.seconds)/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18790470,)\n",
      "(18790470,)\n",
      "(18790470,)\n"
     ]
    }
   ],
   "source": [
    "print(probs_Ridge.shape)\n",
    "print(probs_mnb.shape)\n",
    "print(probs_svc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A custom function to prepare submission files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_submission(predictions,filename = \"new_submission\"):\n",
    "    import pandas as pd\n",
    "    \"\"\"predictions: a list containing the predicted probabilities in the test set. \"\"\"\n",
    "    is_attributed = predictions\n",
    "    click_id = np.array(list(range(1,len(is_attributed)+1)))\n",
    "    submission_frame = pd.DataFrame()\n",
    "    submission_frame[\"click_id\"] = click_id\n",
    "    submission_frame[\"is_attributed\"] = is_attributed\n",
    "    filename = filename + \".csv\"\n",
    "    submission_frame.to_csv(filename,index = False)\n",
    "    print(\"File saved as :\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>click_id</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.273786e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5.268284e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5.270625e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5.275367e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.269636e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   click_id  is_attributed\n",
       "0         1   5.273786e-08\n",
       "1         2   5.268284e-08\n",
       "2         3   5.270625e-08\n",
       "3         4   5.275367e-08\n",
       "4         5   5.269636e-08"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare these 3 predictions for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/Ridge_submission.csv\n",
      "File saved as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/MNB_submission.csv\n",
      "File saved as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/SVC_submission.csv\n"
     ]
    }
   ],
   "source": [
    "prepare_submission(predictions= probs_Ridge[1:], filename= \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/Ridge_submission\")\n",
    "prepare_submission(predictions= probs_mnb[1:], filename= \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/MNB_submission\")\n",
    "prepare_submission(predictions= probs_svc[1:], filename= \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/SVC_submission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the same time built a dataframe with the predictions we keep accumulating so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_frames = pd.DataFrame()\n",
    "prediction_frames[\"click_id\"] = np.array(list(range(1,len(probs_Ridge))))\n",
    "prediction_frames[\"probs_Ridge\"] = probs_Ridge[1:]\n",
    "prediction_frames[\"probs_mnb\"] = probs_mnb[1:]\n",
    "prediction_frames[\"probs_svc\"] = probs_svc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>click_id</th>\n",
       "      <th>probs_Ridge</th>\n",
       "      <th>probs_mnb</th>\n",
       "      <th>probs_svc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.273786e-08</td>\n",
       "      <td>2.544532e-08</td>\n",
       "      <td>0.000837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5.268284e-08</td>\n",
       "      <td>5.561252e-11</td>\n",
       "      <td>0.000602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5.270625e-08</td>\n",
       "      <td>4.261830e-11</td>\n",
       "      <td>0.001191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5.275367e-08</td>\n",
       "      <td>4.717778e-11</td>\n",
       "      <td>0.001255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.269636e-08</td>\n",
       "      <td>2.483058e-09</td>\n",
       "      <td>0.001240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   click_id   probs_Ridge     probs_mnb  probs_svc\n",
       "0         1  5.273786e-08  2.544532e-08   0.000837\n",
       "1         2  5.268284e-08  5.561252e-11   0.000602\n",
       "2         3  5.270625e-08  4.261830e-11   0.001191\n",
       "3         4  5.275367e-08  4.717778e-11   0.001255\n",
       "4         5  5.269636e-08  2.483058e-09   0.001240"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_frames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>click_id</th>\n",
       "      <th>probs_Ridge</th>\n",
       "      <th>probs_mnb</th>\n",
       "      <th>probs_svc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18790464</th>\n",
       "      <td>18790465</td>\n",
       "      <td>5.366130e-08</td>\n",
       "      <td>1.249719e-05</td>\n",
       "      <td>0.001435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18790465</th>\n",
       "      <td>18790466</td>\n",
       "      <td>5.269256e-08</td>\n",
       "      <td>2.442719e-14</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18790466</th>\n",
       "      <td>18790467</td>\n",
       "      <td>5.267692e-08</td>\n",
       "      <td>2.790520e-10</td>\n",
       "      <td>0.000945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18790467</th>\n",
       "      <td>18790468</td>\n",
       "      <td>5.283780e-08</td>\n",
       "      <td>2.501097e-07</td>\n",
       "      <td>0.001181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18790468</th>\n",
       "      <td>18790469</td>\n",
       "      <td>5.241017e-08</td>\n",
       "      <td>2.759973e-17</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          click_id   probs_Ridge     probs_mnb  probs_svc\n",
       "18790464  18790465  5.366130e-08  1.249719e-05   0.001435\n",
       "18790465  18790466  5.269256e-08  2.442719e-14   0.000321\n",
       "18790466  18790467  5.267692e-08  2.790520e-10   0.000945\n",
       "18790467  18790468  5.283780e-08  2.501097e-07   0.001181\n",
       "18790468  18790469  5.241017e-08  2.759973e-17   0.000040"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_frames.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_frames.to_hdf(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/prediction_frames.h5\",\n",
    "                         'prediction_frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method to_hdf in module pandas.core.generic:\n",
      "\n",
      "to_hdf(path_or_buf, key, **kwargs) method of pandas.core.frame.DataFrame instance\n",
      "    Write the contained data to an HDF5 file using HDFStore.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path_or_buf : the path (string) or HDFStore object\n",
      "    key : string\n",
      "        identifier for the group in the store\n",
      "    mode : optional, {'a', 'w', 'r+'}, default 'a'\n",
      "    \n",
      "      ``'w'``\n",
      "          Write; a new file is created (an existing file with the same\n",
      "          name would be deleted).\n",
      "      ``'a'``\n",
      "          Append; an existing file is opened for reading and writing,\n",
      "          and if the file does not exist it is created.\n",
      "      ``'r+'``\n",
      "          It is similar to ``'a'``, but the file must already exist.\n",
      "    format : 'fixed(f)|table(t)', default is 'fixed'\n",
      "        fixed(f) : Fixed format\n",
      "                   Fast writing/reading. Not-appendable, nor searchable\n",
      "        table(t) : Table format\n",
      "                   Write as a PyTables Table structure which may perform\n",
      "                   worse but allow more flexible operations like searching\n",
      "                   / selecting subsets of the data\n",
      "    append : boolean, default False\n",
      "        For Table formats, append the input data to the existing\n",
      "    data_columns :  list of columns, or True, default None\n",
      "        List of columns to create as indexed data columns for on-disk\n",
      "        queries, or True to use all columns. By default only the axes\n",
      "        of the object are indexed. See `here\n",
      "        <http://pandas.pydata.org/pandas-docs/stable/io.html#query-via-data-columns>`__.\n",
      "    \n",
      "        Applicable only to format='table'.\n",
      "    complevel : int, 1-9, default 0\n",
      "        If a complib is specified compression will be applied\n",
      "        where possible\n",
      "    complib : {'zlib', 'bzip2', 'lzo', 'blosc', None}, default None\n",
      "        If complevel is > 0 apply compression to objects written\n",
      "        in the store wherever possible\n",
      "    fletcher32 : bool, default False\n",
      "        If applying compression use the fletcher32 checksum\n",
      "    dropna : boolean, default False.\n",
      "        If true, ALL nan rows will not be written to store.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(prediction_frames.to_hdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
