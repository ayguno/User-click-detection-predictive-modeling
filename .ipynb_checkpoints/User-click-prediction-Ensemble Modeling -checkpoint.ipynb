{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble modeling approach\n",
    "\n",
    "In this exercise, we will try to ensemble predictions from the best models we trained in our first approach and will try to train a second tier model.\n",
    "\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_train_balanced_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_train_balanced_trans_pl2 = pickle.load(f) \n",
    "\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/y_train_balanced.pkl\", \"rb\") as f:\n",
    "    y_train_balanced = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Trained model, it took: 1430.3166666666666 minutes.\n",
      "Val1 ROC score: 0.952874287995\n",
      "Val2 ROC score: 0.951511484007\n",
      "Saved final logistic regression classifier.\n"
     ]
    }
   ],
   "source": [
    "# Let's use C = 3.0589 to re-train the classifier and prepare another submission\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "logbal = LogisticRegression(verbose=10, n_jobs=3, C= 3.0589,penalty= \"l1\")\n",
    "\n",
    "logbal.fit(X_train_balanced_trans_pl2, y_train_balanced)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation sets\n",
    "# Load validation sets previously prepared\n",
    "\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val1_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val1_trans_pl2 = pickle.load(f)\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "    \n",
    "# Target labels:    \n",
    "with open(\"y_val1.pkl\",\"rb\") as f:\n",
    "    y_val1= pickle.load(f)\n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 1\n",
    "probs = logbal.predict_proba(X_val1_trans_pl2)\n",
    "probs = probs[:,1]\n",
    "print(\"Val1 ROC score: \" +str(roc_auc_score(y_val1,probs)))\n",
    "       \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "probs = logbal.predict_proba(X_val2_trans_pl2)\n",
    "probs = probs[:,1]\n",
    "print(\"Val2 ROC score: \" +str(roc_auc_score(y_val2,probs))) \n",
    "\n",
    "\n",
    "# Save the final classifier\n",
    "with open((diskname + str(\"log_final.pkl\")), \"wb\") as f:\n",
    "    pickle.dump(logbal,f)\n",
    "    \n",
    "print(\"Saved final logistic regression classifier.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear]It took: 0.4 minutes.\n",
      "Val1 ROC score: 0.952513357043\n",
      "Val2 ROC score: 0.952253716778\n",
      "Saved final SVM regression classifier.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "# Note that we can't get probabilities directly from this LinearSVC function\n",
    "# We need to wrap into Calibrated Classifier \n",
    "# (see: https://stackoverflow.com/questions/35212213/sklearn-how-to-get-decision-probabilities-for-linearsvc-classifier)\n",
    "\n",
    "lsvcbal = LinearSVC(verbose=10, C = 0.0564)\n",
    "\n",
    "cal_lsvcbal = CalibratedClassifierCV(base_estimator = lsvcbal,\n",
    "                                  cv = 3, # Also performs cross-validation\n",
    "                                  method= \"sigmoid\") # We use sigmoid function to get probabilities\n",
    "\n",
    "cal_lsvcbal.fit(X_train_balanced_trans_pl2,y_train_balanced)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "\n",
    "\n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 1\n",
    "probs = cal_lsvcbal.predict_proba(X_val1_trans_pl2)\n",
    "probs = probs[:,1]\n",
    "print(\"Val1 ROC score: \" +str(roc_auc_score(y_val1,probs)))\n",
    "       \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "probs = cal_lsvcbal.predict_proba(X_val2_trans_pl2)\n",
    "probs = probs[:,1]\n",
    "print(\"Val2 ROC score: \" +str(roc_auc_score(y_val2,probs)))\n",
    "\n",
    "\n",
    "# Save the final classifier\n",
    "with open((diskname + str(\"svm_final.pkl\")), \"wb\") as f:\n",
    "    pickle.dump(cal_lsvcbal,f)\n",
    "    \n",
    "print(\"Saved final SVM regression classifier.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled a smaller training set.\n",
      "Trained model, it took: 1436.2166666666667 minutes.\n",
      "Val1 ROC score: 0.947543785832\n",
      "Val2 ROC score: 0.945366327648\n",
      "Saved final random forest  classifier.\n"
     ]
    }
   ],
   "source": [
    "#let's try to get a small random sample from the class-balanced set to train the RF algorithm\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(112)\n",
    "rows = random.sample(list(range(0,X_train_balanced_trans_pl2.shape[0])),50000)\n",
    "X_train_balanced_trans_pl2_sample = X_train_balanced_trans_pl2[rows,]\n",
    "y_train_balanced_sample = y_train_balanced.iloc[rows]\n",
    "\n",
    "print(\"Sampled a smaller training set.\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1320,\n",
    "                            min_samples_leaf= 2,\n",
    "                            min_samples_split= 9,\n",
    "                            max_features= 306,\n",
    "                            max_depth= 297,n_jobs=3)\n",
    "\n",
    "rf.fit(X_train_balanced_trans_pl2_sample, y_train_balanced_sample)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation sets\n",
    "# Load validation sets previously prepared\n",
    "\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val1_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val1_trans_pl2 = pickle.load(f)\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "    \n",
    "# Target labels:    \n",
    "with open(\"y_val1.pkl\",\"rb\") as f:\n",
    "    y_val1= pickle.load(f)\n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 1\n",
    "probs = rf.predict_proba(X_val1_trans_pl2)\n",
    "probs = probs[:,1]\n",
    "print(\"Val1 ROC score: \" +str(roc_auc_score(y_val1,probs)))\n",
    "       \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "probs = rf.predict_proba(X_val2_trans_pl2)\n",
    "probs = probs[:,1]\n",
    "print(\"Val2 ROC score: \" +str(roc_auc_score(y_val2,probs))) \n",
    "\n",
    "\n",
    "# Save the final classifier\n",
    "with open((diskname + str(\"rf_final.pkl\")), \"wb\") as f:\n",
    "    pickle.dump(rf,f)\n",
    "    \n",
    "print(\"Saved final random forest  classifier.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled a smaller training set.\n",
      "Trained model, it took: 1435.4333333333334 minutes.\n",
      "Val1 ROC score: 0.945943193181\n",
      "Val2 ROC score: 0.946552900812\n",
      "Saved ada boost  classifier.\n"
     ]
    }
   ],
   "source": [
    "#let's try to get a small random sample from the class-balanced set to train the RF algorithm\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(112)\n",
    "rows = random.sample(list(range(0,X_train_balanced_trans_pl2.shape[0])),100000)\n",
    "X_train_balanced_trans_pl2_sample = X_train_balanced_trans_pl2[rows,]\n",
    "y_train_balanced_sample = y_train_balanced.iloc[rows]\n",
    "\n",
    "print(\"Sampled a smaller training set.\")\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ab = AdaBoostClassifier(n_estimators=600, learning_rate= 0.2)\n",
    "\n",
    "ab.fit(X_train_balanced_trans_pl2_sample, y_train_balanced_sample)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation sets\n",
    "# Load validation sets previously prepared\n",
    "\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val1_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val1_trans_pl2 = pickle.load(f)\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "    \n",
    "# Target labels:    \n",
    "with open(\"y_val1.pkl\",\"rb\") as f:\n",
    "    y_val1= pickle.load(f)\n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 1\n",
    "probs = ab.predict_proba(X_val1_trans_pl2)\n",
    "probs = probs[:,1]\n",
    "print(\"Val1 ROC score: \" +str(roc_auc_score(y_val1,probs)))\n",
    "       \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "probs = ab.predict_proba(X_val2_trans_pl2)\n",
    "probs = probs[:,1]\n",
    "print(\"Val2 ROC score: \" +str(roc_auc_score(y_val2,probs))) \n",
    "\n",
    "\n",
    "# Save the final classifier\n",
    "with open((diskname + str(\"ab_final.pkl\")), \"wb\") as f:\n",
    "    pickle.dump(ab,f)\n",
    "    \n",
    "print(\"Saved ada boost  classifier.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the ensemble classifier\n",
    "\n",
    "We will use X_val1 predictions from the two of our best models to train a new classifier against the y_val1. We will try to optimize the performance of this new classifier using X_val2 and y_val2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Features:\n",
    "with open(\"X_val1_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val1_trans_pl2 = pickle.load(f)\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "    \n",
    "# Target labels:    \n",
    "with open(\"y_val1.pkl\",\"rb\") as f:\n",
    "    y_val1= pickle.load(f)\n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that return secondary features\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "def secondary_features(X_train):\n",
    "    import pandas as pd\n",
    "    # Load established classifiers\n",
    "    with open((diskname + str(\"log_final.pkl\")), \"rb\") as f:\n",
    "        clf1 = pickle.load(f)\n",
    "    with open((diskname + str(\"svm_final.pkl\")), \"rb\") as f:\n",
    "        clf2 = pickle.load(f)\n",
    "    with open((diskname + str(\"ab_final.pkl\")), \"rb\") as f:\n",
    "        clf3 = pickle.load(f)    \n",
    "    print(\"Loaded classifiers.\")    \n",
    "    # Collect prediction probabilities as new features    \n",
    "    sec_features = pd.DataFrame()\n",
    "    sec_features[\"f1\"] = clf1.predict_proba(X_train)[:,1]\n",
    "    sec_features[\"f2\"] = clf2.predict_proba(X_train)[:,1]\n",
    "    sec_features[\"f3\"] = clf3.predict_proba(X_train)[:,1]\n",
    "    print(\"Collected features.\")\n",
    "    \n",
    "    # Return new features as array\n",
    "    return sec_features.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Transformed into secondary features, it took: 1438.15 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# it takes about 7 mins to transform\n",
    "start = datetime.datetime.now()\n",
    "X_train_ensemble = secondary_features(X_val1_trans_pl2)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = start - end\n",
    "print(\"Transformed into secondary features, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4198908 ,  0.36075446,  0.49962477],\n",
       "       [ 0.02056251,  0.04736733,  0.49493243],\n",
       "       [ 0.11612122,  0.11748793,  0.49707586],\n",
       "       ..., \n",
       "       [ 0.39121193,  0.34091477,  0.50093734],\n",
       "       [ 0.04425574,  0.05957478,  0.49552605],\n",
       "       [ 0.59588697,  0.60118489,  0.50197116]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Logisticregression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Trained ensemble model, it took: 1439.9666666666667 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.952225786469\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = LogisticRegression(verbose=10)\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear]Trained ensemble model, it took: 1439.1 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.950172219726\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC  \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lsvcbal = LinearSVC(verbose=10, C= 5)\n",
    "\n",
    "ensb = CalibratedClassifierCV(base_estimator = lsvcbal,\n",
    "                                  cv = 3, # Also performs cross-validation\n",
    "                                  method= \"sigmoid\") # We use sigmoid function to get probabilities\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.1166666666666 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.815179565761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = RandomForestClassifier()\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Extratree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.8833333333334 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.827961862994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = ExtraTreesClassifier()\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.9833333333333 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.557963136275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = MultinomialNB()\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble XGboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.1166666666666 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.951161891619\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = xgb.XGBClassifier()\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble QDA classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.9833333333333 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.95241337647\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = QuadraticDiscriminantAnalysis(reg_param= 0.005)\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |        p1 |        p2 |   reg_param | \n",
      "Loaded classifiers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-e9da507a2335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Finally we call .maximize method of the optimizer with the appropriate arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ucb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgp_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, init_points)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Evaluate target function at all initialization points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observe_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Add the points from `self.initialize` to the observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36m_observe_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_observe_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mobserve_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# measure the target function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-e9da507a2335>\u001b[0m in \u001b[0;36mmaximizer\u001b[0;34m(reg_param, p1, p2)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# calculate out-of-the-box roc_score using validation set 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecondary_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val2_trans_pl2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mval2_roc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-16ed6d27d476>\u001b[0m in \u001b[0;36msecondary_features\u001b[0;34m(X_train)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msec_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0msec_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0msec_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f3\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Collected features.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;31m# The weights are all 1. for SAMME.R\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             proba = sum(_samme_proba(estimator, n_classes, X)\n\u001b[0;32m--> 765\u001b[0;31m                         for estimator in self.estimators_)\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# self.algorithm == \"SAMME\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             proba = sum(estimator.predict_proba(X) * w\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;31m# The weights are all 1. for SAMME.R\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             proba = sum(_samme_proba(estimator, n_classes, X)\n\u001b[0;32m--> 765\u001b[0;31m                         for estimator in self.estimators_)\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# self.algorithm == \"SAMME\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             proba = sum(estimator.predict_proba(X) * w\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_samme_proba\u001b[0;34m(estimator, n_classes, X)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     return (n_classes - 1) * (log_proba - (1. / n_classes)\n\u001b[0;32m--> 294\u001b[0;31m                               * log_proba.sum(axis=1)[:, np.newaxis])\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization\n",
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return validated 'roc_auc' score from Classifier\n",
    "# Note that the parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def maximizer(reg_param,p1,p2):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    estimator_function = QuadraticDiscriminantAnalysis(reg_param= reg_param, priors = [p1,p2])\n",
    "    \n",
    "    # Fit the estimator\n",
    "    estimator_function.fit(X_train_ensemble, y_val1)\n",
    "        \n",
    "    # calculate out-of-the-box roc_score using validation set 2\n",
    "    probs = estimator_function.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "    val2_roc = roc_auc_score(y_val2,probs)\n",
    "    \n",
    "    # return the validation score to be maximized \n",
    "    return val2_roc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "\n",
    "hyperparameter_space = {\n",
    "        'reg_param': (0.004,0.005),\n",
    "        \"p1\": (0.8,0.999),\n",
    "        \"p2\": (0.01,0.2)\n",
    "}\n",
    "\n",
    "BO = BayesianOptimization(f = maximizer, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "\n",
    "BO.maximize(init_points=10,n_iter=10,acq='ucb', kappa= 5, **gp_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0041,\n",
       "               store_covariance=False, store_covariances=None, tol=0.0001)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare an ensemble prediction using QDA classifier\n",
    "# Train with tuned parameter\n",
    "ensb = QuadraticDiscriminantAnalysis(reg_param= 0.0041)\n",
    "ensb.fit(X_train_ensemble, y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed test sparse matrix.\n"
     ]
    }
   ],
   "source": [
    "# Prepare a submission using the tuned QDA classifier\n",
    "\n",
    "import pandas as pd\n",
    "click_id = pd.read_hdf(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/click_id.h5\")\n",
    "def prepare_submission(predictions,filename = \"new_submission\", click_id = click_id):\n",
    "    \"\"\"predictions: a list containing the predicted probabilities in the test set. \"\"\"\n",
    "    is_attributed = pd.Series(predictions)\n",
    "    submission_frame = pd.DataFrame()\n",
    "    submission_frame[\"click_id\"] = click_id\n",
    "    submission_frame[\"is_attributed\"] = is_attributed.apply(lambda x: format(x,\".9f\"))  # Reformat the probabilities upto the 9th decimal point\n",
    "    filename = filename + \".csv\"\n",
    "    submission_frame.to_csv(filename,index = False)\n",
    "    print(\"File saved as :\" + filename)\n",
    "\n",
    "\n",
    "# let's perform a prediction using the test set\n",
    "# Load the sparse matrix \n",
    "import scipy.sparse as sp\n",
    "test_proc_p12 = sp.load_npz(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl2.npz\").tocsr()\n",
    "print(\"Loaded processed test sparse matrix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Prepared QDA ensemble probs.\n",
      "File saved as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/ENSB_QDA_submission.csv\n"
     ]
    }
   ],
   "source": [
    "probs = ensb.predict_proba(secondary_features(test_proc_p12))[:,1]\n",
    "print(\"Prepared QDA ensemble probs.\")\n",
    "# Prepare the submission file\n",
    "prepare_submission(predictions= probs, \n",
    "                   filename= \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/ENSB_QDA_submission\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission scored 0.9605."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.95 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9c1a299b22d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Note that we are converting features to secondary features since we are using ensemble model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecondary_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val2_trans_pl2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ensemble Val ROC score: \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/neighbors/classification.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;31m# a simple ':' index doesn't work right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# loop is O(n_neighbors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mproba_k\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;31m# normalize 'votes' into real [0,1] probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = KNeighborsClassifier(n_neighbors= 150)\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble LDA classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.9833333333333 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.952363803875\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = LinearDiscriminantAnalysis(shrinkage= 'auto', solver= 'lsqr')\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.9833333333333 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.950236607507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = GaussianNB(priors = [0.999999,0.000001])\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.5666666666666 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.95230705822\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = MLPClassifier(alpha = 0.1, hidden_layer_sizes=(10,10,10,))\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10, 10, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.5,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensb.get_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |        N1 |        N2 |        N3 |     alpha | \n",
      "Iteration 1, loss = 0.02328634\n",
      "Iteration 2, loss = 0.01417183\n",
      "Iteration 3, loss = 0.01323872\n",
      "Iteration 4, loss = 0.01295399\n",
      "Iteration 5, loss = 0.01282552\n",
      "Iteration 6, loss = 0.01278816\n",
      "Iteration 7, loss = 0.01274812\n",
      "Iteration 8, loss = 0.01273363\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    1 | 04m06s | \u001b[35m   0.50000\u001b[0m | \u001b[32m 138.1691\u001b[0m | \u001b[32m 146.4545\u001b[0m | \u001b[32m 163.6315\u001b[0m | \u001b[32m   0.3751\u001b[0m | \n",
      "Iteration 1, loss = 0.02400890\n",
      "Iteration 2, loss = 0.01370346\n",
      "Iteration 3, loss = 0.01319368\n",
      "Iteration 4, loss = 0.01297327\n",
      "Iteration 5, loss = 0.01287496\n",
      "Iteration 6, loss = 0.01281547\n",
      "Iteration 7, loss = 0.01278381\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    2 | 01m56s | \u001b[35m   0.95234\u001b[0m | \u001b[32m  41.7572\u001b[0m | \u001b[32m  98.4419\u001b[0m | \u001b[32m 146.8079\u001b[0m | \u001b[32m   0.6403\u001b[0m | \n",
      "Iteration 1, loss = 0.02905142\n",
      "Iteration 2, loss = 0.01398987\n",
      "Iteration 3, loss = 0.01329819\n",
      "Iteration 4, loss = 0.01304417\n",
      "Iteration 5, loss = 0.01292479\n",
      "Iteration 6, loss = 0.01285939\n",
      "Iteration 7, loss = 0.01280956\n",
      "Iteration 8, loss = 0.01277473\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    3 | 02m31s |    0.04769 |  100.8463 |  104.1062 |  117.5487 |    0.9500 | \n",
      "Iteration 1, loss = 0.01323074\n",
      "Iteration 2, loss = 0.00936950\n",
      "Iteration 3, loss = 0.00934193\n",
      "Iteration 4, loss = 0.00932505\n",
      "Iteration 5, loss = 0.00933296\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    4 | 05m19s |    0.95231 |  139.4123 |   66.4374 |  135.2485 |    0.0757 | \n",
      "Iteration 1, loss = 0.02430100\n",
      "Iteration 2, loss = 0.01364201\n",
      "Iteration 3, loss = 0.01314227\n",
      "Iteration 4, loss = 0.01296365\n",
      "Iteration 5, loss = 0.01287378\n",
      "Iteration 6, loss = 0.01281014\n",
      "Iteration 7, loss = 0.01279053\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    5 | 01m09s |    0.04769 |   50.4390 |   24.2249 |  159.5039 |    0.7769 | \n",
      "Iteration 1, loss = 0.02656183\n",
      "Iteration 2, loss = 0.01367871\n",
      "Iteration 3, loss = 0.01319095\n",
      "Iteration 4, loss = 0.01297126\n",
      "Iteration 5, loss = 0.01287076\n",
      "Iteration 6, loss = 0.01282096\n",
      "Iteration 7, loss = 0.01278400\n",
      "Iteration 8, loss = 0.01277504\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    6 | 02m34s |    0.04771 |   85.0652 |  109.9899 |  161.8494 |    0.8327 | \n",
      "Iteration 1, loss = 0.01568289\n",
      "Iteration 2, loss = 0.00890825\n",
      "Iteration 3, loss = 0.00879751\n",
      "Iteration 4, loss = 0.00879206\n",
      "Iteration 5, loss = 0.00875018\n",
      "Iteration 6, loss = 0.00874811\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    7 | 03m36s |    0.95231 |   48.7647 |  111.2722 |   22.5143 |    0.0548 | \n",
      "Iteration 1, loss = 0.02668585\n",
      "Iteration 2, loss = 0.01378049\n",
      "Iteration 3, loss = 0.01322674\n",
      "Iteration 4, loss = 0.01299266\n",
      "Iteration 5, loss = 0.01289550\n",
      "Iteration 6, loss = 0.01285191\n",
      "Iteration 7, loss = 0.01279836\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    8 | 02m32s |    0.04768 |   48.2439 |  132.6396 |  168.0408 |    0.8177 | \n",
      "Iteration 1, loss = 0.03228374\n",
      "Iteration 2, loss = 0.01473090\n",
      "Iteration 3, loss = 0.01369837\n",
      "Iteration 4, loss = 0.01327700\n",
      "Iteration 5, loss = 0.01306305\n",
      "Iteration 6, loss = 0.01293324\n",
      "Iteration 7, loss = 0.01285012\n",
      "Iteration 8, loss = 0.01279021\n",
      "Iteration 9, loss = 0.01274936\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    9 | 01m33s |    0.95233 |   75.3800 |   67.2475 |   43.1665 |    0.8854 | \n",
      "Iteration 1, loss = 0.02626991\n",
      "Iteration 2, loss = 0.01501972\n",
      "Iteration 3, loss = 0.01396991\n",
      "Iteration 4, loss = 0.01349758\n",
      "Iteration 5, loss = 0.01324572\n",
      "Iteration 6, loss = 0.01310094\n",
      "Iteration 7, loss = 0.01298928\n",
      "Iteration 8, loss = 0.01291262\n",
      "Iteration 9, loss = 0.01287298\n",
      "Iteration 10, loss = 0.01281574\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   10 | 02m54s |    0.04767 |  139.7419 |  161.6549 |   14.8526 |    0.7223 | \n",
      "Iteration 1, loss = 0.00985688\n",
      "Iteration 2, loss = 0.00733662\n",
      "Iteration 3, loss = 0.00713308\n",
      "Iteration 4, loss = 0.00703238\n",
      "Iteration 5, loss = 0.00698020\n",
      "Iteration 6, loss = 0.00695345\n",
      "Iteration 7, loss = 0.00691807\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   11 | 09m36s |    0.95230 |  176.8832 |   77.7823 |  123.2280 |    0.0026 | \n",
      "Iteration 1, loss = 0.02928769\n",
      "Iteration 2, loss = 0.01409641\n",
      "Iteration 3, loss = 0.01335832\n",
      "Iteration 4, loss = 0.01308178\n",
      "Iteration 5, loss = 0.01292456\n",
      "Iteration 6, loss = 0.01285394\n",
      "Iteration 7, loss = 0.01279048\n",
      "Iteration 8, loss = 0.01276728\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   12 | 02m56s |    0.95231 |   95.6265 |  146.1380 |  109.9270 |    0.9812 | \n",
      "Iteration 1, loss = 0.02116851\n",
      "Iteration 2, loss = 0.01469972\n",
      "Iteration 3, loss = 0.01464688\n",
      "Iteration 4, loss = 0.01444183\n",
      "Iteration 5, loss = 0.01369848\n",
      "Iteration 6, loss = 0.01319391\n",
      "Iteration 7, loss = 0.01297417\n",
      "Iteration 8, loss = 0.01287272\n",
      "Iteration 9, loss = 0.01280350\n",
      "Iteration 10, loss = 0.01275566\n",
      "Iteration 11, loss = 0.01271284\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   13 | 03m17s |    0.95230 |  165.8459 |  119.1200 |   46.2841 |    0.3434 | \n",
      "Iteration 1, loss = 0.01468832\n",
      "Iteration 2, loss = 0.00982150\n",
      "Iteration 3, loss = 0.00980456\n",
      "Iteration 4, loss = 0.00980036\n",
      "Iteration 5, loss = 0.00977403\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   14 | 03m10s |    0.95231 |  140.2167 |  109.8405 |   62.4206 |    0.0948 | \n",
      "Iteration 1, loss = 0.02215611\n",
      "Iteration 2, loss = 0.01417535\n",
      "Iteration 3, loss = 0.01317483\n",
      "Iteration 4, loss = 0.01294072\n",
      "Iteration 5, loss = 0.01283122\n",
      "Iteration 6, loss = 0.01279653\n",
      "Iteration 7, loss = 0.01275337\n",
      "Iteration 8, loss = 0.01275359\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   15 | 02m22s |    0.50000 |  162.6109 |   68.5861 |  166.2029 |    0.3946 | \n",
      "Iteration 1, loss = 0.01444690\n",
      "Iteration 2, loss = 0.00757882\n",
      "Iteration 3, loss = 0.00736076\n",
      "Iteration 4, loss = 0.00722844\n",
      "Iteration 5, loss = 0.00710995\n",
      "Iteration 6, loss = 0.00706428\n",
      "Iteration 7, loss = 0.00705079\n",
      "Iteration 8, loss = 0.00701850\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   16 | 03m02s |    0.95230 |   59.6327 |   38.8771 |   31.1648 |    0.0050 | \n",
      "Iteration 1, loss = 0.02372005\n",
      "Iteration 2, loss = 0.01357671\n",
      "Iteration 3, loss = 0.01310474\n",
      "Iteration 4, loss = 0.01295989\n",
      "Iteration 5, loss = 0.01286334\n",
      "Iteration 6, loss = 0.01282587\n",
      "Iteration 7, loss = 0.01280382\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   17 | 01m47s |    0.95228 |  118.9102 |   39.9617 |  194.4173 |    0.7367 | \n",
      "Iteration 1, loss = 0.02513518\n",
      "Iteration 2, loss = 0.01364272\n",
      "Iteration 3, loss = 0.01315461\n",
      "Iteration 4, loss = 0.01298473\n",
      "Iteration 5, loss = 0.01286748\n",
      "Iteration 6, loss = 0.01283835\n",
      "Iteration 7, loss = 0.01279884\n",
      "Iteration 8, loss = 0.01277039\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected features.\n",
      "   18 | 01m51s |    0.04766 |   39.3856 |   70.8663 |  185.0109 |    0.9558 | \n",
      "Iteration 1, loss = 0.02696712\n",
      "Iteration 2, loss = 0.01435707\n",
      "Iteration 3, loss = 0.01354445\n",
      "Iteration 4, loss = 0.01319261\n",
      "Iteration 5, loss = 0.01300981\n",
      "Iteration 6, loss = 0.01289042\n",
      "Iteration 7, loss = 0.01281019\n",
      "Iteration 8, loss = 0.01276652\n",
      "Iteration 9, loss = 0.01272297\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   19 | 03m06s |    0.95212 |  190.6947 |  147.1240 |   34.7259 |    0.8206 | \n",
      "Iteration 1, loss = 0.02259393\n",
      "Iteration 2, loss = 0.01470273\n",
      "Iteration 3, loss = 0.01462508\n",
      "Iteration 4, loss = 0.01404491\n",
      "Iteration 5, loss = 0.01332289\n",
      "Iteration 6, loss = 0.01305312\n",
      "Iteration 7, loss = 0.01293119\n",
      "Iteration 8, loss = 0.01283964\n",
      "Iteration 9, loss = 0.01278164\n",
      "Iteration 10, loss = 0.01273478\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   20 | 01m54s |    0.04768 |   45.4879 |  109.8809 |   62.4532 |    0.3450 | \n",
      "Iteration 1, loss = 0.02277086\n",
      "Iteration 2, loss = 0.01459894\n",
      "Iteration 3, loss = 0.01332311\n",
      "Iteration 4, loss = 0.01298281\n",
      "Iteration 5, loss = 0.01287184\n",
      "Iteration 6, loss = 0.01280849\n",
      "Iteration 7, loss = 0.01276254\n",
      "Iteration 8, loss = 0.01275126\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   21 | 00m54s |    0.04770 |   19.6975 |   10.6102 |  138.6965 |    0.3786 | \n",
      "Iteration 1, loss = 0.02636776\n",
      "Iteration 2, loss = 0.01437856\n",
      "Iteration 3, loss = 0.01334353\n",
      "Iteration 4, loss = 0.01303806\n",
      "Iteration 5, loss = 0.01290396\n",
      "Iteration 6, loss = 0.01283659\n",
      "Iteration 7, loss = 0.01277371\n",
      "Iteration 8, loss = 0.01275888\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   22 | 01m06s |    0.04770 |   59.7168 |   29.0610 |   77.0179 |    0.7848 | \n",
      "Iteration 1, loss = 0.01406295\n",
      "Iteration 2, loss = 0.00963853\n",
      "Iteration 3, loss = 0.00958067\n",
      "Iteration 4, loss = 0.00957772\n",
      "Iteration 5, loss = 0.00957110\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   23 | 03m04s |    0.95231 |   41.0262 |  123.5162 |   89.5153 |    0.0862 | \n",
      "Iteration 1, loss = 0.02693518\n",
      "Iteration 2, loss = 0.01461882\n",
      "Iteration 3, loss = 0.01354957\n",
      "Iteration 4, loss = 0.01312288\n",
      "Iteration 5, loss = 0.01294055\n",
      "Iteration 6, loss = 0.01284797\n",
      "Iteration 7, loss = 0.01277510\n",
      "Iteration 8, loss = 0.01274486\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   24 | 01m29s |    0.04767 |  164.6281 |   35.4948 |   59.6526 |    0.5461 | \n",
      "Iteration 1, loss = 0.01685885\n",
      "Iteration 2, loss = 0.01129306\n",
      "Iteration 3, loss = 0.01127431\n",
      "Iteration 4, loss = 0.01126376\n",
      "Iteration 5, loss = 0.01125370\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   25 | 01m02s |    0.95231 |   36.5309 |   99.4850 |   27.1605 |    0.1622 | \n",
      "Iteration 1, loss = 0.01990437\n",
      "Iteration 2, loss = 0.01383144\n",
      "Iteration 3, loss = 0.01379974\n",
      "Iteration 4, loss = 0.01379959\n",
      "Iteration 5, loss = 0.01377685\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   26 | 01m35s |    0.95232 |  146.4072 |   71.4337 |  107.8808 |    0.2901 | \n",
      "Iteration 1, loss = 0.01253318\n",
      "Iteration 2, loss = 0.00862738\n",
      "Iteration 3, loss = 0.00856418\n",
      "Iteration 4, loss = 0.00849720\n",
      "Iteration 5, loss = 0.00850358\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   27 | 08m09s |    0.95231 |   91.1539 |  193.5876 |   86.4950 |    0.0450 | \n",
      "Iteration 1, loss = 0.02113260\n",
      "Iteration 2, loss = 0.01452223\n",
      "Iteration 3, loss = 0.01450100\n",
      "Iteration 4, loss = 0.01378403\n",
      "Iteration 5, loss = 0.01309674\n",
      "Iteration 6, loss = 0.01289521\n",
      "Iteration 7, loss = 0.01281231\n",
      "Iteration 8, loss = 0.01275731\n",
      "Iteration 9, loss = 0.01273718\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   28 | 02m30s |    0.50000 |   45.2879 |   89.1619 |  168.8022 |    0.3322 | \n",
      "Iteration 1, loss = 0.02590740\n",
      "Iteration 2, loss = 0.01404399\n",
      "Iteration 3, loss = 0.01332867\n",
      "Iteration 4, loss = 0.01304469\n",
      "Iteration 5, loss = 0.01290917\n",
      "Iteration 6, loss = 0.01283913\n",
      "Iteration 7, loss = 0.01276325\n",
      "Iteration 8, loss = 0.01274006\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   29 | 01m19s |    0.04770 |   37.9896 |   74.5698 |   49.3203 |    0.7365 | \n",
      "Iteration 1, loss = 0.02759410\n",
      "Iteration 2, loss = 0.01370332\n",
      "Iteration 3, loss = 0.01320334\n",
      "Iteration 4, loss = 0.01302004\n",
      "Iteration 5, loss = 0.01289083\n",
      "Iteration 6, loss = 0.01284198\n",
      "Iteration 7, loss = 0.01281091\n",
      "Iteration 8, loss = 0.01279348\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   30 | 03m17s |    0.95228 |  107.8476 |  142.4362 |  144.2439 |    0.8392 | \n",
      "Iteration 1, loss = 0.02450267\n",
      "Iteration 2, loss = 0.01453507\n",
      "Iteration 3, loss = 0.01339783\n",
      "Iteration 4, loss = 0.01302615\n",
      "Iteration 5, loss = 0.01289644\n",
      "Iteration 6, loss = 0.01280976\n",
      "Iteration 7, loss = 0.01277028\n",
      "Iteration 8, loss = 0.01273333\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   31 | 01m21s |    0.95231 |  186.0619 |   19.1753 |   63.7318 |    0.7094 | \n",
      "Iteration 1, loss = 0.01789781\n",
      "Iteration 2, loss = 0.01188371\n",
      "Iteration 3, loss = 0.01183605\n",
      "Iteration 4, loss = 0.01182674\n",
      "Iteration 5, loss = 0.01184278\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   32 | 01m45s |    0.95231 |   10.3331 |  149.4549 |  109.5836 |    0.1884 | \n",
      "Iteration 1, loss = 0.02777606\n",
      "Iteration 2, loss = 0.01382247\n",
      "Iteration 3, loss = 0.01325187\n",
      "Iteration 4, loss = 0.01303557\n",
      "Iteration 5, loss = 0.01292283\n",
      "Iteration 6, loss = 0.01284993\n",
      "Iteration 7, loss = 0.01280589\n",
      "Iteration 8, loss = 0.01279973\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   33 | 02m33s |    0.04766 |   37.0325 |  119.0916 |  181.5541 |    0.9857 | \n",
      "Iteration 1, loss = 0.02131389\n",
      "Iteration 2, loss = 0.01443504\n",
      "Iteration 3, loss = 0.01322518\n",
      "Iteration 4, loss = 0.01293602\n",
      "Iteration 5, loss = 0.01283116\n",
      "Iteration 6, loss = 0.01277121\n",
      "Iteration 7, loss = 0.01275182\n",
      "Iteration 8, loss = 0.01273682\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   34 | 01m26s |    0.50000 |   78.6784 |   28.8389 |  164.2947 |    0.3880 | \n",
      "Iteration 1, loss = 0.02463861\n",
      "Iteration 2, loss = 0.01484215\n",
      "Iteration 3, loss = 0.01349769\n",
      "Iteration 4, loss = 0.01306060\n",
      "Iteration 5, loss = 0.01289897\n",
      "Iteration 6, loss = 0.01282920\n",
      "Iteration 7, loss = 0.01277297\n",
      "Iteration 8, loss = 0.01275477\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   35 | 01m18s |    0.95207 |  192.0531 |   14.9033 |   68.8198 |    0.5032 | \n",
      "Iteration 1, loss = 0.03132957\n",
      "Iteration 2, loss = 0.01551071\n",
      "Iteration 3, loss = 0.01414172\n",
      "Iteration 4, loss = 0.01357250\n",
      "Iteration 5, loss = 0.01329366\n",
      "Iteration 6, loss = 0.01311462\n",
      "Iteration 7, loss = 0.01302144\n",
      "Iteration 8, loss = 0.01292423\n",
      "Iteration 9, loss = 0.01286090\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   36 | 01m12s |    0.04771 |   16.8780 |  118.7673 |   20.4376 |    0.7635 | \n",
      "Iteration 1, loss = 0.02599177\n",
      "Iteration 2, loss = 0.01415728\n",
      "Iteration 3, loss = 0.01324584\n",
      "Iteration 4, loss = 0.01299099\n",
      "Iteration 5, loss = 0.01289760\n",
      "Iteration 6, loss = 0.01281726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.01279173\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   37 | 01m04s |    0.04770 |  128.1215 |   14.6908 |  107.9998 |    0.6837 | \n",
      "Iteration 1, loss = 0.02865082\n",
      "Iteration 2, loss = 0.01415810\n",
      "Iteration 3, loss = 0.01339232\n",
      "Iteration 4, loss = 0.01309542\n",
      "Iteration 5, loss = 0.01295149\n",
      "Iteration 6, loss = 0.01286207\n",
      "Iteration 7, loss = 0.01279973\n",
      "Iteration 8, loss = 0.01276512\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   38 | 01m26s |    0.95231 |   74.0596 |   58.1342 |   82.8534 |    0.7220 | \n",
      "Iteration 1, loss = 0.01768165\n",
      "Iteration 2, loss = 0.01241947\n",
      "Iteration 3, loss = 0.01236736\n",
      "Iteration 4, loss = 0.01236734\n",
      "Iteration 5, loss = 0.01234383\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   39 | 02m12s |    0.95231 |  122.5912 |  106.0740 |  173.6062 |    0.2142 | \n",
      "Iteration 1, loss = 0.02726278\n",
      "Iteration 2, loss = 0.01498759\n",
      "Iteration 3, loss = 0.01383707\n",
      "Iteration 4, loss = 0.01331604\n",
      "Iteration 5, loss = 0.01306931\n",
      "Iteration 6, loss = 0.01293615\n",
      "Iteration 7, loss = 0.01284017\n",
      "Iteration 8, loss = 0.01277685\n",
      "Iteration 9, loss = 0.01273162\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   40 | 02m04s | \u001b[35m   0.95234\u001b[0m | \u001b[32m 199.7388\u001b[0m | \u001b[32m  76.6926\u001b[0m | \u001b[32m  14.4816\u001b[0m | \u001b[32m   0.7631\u001b[0m | \n",
      "Iteration 1, loss = 0.01284355\n",
      "Iteration 2, loss = 0.00940192\n",
      "Iteration 3, loss = 0.00938248\n",
      "Iteration 4, loss = 0.00931472\n",
      "Iteration 5, loss = 0.00934031\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   41 | 08m00s |    0.95231 |   90.7861 |  107.7396 |  167.9014 |    0.0752 | \n",
      "Iteration 1, loss = 0.02563312\n",
      "Iteration 2, loss = 0.01427067\n",
      "Iteration 3, loss = 0.01332256\n",
      "Iteration 4, loss = 0.01301995\n",
      "Iteration 5, loss = 0.01288606\n",
      "Iteration 6, loss = 0.01280785\n",
      "Iteration 7, loss = 0.01276404\n",
      "Iteration 8, loss = 0.01274886\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   42 | 01m50s |    0.04769 |  191.6914 |   56.6308 |   73.7662 |    0.6109 | \n",
      "Iteration 1, loss = 0.02249414\n",
      "Iteration 2, loss = 0.01364632\n",
      "Iteration 3, loss = 0.01311598\n",
      "Iteration 4, loss = 0.01292884\n",
      "Iteration 5, loss = 0.01282492\n",
      "Iteration 6, loss = 0.01277660\n",
      "Iteration 7, loss = 0.01276340\n",
      "Iteration 8, loss = 0.01274235\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   43 | 01m41s |    0.04764 |   39.8304 |   67.9790 |  138.8673 |    0.4256 | \n",
      "Iteration 1, loss = 0.01905384\n",
      "Iteration 2, loss = 0.01278765\n",
      "Iteration 3, loss = 0.01278083\n",
      "Iteration 4, loss = 0.01276535\n",
      "Iteration 5, loss = 0.01276343\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   44 | 01m32s |    0.95231 |   51.2419 |  144.8884 |   81.5054 |    0.2344 | \n",
      "Iteration 1, loss = 0.02395119\n",
      "Iteration 2, loss = 0.01451096\n",
      "Iteration 3, loss = 0.01371112\n",
      "Iteration 4, loss = 0.01333936\n",
      "Iteration 5, loss = 0.01316001\n",
      "Iteration 6, loss = 0.01304856\n",
      "Iteration 7, loss = 0.01296347\n",
      "Iteration 8, loss = 0.01288639\n",
      "Iteration 9, loss = 0.01282997\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   45 | 01m25s |    0.04769 |   18.7276 |  147.4276 |   22.9445 |    0.5490 | \n",
      "Iteration 1, loss = 0.02321981\n",
      "Iteration 2, loss = 0.01361848\n",
      "Iteration 3, loss = 0.01314593\n",
      "Iteration 4, loss = 0.01296660\n",
      "Iteration 5, loss = 0.01285354\n",
      "Iteration 6, loss = 0.01279232\n",
      "Iteration 7, loss = 0.01276225\n",
      "Iteration 8, loss = 0.01273528\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   46 | 02m20s |    0.04768 |   21.4194 |  137.1475 |  150.5467 |    0.5769 | \n",
      "Iteration 1, loss = 0.02882620\n",
      "Iteration 2, loss = 0.01525535\n",
      "Iteration 3, loss = 0.01396122\n",
      "Iteration 4, loss = 0.01336692\n",
      "Iteration 5, loss = 0.01307597\n",
      "Iteration 6, loss = 0.01293357\n",
      "Iteration 7, loss = 0.01283831\n",
      "Iteration 8, loss = 0.01278981\n",
      "Iteration 9, loss = 0.01274665\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   47 | 01m14s |    0.04772 |  107.5603 |   28.4653 |   22.5295 |    0.5480 | \n",
      "Iteration 1, loss = 0.01556757\n",
      "Iteration 2, loss = 0.01118075\n",
      "Iteration 3, loss = 0.01114570\n",
      "Iteration 4, loss = 0.01112587\n",
      "Iteration 5, loss = 0.01111579\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   48 | 06m01s |    0.95231 |   31.7818 |  190.5932 |  199.4805 |    0.1549 | \n",
      "Iteration 1, loss = 0.02544322\n",
      "Iteration 2, loss = 0.01372339\n",
      "Iteration 3, loss = 0.01323522\n",
      "Iteration 4, loss = 0.01296591\n",
      "Iteration 5, loss = 0.01287218\n",
      "Iteration 6, loss = 0.01281468\n",
      "Iteration 7, loss = 0.01278527\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   49 | 359m53s |    0.04771 |  175.2787 |  199.9297 |  164.2123 |    0.6326 | \n",
      "Iteration 1, loss = 0.01658405\n",
      "Iteration 2, loss = 0.01015239\n",
      "Iteration 3, loss = 0.01010031\n",
      "Iteration 4, loss = 0.01012092\n",
      "Iteration 5, loss = 0.01009270\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   50 | 01m49s |    0.95231 |  172.7145 |   30.5161 |   87.4388 |    0.1092 | \n",
      "Iteration 1, loss = 0.01601769\n",
      "Iteration 2, loss = 0.01032372\n",
      "Iteration 3, loss = 0.01026066\n",
      "Iteration 4, loss = 0.01025837\n",
      "Iteration 5, loss = 0.01023509\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   51 | 03m30s |    0.95231 |   94.4511 |  164.3267 |  130.9837 |    0.1147 | \n",
      "Iteration 1, loss = 0.02502757\n",
      "Iteration 2, loss = 0.01425080\n",
      "Iteration 3, loss = 0.01329158\n",
      "Iteration 4, loss = 0.01299786\n",
      "Iteration 5, loss = 0.01288401\n",
      "Iteration 6, loss = 0.01281454\n",
      "Iteration 7, loss = 0.01277951\n",
      "Iteration 8, loss = 0.01274732\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   52 | 02m33s |    0.04775 |  192.2350 |   77.9879 |  113.9788 |    0.4962 | \n",
      "Iteration 1, loss = 0.02191828\n",
      "Iteration 2, loss = 0.01483812\n",
      "Iteration 3, loss = 0.01381830\n",
      "Iteration 4, loss = 0.01321493\n",
      "Iteration 5, loss = 0.01297707\n",
      "Iteration 6, loss = 0.01286639\n",
      "Iteration 7, loss = 0.01279515\n",
      "Iteration 8, loss = 0.01274980\n",
      "Iteration 9, loss = 0.01272533\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   53 | 02m32s |    0.04776 |  189.8292 |   76.1347 |   56.5130 |    0.3772 | \n",
      "Iteration 1, loss = 0.02808118\n",
      "Iteration 2, loss = 0.01446495\n",
      "Iteration 3, loss = 0.01346979\n",
      "Iteration 4, loss = 0.01311835\n",
      "Iteration 5, loss = 0.01295135\n",
      "Iteration 6, loss = 0.01284074\n",
      "Iteration 7, loss = 0.01276595\n",
      "Iteration 8, loss = 0.01271867\n",
      "Iteration 9, loss = 0.01268760\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   54 | 00m51s |    0.95226 |   12.7135 |   51.2848 |   21.3878 |    0.9791 | \n",
      "Iteration 1, loss = 0.02491171\n",
      "Iteration 2, loss = 0.01418611\n",
      "Iteration 3, loss = 0.01326299\n",
      "Iteration 4, loss = 0.01298235\n",
      "Iteration 5, loss = 0.01286245\n",
      "Iteration 6, loss = 0.01279991\n",
      "Iteration 7, loss = 0.01277047\n",
      "Iteration 8, loss = 0.01274110\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   55 | 01m16s |    0.95232 |  166.8246 |   10.5351 |   81.6493 |    0.7133 | \n",
      "Iteration 1, loss = 0.01305953\n",
      "Iteration 2, loss = 0.00904949\n",
      "Iteration 3, loss = 0.00900301\n",
      "Iteration 4, loss = 0.00897440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.00895295\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   56 | 10m29s |    0.95231 |  125.7681 |  177.4500 |  123.0401 |    0.0614 | \n",
      "Iteration 1, loss = 0.02826262\n",
      "Iteration 2, loss = 0.01393317\n",
      "Iteration 3, loss = 0.01326702\n",
      "Iteration 4, loss = 0.01301034\n",
      "Iteration 5, loss = 0.01289063\n",
      "Iteration 6, loss = 0.01284299\n",
      "Iteration 7, loss = 0.01279609\n",
      "Iteration 8, loss = 0.01275392\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   57 | 02m59s |    0.95231 |  157.9416 |  120.3326 |  111.7916 |    0.9003 | \n",
      "Iteration 1, loss = 0.03073818\n",
      "Iteration 2, loss = 0.01420939\n",
      "Loaded classifiers.\n",
      "Collected features.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7d0804ad4ff3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Finally we call .maximize method of the optimizer with the appropriate arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ucb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgp_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, init_points)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Evaluate target function at all initialization points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observe_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Add the points from `self.initialize` to the observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36m_observe_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_observe_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mobserve_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# measure the target function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-7d0804ad4ff3>\u001b[0m in \u001b[0;36mmaximizer\u001b[0;34m(alpha, N1, N2, N3)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# calculate out-of-the-box roc_score using validation set 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecondary_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val2_trans_pl2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mval2_roc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \"\"\"\n\u001b[1;32m   1049\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coefs_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    676\u001b[0m                                          layer_units[i + 1])))\n\u001b[1;32m    677\u001b[0m         \u001b[0;31m# forward propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[0;34m(self, activations)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[0;32m--> 105\u001b[0;31m                                                  self.coefs_[i])\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization\n",
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return validated 'roc_auc' score from Classifier\n",
    "# Note that the parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def maximizer(alpha,N1,N2,N3):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    estimator_function = MLPClassifier(alpha = alpha,\n",
    "                                       hidden_layer_sizes=(int(N1),int(N2),int(N3),),\n",
    "                                       verbose = True, warm_start = True)\n",
    "                                       \n",
    "    \n",
    "    # Fit the estimator\n",
    "    estimator_function.fit(X_train_ensemble, y_val1)\n",
    "        \n",
    "    # calculate out-of-the-box roc_score using validation set 2\n",
    "    probs = estimator_function.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "    val2_roc = roc_auc_score(y_val2,probs)\n",
    "    \n",
    "    # return the validation score to be maximized \n",
    "    return val2_roc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "\n",
    "hyperparameter_space = {\n",
    "        'alpha': (0.00001,1),\n",
    "        'N1': (10,200),\n",
    "        'N2': (10,200),\n",
    "        'N3': (10,200)\n",
    "}\n",
    "\n",
    "BO = BayesianOptimization(f = maximizer, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "\n",
    "BO.maximize(init_points=100,n_iter=100,acq='ucb', kappa= 5, **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed test sparse matrix.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Prepared MLPC ensemble probs.\n",
      "File saved as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/ENSB_MLPC_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare a submission using the tuned MLPC classifier\n",
    "\n",
    "import pandas as pd\n",
    "click_id = pd.read_hdf(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/click_id.h5\")\n",
    "def prepare_submission(predictions,filename = \"new_submission\", click_id = click_id):\n",
    "    \"\"\"predictions: a list containing the predicted probabilities in the test set. \"\"\"\n",
    "    is_attributed = pd.Series(predictions)\n",
    "    submission_frame = pd.DataFrame()\n",
    "    submission_frame[\"click_id\"] = click_id\n",
    "    submission_frame[\"is_attributed\"] = is_attributed.apply(lambda x: format(x,\".9f\"))  # Reformat the probabilities upto the 9th decimal point\n",
    "    filename = filename + \".csv\"\n",
    "    submission_frame.to_csv(filename,index = False)\n",
    "    print(\"File saved as :\" + filename)\n",
    "\n",
    "\n",
    "# let's perform a prediction using the test set\n",
    "# Load the sparse matrix \n",
    "import scipy.sparse as sp\n",
    "test_proc_p12 = sp.load_npz(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl2.npz\").tocsr()\n",
    "print(\"Loaded processed test sparse matrix.\")\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(test_proc_p12))[:,1]\n",
    "print(\"Prepared MLPC ensemble probs.\")\n",
    "# Prepare the submission file\n",
    "prepare_submission(predictions= probs, \n",
    "                   filename= \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/ENSB_MLPC_submission\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ensemble models to train a weight-based meta-model using Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classifiers.\n",
      "Collected features.\n",
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |        w1 | \n",
      "    1 | 00m00s | \u001b[35m   0.95257\u001b[0m | \u001b[32m   0.1750\u001b[0m | \n",
      "    2 | 00m00s | \u001b[35m   0.95259\u001b[0m | \u001b[32m   0.2281\u001b[0m | \n",
      "    3 | 00m00s |    0.95258 |    0.2900 | \n",
      "    4 | 00m00s |    0.95250 |    0.1151 | \n",
      "    5 | 00m00s | \u001b[35m   0.95259\u001b[0m | \u001b[32m   0.2554\u001b[0m | \n",
      "    6 | 00m00s |    0.95259 |    0.2665 | \n",
      "    7 | 00m00s |    0.95249 |    0.1110 | \n",
      "    8 | 00m00s |    0.95259 |    0.2635 | \n",
      "    9 | 00m00s |    0.95258 |    0.2771 | \n",
      "   10 | 00m00s | \u001b[35m   0.95260\u001b[0m | \u001b[32m   0.2445\u001b[0m | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |        w1 | \n",
      "   11 | 00m07s |    0.95260 |    0.2359 | \n",
      "   12 | 00m05s |    0.95259 |    0.2539 | \n",
      "   13 | 00m04s |    0.95257 |    0.1624 | \n",
      "   14 | 00m03s |    0.95259 |    0.2151 | \n",
      "   15 | 00m06s |    0.95252 |    0.1291 | \n",
      "   16 | 00m03s |    0.95260 |    0.2377 | \n",
      "   17 | 00m08s |    0.95255 |    0.1510 | \n",
      "   18 | 00m05s |    0.95259 |    0.2265 | \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-99832b6c9a07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Finally we call .maximize method of the optimizer with the appropriate arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ucb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgp_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    299\u001b[0m                             \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m                             **self._acqkw)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;31m# Keep track of total number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/helpers.py\u001b[0m in \u001b[0;36macq_max\u001b[0;34m(ac, gp, y_max, bounds, random_state, n_warmup, n_iter)\u001b[0m\n\u001b[1;32m     58\u001b[0m                        \u001b[0mx_try\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                        \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                        method=\"L-BFGS-B\")\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Store it if better than previous minimum(maximum).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 487\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_approx_fprime_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/helpers.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx_try\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_seeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Find the minimum of minus the acquisition function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp, y_max=y_max),\n\u001b[0m\u001b[1;32m     58\u001b[0m                        \u001b[0mx_try\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                        \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/helpers.py\u001b[0m in \u001b[0;36mutility\u001b[0;34m(self, x, gp, y_max)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ucb'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ucb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkappa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ei'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ei\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/helpers.py\u001b[0m in \u001b[0;36m_ucb\u001b[0;34m(x, gp, kappa)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ucb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkappa\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, return_std, return_cov)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \"returning full covariance.\")\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X_train_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unfitted;predict based on GP prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "\n",
    "import pandas as pd\n",
    "# Load established classifiers\n",
    "with open((diskname + str(\"log_final.pkl\")), \"rb\") as f:\n",
    "    clf1 = pickle.load(f)\n",
    "with open((diskname + str(\"svm_final.pkl\")), \"rb\") as f:\n",
    "    clf2 = pickle.load(f)   \n",
    "print(\"Loaded classifiers.\")    \n",
    "# Collect prediction probabilities as new features    \n",
    "f1 = clf1.predict_proba(X_val2_trans_pl2)[:,1]\n",
    "f2 = clf2.predict_proba(X_val2_trans_pl2)[:,1]\n",
    "print(\"Collected features.\")\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def maximizer(w1):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    w2 = abs(1-w1)\n",
    "        \n",
    "    # calculate out-of-the-box roc_score using validation set 2\n",
    "    probs = (w1 * (np.sqrt(f1) + np.cbrt(f1))) + (w2 * (f2 + f2 **2 + f2 **3)) \n",
    "    val2_roc = roc_auc_score(y_val2,probs)\n",
    "    \n",
    "    # return the validation score to be maximized \n",
    "    return val2_roc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "\n",
    "hyperparameter_space = {\n",
    "        'w1': (0.1,0.3)\n",
    "}\n",
    "\n",
    "BO = BayesianOptimization(f = maximizer, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "\n",
    "BO.maximize(init_points=10,n_iter=100,acq='ucb', kappa= 20, **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed test sparse matrix.\n",
      "Collected features.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-d60a7bae4bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf2\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf2\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prepared MLPC ensemble probs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Prepare the submission file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's submit a prediction we prepared this emprical way\n",
    "# Load the sparse matrix \n",
    "import scipy.sparse as sp\n",
    "test_proc_p12 = sp.load_npz(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl2.npz\").tocsr()\n",
    "print(\"Loaded processed test sparse matrix.\")\n",
    "\n",
    "import pandas as pd\n",
    "click_id = pd.read_hdf(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/click_id.h5\")\n",
    "def prepare_submission(predictions,filename = \"new_submission\", click_id = click_id):\n",
    "    \"\"\"predictions: a list containing the predicted probabilities in the test set. \"\"\"\n",
    "    is_attributed = pd.Series(predictions)\n",
    "    submission_frame = pd.DataFrame()\n",
    "    submission_frame[\"click_id\"] = click_id\n",
    "    submission_frame[\"is_attributed\"] = is_attributed.apply(lambda x: format(x,\".9f\"))  # Reformat the probabilities upto the 9th decimal point\n",
    "    filename = filename + \".csv\"\n",
    "    submission_frame.to_csv(filename,index = False)\n",
    "    print(\"File saved as :\" + filename)\n",
    "\n",
    "# Collect prediction probabilities as new features    \n",
    "f1 = clf1.predict_proba(test_proc_p12)[:,1]\n",
    "f2 = clf2.predict_proba(test_proc_p12)[:,1]\n",
    "print(\"Collected features.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared WB ensemble probs.\n",
      "File saved as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/ENSB_WB_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "w1 = 0.2445\n",
    "w2 = abs(1-w1)\n",
    "\n",
    "probs = (w1 * (np.sqrt(f1) + np.cbrt(f1))) + (w2 * (f2 + f2 **2 + f2 **3)) \n",
    "print(\"Prepared WB ensemble probs.\")\n",
    "# Prepare the submission file\n",
    "prepare_submission(predictions= probs, \n",
    "                   filename= \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/ENSB_WB_submission\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming data approach to train a Multinomial Naive Bayes Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to stream and process mini-batches of training data\n",
    "# We will avoid the first 3 million rows from the training set since this includes the validation set \n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Label text features\n",
    "Text_features = [\"app\",\"device\",\"os\",\"channel\"]\n",
    "\n",
    "##############################################################\n",
    "# Define utility function to parse and process text features\n",
    "##############################################################\n",
    "# Note we avoid lambda functions since they don't pickle when we want to save the pipeline later   \n",
    "def column_text_processer_nolambda(df,text_columns = Text_features):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \"\"\"\"A function that will merge/join all text in a given row to make it ready for tokenization. \n",
    "    - This function should take care of converting missing values to empty strings. \n",
    "    - It should also convert the text to lowercase.\n",
    "    df= pandas dataframe\n",
    "    text_columns = names of the text features in df\n",
    "    \"\"\" \n",
    "    # Select only non-text columns that are in the df\n",
    "    text_data = df[text_columns]\n",
    "    \n",
    "    # Fill the missing values in text_data using empty strings\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    \n",
    "    # Concatenate feature name to each category encoding for each row\n",
    "    # E.g: encoding 3 at device column will read as device3 to make each encoding unique for a given feature\n",
    "    for col_index in list(text_data.columns):\n",
    "        text_data[col_index] = col_index + text_data[col_index].astype(str)\n",
    "    \n",
    "    # Join all the strings in a given row to make a vector\n",
    "    # text_vector = text_data.apply(lambda x: \" \".join(x), axis = 1)\n",
    "    text_vector = []\n",
    "    for index,rows in text_data.iterrows():\n",
    "        text_item = \" \".join(rows).lower()\n",
    "        text_vector.append(text_item)\n",
    "\n",
    "    # return text_vector as pd.Series object to enter the tokenization pipeline\n",
    "    return pd.Series(text_vector)\n",
    "\n",
    "#######################################################################\n",
    "# Define custom processing functions to add the log_total_clicks and \n",
    "# log_total_click_time features, and remove the unwanted base features\n",
    "#######################################################################\n",
    "def column_time_processer(X_train):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert click_time to datetime64 dtype \n",
    "    X_train.click_time = pd.to_datetime(X_train.click_time)\n",
    "\n",
    "    # Calculate the log_total_clicks for each ip and add as a new feature to temp_data\n",
    "    temp_data = pd.DataFrame(np.log(X_train.groupby([\"ip\"]).size()),\n",
    "                                    columns = [\"log_total_clicks\"]).reset_index()\n",
    "\n",
    "\n",
    "    # Calculate the log_total_click_time for each ip and add as a new feature to temp_data\n",
    "    # First define a function to process selected ip group \n",
    "    def get_log_total_click_time(group):\n",
    "        diff = (max(group.click_time) - min(group.click_time)).seconds\n",
    "        return np.log(diff+1)\n",
    "\n",
    "    # Then apply this function to each ip group and extract the total click time per ip group\n",
    "    log_time_frame = pd.DataFrame(X_train.groupby([\"ip\"]).apply(get_log_total_click_time),\n",
    "                                  columns=[\"log_total_click_time\"]).reset_index()\n",
    "\n",
    "    # Then add this new feature to the temp_data\n",
    "    temp_data = pd.merge(temp_data,log_time_frame, how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Combine temp_data with X_train to maintain X_train key order\n",
    "    temp_data = pd.merge(X_train,temp_data,how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Drop features that are not needed\n",
    "    temp_data = temp_data[[\"log_total_clicks\",\"log_total_click_time\"]]\n",
    "\n",
    "    # Return only the numeric features as a tensor to integrate into the numeric feature branch of the pipeline\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# We need to wrap these custom utility functions using FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# FunctionTransformer wrapper of utility functions to parse text and numeric features\n",
    "# Note how we avoid putting any arguments into column_text_processer or column_time_processer\n",
    "#############################################################################\n",
    "get_numeric_data = FunctionTransformer(func = column_time_processer, validate=False) \n",
    "get_text_data = FunctionTransformer(func = column_text_processer_nolambda,validate=False) \n",
    "\n",
    "#############################################################################\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "# #Note this regex will match either a whitespace or a punctuation to tokenize \n",
    "# the string vector on these preferences, in our case we only have white spaces in our text  \n",
    "#############################################################################\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Re-load the userclick_pipeline2 to work with\n",
    "import pickle\n",
    "with open(\"userclick_pipeline2.pkl\",\"rb\") as f:\n",
    "    userclick_pipeline2 = pickle.load(f)\n",
    "\n",
    "\n",
    "# This is just to initially get the column names\n",
    "training_set = pd.read_csv(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/train.csv\",\n",
    "                           nrows=2,\n",
    "                           dtype = \"str\")\n",
    "column_names = training_set.columns\n",
    "\n",
    "train_file = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/train.csv\"\n",
    "\n",
    "def stream_training_data(filename,batchsize,skiprows = 3000000):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    batch = pd.read_csv(filename,skiprows = skiprows,names = list(column_names),nrows= batchsize,dtype = \"str\")\n",
    "    print(\"Read the new batch.\")\n",
    "    # Seperate features and labels\n",
    "    X_train = batch.drop([\"is_attributed\",\"attributed_time\"], axis = 1)\n",
    "    y_train = pd.to_numeric(batch.is_attributed)\n",
    "    # Process features using the pipeline\n",
    "    X_train = userclick_pipeline2.transform(X_train)\n",
    "    print(\"Processed the new batch.\")\n",
    "    # Return features and labels ready for training\n",
    "    return (X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classifiers.\n"
     ]
    }
   ],
   "source": [
    "# Write a function that return secondary features\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "import pandas as pd\n",
    "# Load established classifiers\n",
    "with open((diskname + str(\"log_final.pkl\")), \"rb\") as f:\n",
    "    clf1 = pickle.load(f)\n",
    "with open((diskname + str(\"svm_final.pkl\")), \"rb\") as f:\n",
    "    clf2 = pickle.load(f)\n",
    "with open((diskname + str(\"ab_final.pkl\")), \"rb\") as f:\n",
    "    clf3 = pickle.load(f)    \n",
    "print(\"Loaded classifiers.\") \n",
    "    \n",
    "def secondary_features(X_train):   \n",
    "    # Collect prediction probabilities as new features    \n",
    "    sec_features = pd.DataFrame()\n",
    "    sec_features[\"f1\"] = clf1.predict_proba(X_train)[:,1]\n",
    "    sec_features[\"f2\"] = clf2.predict_proba(X_train)[:,1]\n",
    "    sec_features[\"f3\"] = clf3.predict_proba(X_train)[:,1]\n",
    "    print(\">>> From secondary_features: Collected features.\")\n",
    "    # Return new features as array\n",
    "    return sec_features.values\n",
    "\n",
    "# Prepare two validation sets: load and transform them into secondary features\n",
    "# Features:\n",
    "with open(\"X_val1_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val1 = secondary_features(pickle.load(f)) \n",
    "# labels:    \n",
    "with open(\"y_val1.pkl\",\"rb\") as f:\n",
    "    y_val1= pickle.load(f)\n",
    "\n",
    "print(\"Loaded and transformed Val 1.\")    \n",
    "    \n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2 = secondary_features(pickle.load(f)) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "\n",
    "print(\"Loaded and transformed Val 2.\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a new batch.\n",
      "Read the new batch.\n",
      "Processed the new batch.\n",
      "Streamed new batch of training data.\n",
      ">>> From secondary_features: Collected features.\n",
      "Processed new batch of training data.\n",
      "Trained mnbi classifier.\n",
      "Trained sgdc classifier.\n",
      "mnbi completed batch 1 with validation score: 0.95207429499\n",
      "mnbi_vscore_max exceeded, model updated.\n",
      "sgdc completed batch 1 with validation score: 0.952631656404\n",
      "sgdc_vscore_max exceeded, model updated.\n",
      "Batch completed in: 0.5666666666666667 minutes.\n",
      "So far it took: 0.5666666666666667 minutes.\n",
      "So far model have seen 10000 training samples.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting a new batch.\n",
      "Read the new batch.\n",
      "Processed the new batch.\n",
      "Streamed new batch of training data.\n",
      ">>> From secondary_features: Collected features.\n",
      "Processed new batch of training data.\n",
      "Trained mnbi classifier.\n",
      "Trained sgdc classifier.\n",
      "mnbi completed batch 2 with validation score: 0.952087729666\n",
      "mnbi_vscore_max exceeded, model updated.\n",
      "sgdc completed batch 2 with validation score: 0.952688759544\n",
      "sgdc_vscore_max exceeded, model updated.\n",
      "Batch completed in: 0.6333333333333333 minutes.\n",
      "So far it took: 1.2 minutes.\n",
      "So far model have seen 20000 training samples.\n",
      "--------------------------------------------------------------------------------\n",
      "Starting a new batch.\n",
      "Read the new batch.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-e3124d98c5ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Stream a small batch of training data to convert into secondary features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Streamed new batch of training data.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mX_train_ensemble\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecondary_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-790459489712>\u001b[0m in \u001b[0;36mstream_training_data\u001b[0;34m(filename, batchsize, skiprows)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_attributed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# Process features using the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserclick_pipeline2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processed the new batch.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# Return features and labels ready for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/2016/Data_science/Kaggle/User-click-detection-predictive-modeling/SparseInteractions.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_col_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mspi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_sparse_interactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mspi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/2016/Data_science/Kaggle/User-click-detection-predictive-modeling/SparseInteractions.py\u001b[0m in \u001b[0;36m_create_sparse_interactions\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# get column multiplications value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_ixs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol_ixs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/csc.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    165\u001b[0m         if (isinstance(row, slice) or isinstance(col, slice) or\n\u001b[1;32m    166\u001b[0m                 isintlike(row) or isintlike(col)):\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;31m# Things that return a sequence of values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'T'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'H'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(self, axes, copy)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcsc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsc_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         return csc_matrix((self.data, self.indices,\n\u001b[0;32m--> 141\u001b[0;31m                            self.indptr), shape=(N, M), copy=copy)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mtranspose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0midx_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_index_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Write a main loop to train classifiers and accumulate performance metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "\n",
    "# Instantiate classifiers\n",
    "mnbi = MultinomialNB(alpha = 0.01)\n",
    "sgdc = SGDClassifier(loss=\"log\")\n",
    "\n",
    "all_classes = np.array([0, 1])\n",
    "skiprows = 3000000\n",
    "batchsize = 10000\n",
    "mnbi_vscore = []\n",
    "sgdc_vcsore = []\n",
    "mnbi_vscore_max = 0\n",
    "sgdc_vcsore_max = 0\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "for i in range(1,10000):\n",
    "    start_batch = datetime.datetime.now()\n",
    "    print(\"Starting a new batch.\")\n",
    "    \n",
    "    ##################\n",
    "    # Data Streaming\n",
    "    ##################\n",
    "    \n",
    "    # Stream a small batch of training data to convert into secondary features\n",
    "    X_train, y_train = stream_training_data(filename= train_file,batchsize= batchsize, skiprows= skiprows)\n",
    "    print(\"Streamed new batch of training data.\")\n",
    "    X_train_ensemble = secondary_features(X_train)\n",
    "    print(\"Processed new batch of training data.\")\n",
    "    skiprows += batchsize\n",
    "    \n",
    "    #######################\n",
    "    # Training Classifiers\n",
    "    #######################\n",
    "    \n",
    "    # Train mnbi classifier using partial_fit and secondary features\n",
    "    mnbi.partial_fit(X_train_ensemble, y_train,all_classes)\n",
    "    print(\"Trained mnbi classifier.\")\n",
    "    \n",
    "    # Train sgdc classifier using partial_fit and secondary features\n",
    "    sgdc.partial_fit(X_train_ensemble, y_train,all_classes)\n",
    "    print(\"Trained sgdc classifier.\")\n",
    "    \n",
    "    ###############################\n",
    "    # Model validation and Update\n",
    "    ###############################\n",
    "    \n",
    "    \n",
    "    ###############\n",
    "    clf = mnbi\n",
    "    # Make predictions and calculate average valdation roc score \n",
    "    # calculate out-of-the-box roc_score using validation set 1\n",
    "    probs = clf.predict_proba(X_val1)\n",
    "    probs = probs[:,1]\n",
    "    val1_roc = roc_auc_score(y_val1,probs)\n",
    "    \n",
    "    # calculate out-of-the-box roc_score using validation set 2\n",
    "    probs = clf.predict_proba(X_val2)\n",
    "    probs = probs[:,1]\n",
    "    val2_roc = roc_auc_score(y_val2,probs)\n",
    "    \n",
    "    # Accumulate the mean validation score to be maximized \n",
    "    mnbi_vscore.append(np.array([val1_roc,val2_roc]).mean())\n",
    "    print(\"mnbi completed batch \" + str(i) + \" with validation score: \" + str(np.array([val1_roc,val2_roc]).mean()))\n",
    "    \n",
    "    # Save the interim model if validation score has increased\n",
    "    if np.array([val1_roc,val2_roc]).mean() > mnbi_vscore_max:\n",
    "        mnbi_vscore_max = np.array([val1_roc,val2_roc]).mean()\n",
    "        with open((diskname + str(\"ensmb_mnbi_final.pkl\")), \"wb\") as f:\n",
    "            pickle.dump(mnbi,f)\n",
    "        print(\"**** mnbi_vscore_max exceeded, model updated.****\")\n",
    "    \n",
    "    ################\n",
    "    clf = sgdc\n",
    "    # Make predictions and calculate average valdation roc score \n",
    "    # calculate out-of-the-box roc_score using validation set 1\n",
    "    probs = clf.predict_proba(X_val1)\n",
    "    probs = probs[:,1]\n",
    "    val1_roc = roc_auc_score(y_val1,probs)\n",
    "    \n",
    "    # calculate out-of-the-box roc_score using validation set 2\n",
    "    probs = clf.predict_proba(X_val2)\n",
    "    probs = probs[:,1]\n",
    "    val2_roc = roc_auc_score(y_val2,probs)\n",
    "    \n",
    "    # Accumulate the mean validation score to be maximized \n",
    "    sgdc_vcsore.append(np.array([val1_roc,val2_roc]).mean())\n",
    "    print(\"sgdc completed batch \" + str(i) + \" with validation score: \" + str(np.array([val1_roc,val2_roc]).mean()))\n",
    "    \n",
    "    # Save the interim model if validation score has increased\n",
    "    if np.array([val1_roc,val2_roc]).mean() > sgdc_vcsore_max:\n",
    "        sgdc_vcsore_max = np.array([val1_roc,val2_roc]).mean()\n",
    "        with open((diskname + str(\"ensmb_sgdc_final.pkl\")), \"wb\") as f:\n",
    "            pickle.dump(sgdc,f)\n",
    "        print(\"**** sgdc_vscore_max exceeded, model updated. ****\")\n",
    "    \n",
    "    \n",
    "    end_batch = datetime.datetime.now()\n",
    "    process_time = end_batch - start_batch\n",
    "    process_todate = end_batch - start\n",
    "    print(\"Batch completed in: \" +str(process_time.seconds/60) + \" minutes.\" )\n",
    "    print(\"So far it took: \" +str(process_todate.seconds/60) + \" minutes.\" )\n",
    "    print(\"So far model have seen \" + str(i * batchsize) + \" training samples.\")\n",
    "    print(\"--\" * 40)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open((diskname + str(\"ensmb_sgdc_final.pkl\")), \"rb\") as f:\n",
    "    ensb = pickle.load(f)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
