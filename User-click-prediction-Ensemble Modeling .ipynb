{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble modeling approach\n",
    "\n",
    "In this exercise, we will try to ensemble predictions from the best models we trained in our first approach and will try to train a second tier model.\n",
    "\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/X_train_balanced_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_train_balanced_trans_pl2 = pickle.load(f) \n",
    "\n",
    "with open(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/y_train_balanced.pkl\", \"rb\") as f:\n",
    "    y_train_balanced = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Trained model, it took: 1430.3166666666666 minutes.\n",
      "Val1 ROC score: 0.952874287995\n",
      "Val2 ROC score: 0.951511484007\n",
      "Saved final logistic regression classifier.\n"
     ]
    }
   ],
   "source": [
    "# Let's use C = 3.0589 to re-train the classifier and prepare another submission\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "logbal = LogisticRegression(verbose=10, n_jobs=3, C= 3.0589,penalty= \"l1\")\n",
    "\n",
    "logbal.fit(X_train_balanced_trans_pl2, y_train_balanced)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation sets\n",
    "# Load validation sets previously prepared\n",
    "\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val1_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val1_trans_pl2 = pickle.load(f)\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "    \n",
    "# Target labels:    \n",
    "with open(\"y_val1.pkl\",\"rb\") as f:\n",
    "    y_val1= pickle.load(f)\n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 1\n",
    "probs = logbal.predict_proba(X_val1_trans_pl2)\n",
    "probs = probs[:,1]\n",
    "print(\"Val1 ROC score: \" +str(roc_auc_score(y_val1,probs)))\n",
    "       \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "probs = logbal.predict_proba(X_val2_trans_pl2)\n",
    "probs = probs[:,1]\n",
    "print(\"Val2 ROC score: \" +str(roc_auc_score(y_val2,probs))) \n",
    "\n",
    "\n",
    "# Save the final classifier\n",
    "with open((diskname + str(\"log_final.pkl\")), \"wb\") as f:\n",
    "    pickle.dump(logbal,f)\n",
    "    \n",
    "print(\"Saved final logistic regression classifier.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear]It took: 0.4 minutes.\n",
      "Val1 ROC score: 0.952513357043\n",
      "Val2 ROC score: 0.952253716778\n",
      "Saved final SVM regression classifier.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "# Note that we can't get probabilities directly from this LinearSVC function\n",
    "# We need to wrap into Calibrated Classifier \n",
    "# (see: https://stackoverflow.com/questions/35212213/sklearn-how-to-get-decision-probabilities-for-linearsvc-classifier)\n",
    "\n",
    "lsvcbal = LinearSVC(verbose=10, C = 0.0564)\n",
    "\n",
    "cal_lsvcbal = CalibratedClassifierCV(base_estimator = lsvcbal,\n",
    "                                  cv = 3, # Also performs cross-validation\n",
    "                                  method= \"sigmoid\") # We use sigmoid function to get probabilities\n",
    "\n",
    "cal_lsvcbal.fit(X_train_balanced_trans_pl2,y_train_balanced)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")\n",
    "\n",
    "\n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 1\n",
    "probs = cal_lsvcbal.predict_proba(X_val1_trans_pl2)\n",
    "probs = probs[:,1]\n",
    "print(\"Val1 ROC score: \" +str(roc_auc_score(y_val1,probs)))\n",
    "       \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "probs = cal_lsvcbal.predict_proba(X_val2_trans_pl2)\n",
    "probs = probs[:,1]\n",
    "print(\"Val2 ROC score: \" +str(roc_auc_score(y_val2,probs)))\n",
    "\n",
    "\n",
    "# Save the final classifier\n",
    "with open((diskname + str(\"svm_final.pkl\")), \"wb\") as f:\n",
    "    pickle.dump(cal_lsvcbal,f)\n",
    "    \n",
    "print(\"Saved final SVM regression classifier.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the ensemble classifier\n",
    "\n",
    "We will use X_val1 predictions from the two of our best models to train a new classifier against the y_val1. We will try to optimize the performance of this new classifier using X_val2 and y_val2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Features:\n",
    "with open(\"X_val1_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val1_trans_pl2 = pickle.load(f)\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "    \n",
    "# Target labels:    \n",
    "with open(\"y_val1.pkl\",\"rb\") as f:\n",
    "    y_val1= pickle.load(f)\n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that return secondary features\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "def secondary_features(X_train):\n",
    "    import pandas as pd\n",
    "    # Load established classifiers\n",
    "    with open((diskname + str(\"log_final.pkl\")), \"rb\") as f:\n",
    "        clf1 = pickle.load(f)\n",
    "    with open((diskname + str(\"svm_final.pkl\")), \"rb\") as f:\n",
    "        clf2 = pickle.load(f)\n",
    "    print(\"Loaded classifiers.\")    \n",
    "    # Collect prediction probabilities as new features    \n",
    "    sec_features = pd.DataFrame()\n",
    "    sec_features[\"f1\"] = clf1.predict_proba(X_train)[:,1]\n",
    "    sec_features[\"f2\"] = clf2.predict_proba(X_train)[:,1]\n",
    "    print(\"Collected features.\")\n",
    "    \n",
    "    # Return new features as array\n",
    "    return sec_features.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classifiers.\n",
      "Collected features.\n"
     ]
    }
   ],
   "source": [
    "X_train_ensemble = secondary_features(X_val1_trans_pl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ensemble.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Logisticregression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Trained ensemble model, it took: 1439.9666666666667 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.95217429678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = LogisticRegression(verbose=10)\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear]Trained ensemble model, it took: 1439.3833333333334 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.951845649207\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC  \n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lsvcbal = LinearSVC(verbose=10, C= 5)\n",
    "\n",
    "ensb = CalibratedClassifierCV(base_estimator = lsvcbal,\n",
    "                                  cv = 3, # Also performs cross-validation\n",
    "                                  method= \"sigmoid\") # We use sigmoid function to get probabilities\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.1166666666666 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.815179565761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = RandomForestClassifier()\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.9833333333333 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.557963136275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = MultinomialNB()\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble XGboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.2333333333333 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.950612549739\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = xgb.XGBClassifier()\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble QDA classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.9833333333333 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.952400780002\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = QuadraticDiscriminantAnalysis(reg_param= 0.005)\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |        p1 |        p2 |   reg_param | \n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    1 | 00m05s | \u001b[35m   0.95241\u001b[0m | \u001b[32m   0.8005\u001b[0m | \u001b[32m   0.0819\u001b[0m | \u001b[32m     0.0044\u001b[0m | \n",
      "Loaded classifiers.\n",
      "Collected features.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-babf4ccdbbb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Finally we call .maximize method of the optimizer with the appropriate arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ucb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgp_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, init_points)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Evaluate target function at all initialization points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observe_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Add the points from `self.initialize` to the observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36m_observe_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_observe_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mobserve_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# measure the target function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-babf4ccdbbb5>\u001b[0m in \u001b[0;36mmaximizer\u001b[0;34m(reg_param, p1, p2)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# calculate out-of-the-box roc_score using validation set 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecondary_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val2_trans_pl2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mval2_roc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# return the validation score to be maximized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    275\u001b[0m     return _average_binary_score(\n\u001b[1;32m    276\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         fpr, tpr, tresholds = roc_curve(y_true, y_score,\n\u001b[0;32m--> 272\u001b[0;31m                                         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreorder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \"\"\"\n\u001b[1;32m    533\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 534\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;31m# sort scores and corresponding truth values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0mdesc_score_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mergesort\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdesc_score_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdesc_score_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m     \"\"\"\n\u001b[0;32m--> 907\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argsort'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization\n",
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return validated 'roc_auc' score from Classifier\n",
    "# Note that the parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def maximizer(reg_param,p1,p2):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    estimator_function = QuadraticDiscriminantAnalysis(reg_param= reg_param, priors = [p1,p2])\n",
    "    \n",
    "    # Fit the estimator\n",
    "    estimator_function.fit(X_train_ensemble, y_val1)\n",
    "        \n",
    "    # calculate out-of-the-box roc_score using validation set 2\n",
    "    probs = estimator_function.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "    val2_roc = roc_auc_score(y_val2,probs)\n",
    "    \n",
    "    # return the validation score to be maximized \n",
    "    return val2_roc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "\n",
    "hyperparameter_space = {\n",
    "        'reg_param': (0.004,0.005),\n",
    "        \"p1\": (0.8,0.999),\n",
    "        \"p2\": (0.01,0.2)\n",
    "}\n",
    "\n",
    "BO = BayesianOptimization(f = maximizer, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "\n",
    "BO.maximize(init_points=10,n_iter=10,acq='ucb', kappa= 5, **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0041,\n",
       "               store_covariance=False, store_covariances=None, tol=0.0001)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare an ensemble prediction using QDA classifier\n",
    "# Train with tuned parameter\n",
    "ensb = QuadraticDiscriminantAnalysis(reg_param= 0.0041)\n",
    "ensb.fit(X_train_ensemble, y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed test sparse matrix.\n"
     ]
    }
   ],
   "source": [
    "# Prepare a submission using the tuned QDA classifier\n",
    "\n",
    "import pandas as pd\n",
    "click_id = pd.read_hdf(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/click_id.h5\")\n",
    "def prepare_submission(predictions,filename = \"new_submission\", click_id = click_id):\n",
    "    \"\"\"predictions: a list containing the predicted probabilities in the test set. \"\"\"\n",
    "    is_attributed = pd.Series(predictions)\n",
    "    submission_frame = pd.DataFrame()\n",
    "    submission_frame[\"click_id\"] = click_id\n",
    "    submission_frame[\"is_attributed\"] = is_attributed.apply(lambda x: format(x,\".9f\"))  # Reformat the probabilities upto the 9th decimal point\n",
    "    filename = filename + \".csv\"\n",
    "    submission_frame.to_csv(filename,index = False)\n",
    "    print(\"File saved as :\" + filename)\n",
    "\n",
    "\n",
    "# let's perform a prediction using the test set\n",
    "# Load the sparse matrix \n",
    "import scipy.sparse as sp\n",
    "test_proc_p12 = sp.load_npz(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl2.npz\").tocsr()\n",
    "print(\"Loaded processed test sparse matrix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Prepared QDA ensemble probs.\n",
      "File saved as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/ENSB_QDA_submission.csv\n"
     ]
    }
   ],
   "source": [
    "probs = ensb.predict_proba(secondary_features(test_proc_p12))[:,1]\n",
    "print(\"Prepared QDA ensemble probs.\")\n",
    "# Prepare the submission file\n",
    "prepare_submission(predictions= probs, \n",
    "                   filename= \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/ENSB_QDA_submission\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission scored 0.9605."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.95 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9c1a299b22d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Note that we are converting features to secondary features since we are using ensemble model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecondary_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val2_trans_pl2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ensemble Val ROC score: \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/neighbors/classification.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;31m# a simple ':' index doesn't work right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# loop is O(n_neighbors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mproba_k\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;31m# normalize 'votes' into real [0,1] probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = KNeighborsClassifier(n_neighbors= 150)\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble LDA classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.9833333333333 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.952363803875\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = LinearDiscriminantAnalysis(shrinkage= 'auto', solver= 'lsqr')\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.9833333333333 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.952297825127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = GaussianNB(priors = [0.999999,0.000001])\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ensemble model, it took: 1439.5666666666666 minutes.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Ensemble Val ROC score: 0.95230705822\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "ensb = MLPClassifier(alpha = 0.1, hidden_layer_sizes=(10,10,10,))\n",
    "ensb.fit(X_train_ensemble, y_val1)\n",
    "\n",
    "end2 = datetime.datetime.now()\n",
    "process_time = start - end2\n",
    "print(\"Trained ensemble model, it took: \" + str((process_time.seconds)/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "diskname = \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/\"\n",
    "\n",
    "# Test performance using the validation set 2\n",
    "# Load validation set previously prepared\n",
    "\n",
    "# Features:\n",
    "with open(\"X_val2_trans_pl2.pkl\",\"rb\") as f:\n",
    "    X_val2_trans_pl2 = pickle.load(f) \n",
    "# labels:    \n",
    "with open(\"y_val2.pkl\",\"rb\") as f:\n",
    "    y_val2= pickle.load(f)\n",
    "    \n",
    "# Make predictions and calculate average valdation roc score \n",
    "# calculate out-of-the-box roc_score using validation set 2\n",
    "# Note that we are converting features to secondary features since we are using ensemble model\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "print(\"Ensemble Val ROC score: \" +str(roc_auc_score(y_val2,probs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10, 10, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.5,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensb.get_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |        N1 |        N2 |        N3 |     alpha | \n",
      "Iteration 1, loss = 0.02328634\n",
      "Iteration 2, loss = 0.01417183\n",
      "Iteration 3, loss = 0.01323872\n",
      "Iteration 4, loss = 0.01295399\n",
      "Iteration 5, loss = 0.01282552\n",
      "Iteration 6, loss = 0.01278816\n",
      "Iteration 7, loss = 0.01274812\n",
      "Iteration 8, loss = 0.01273363\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    1 | 04m06s | \u001b[35m   0.50000\u001b[0m | \u001b[32m 138.1691\u001b[0m | \u001b[32m 146.4545\u001b[0m | \u001b[32m 163.6315\u001b[0m | \u001b[32m   0.3751\u001b[0m | \n",
      "Iteration 1, loss = 0.02400890\n",
      "Iteration 2, loss = 0.01370346\n",
      "Iteration 3, loss = 0.01319368\n",
      "Iteration 4, loss = 0.01297327\n",
      "Iteration 5, loss = 0.01287496\n",
      "Iteration 6, loss = 0.01281547\n",
      "Iteration 7, loss = 0.01278381\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    2 | 01m56s | \u001b[35m   0.95234\u001b[0m | \u001b[32m  41.7572\u001b[0m | \u001b[32m  98.4419\u001b[0m | \u001b[32m 146.8079\u001b[0m | \u001b[32m   0.6403\u001b[0m | \n",
      "Iteration 1, loss = 0.02905142\n",
      "Iteration 2, loss = 0.01398987\n",
      "Iteration 3, loss = 0.01329819\n",
      "Iteration 4, loss = 0.01304417\n",
      "Iteration 5, loss = 0.01292479\n",
      "Iteration 6, loss = 0.01285939\n",
      "Iteration 7, loss = 0.01280956\n",
      "Iteration 8, loss = 0.01277473\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    3 | 02m31s |    0.04769 |  100.8463 |  104.1062 |  117.5487 |    0.9500 | \n",
      "Iteration 1, loss = 0.01323074\n",
      "Iteration 2, loss = 0.00936950\n",
      "Iteration 3, loss = 0.00934193\n",
      "Iteration 4, loss = 0.00932505\n",
      "Iteration 5, loss = 0.00933296\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    4 | 05m19s |    0.95231 |  139.4123 |   66.4374 |  135.2485 |    0.0757 | \n",
      "Iteration 1, loss = 0.02430100\n",
      "Iteration 2, loss = 0.01364201\n",
      "Iteration 3, loss = 0.01314227\n",
      "Iteration 4, loss = 0.01296365\n",
      "Iteration 5, loss = 0.01287378\n",
      "Iteration 6, loss = 0.01281014\n",
      "Iteration 7, loss = 0.01279053\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    5 | 01m09s |    0.04769 |   50.4390 |   24.2249 |  159.5039 |    0.7769 | \n",
      "Iteration 1, loss = 0.02656183\n",
      "Iteration 2, loss = 0.01367871\n",
      "Iteration 3, loss = 0.01319095\n",
      "Iteration 4, loss = 0.01297126\n",
      "Iteration 5, loss = 0.01287076\n",
      "Iteration 6, loss = 0.01282096\n",
      "Iteration 7, loss = 0.01278400\n",
      "Iteration 8, loss = 0.01277504\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    6 | 02m34s |    0.04771 |   85.0652 |  109.9899 |  161.8494 |    0.8327 | \n",
      "Iteration 1, loss = 0.01568289\n",
      "Iteration 2, loss = 0.00890825\n",
      "Iteration 3, loss = 0.00879751\n",
      "Iteration 4, loss = 0.00879206\n",
      "Iteration 5, loss = 0.00875018\n",
      "Iteration 6, loss = 0.00874811\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    7 | 03m36s |    0.95231 |   48.7647 |  111.2722 |   22.5143 |    0.0548 | \n",
      "Iteration 1, loss = 0.02668585\n",
      "Iteration 2, loss = 0.01378049\n",
      "Iteration 3, loss = 0.01322674\n",
      "Iteration 4, loss = 0.01299266\n",
      "Iteration 5, loss = 0.01289550\n",
      "Iteration 6, loss = 0.01285191\n",
      "Iteration 7, loss = 0.01279836\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    8 | 02m32s |    0.04768 |   48.2439 |  132.6396 |  168.0408 |    0.8177 | \n",
      "Iteration 1, loss = 0.03228374\n",
      "Iteration 2, loss = 0.01473090\n",
      "Iteration 3, loss = 0.01369837\n",
      "Iteration 4, loss = 0.01327700\n",
      "Iteration 5, loss = 0.01306305\n",
      "Iteration 6, loss = 0.01293324\n",
      "Iteration 7, loss = 0.01285012\n",
      "Iteration 8, loss = 0.01279021\n",
      "Iteration 9, loss = 0.01274936\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "    9 | 01m33s |    0.95233 |   75.3800 |   67.2475 |   43.1665 |    0.8854 | \n",
      "Iteration 1, loss = 0.02626991\n",
      "Iteration 2, loss = 0.01501972\n",
      "Iteration 3, loss = 0.01396991\n",
      "Iteration 4, loss = 0.01349758\n",
      "Iteration 5, loss = 0.01324572\n",
      "Iteration 6, loss = 0.01310094\n",
      "Iteration 7, loss = 0.01298928\n",
      "Iteration 8, loss = 0.01291262\n",
      "Iteration 9, loss = 0.01287298\n",
      "Iteration 10, loss = 0.01281574\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   10 | 02m54s |    0.04767 |  139.7419 |  161.6549 |   14.8526 |    0.7223 | \n",
      "Iteration 1, loss = 0.00985688\n",
      "Iteration 2, loss = 0.00733662\n",
      "Iteration 3, loss = 0.00713308\n",
      "Iteration 4, loss = 0.00703238\n",
      "Iteration 5, loss = 0.00698020\n",
      "Iteration 6, loss = 0.00695345\n",
      "Iteration 7, loss = 0.00691807\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   11 | 09m36s |    0.95230 |  176.8832 |   77.7823 |  123.2280 |    0.0026 | \n",
      "Iteration 1, loss = 0.02928769\n",
      "Iteration 2, loss = 0.01409641\n",
      "Iteration 3, loss = 0.01335832\n",
      "Iteration 4, loss = 0.01308178\n",
      "Iteration 5, loss = 0.01292456\n",
      "Iteration 6, loss = 0.01285394\n",
      "Iteration 7, loss = 0.01279048\n",
      "Iteration 8, loss = 0.01276728\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   12 | 02m56s |    0.95231 |   95.6265 |  146.1380 |  109.9270 |    0.9812 | \n",
      "Iteration 1, loss = 0.02116851\n",
      "Iteration 2, loss = 0.01469972\n",
      "Iteration 3, loss = 0.01464688\n",
      "Iteration 4, loss = 0.01444183\n",
      "Iteration 5, loss = 0.01369848\n",
      "Iteration 6, loss = 0.01319391\n",
      "Iteration 7, loss = 0.01297417\n",
      "Iteration 8, loss = 0.01287272\n",
      "Iteration 9, loss = 0.01280350\n",
      "Iteration 10, loss = 0.01275566\n",
      "Iteration 11, loss = 0.01271284\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   13 | 03m17s |    0.95230 |  165.8459 |  119.1200 |   46.2841 |    0.3434 | \n",
      "Iteration 1, loss = 0.01468832\n",
      "Iteration 2, loss = 0.00982150\n",
      "Iteration 3, loss = 0.00980456\n",
      "Iteration 4, loss = 0.00980036\n",
      "Iteration 5, loss = 0.00977403\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   14 | 03m10s |    0.95231 |  140.2167 |  109.8405 |   62.4206 |    0.0948 | \n",
      "Iteration 1, loss = 0.02215611\n",
      "Iteration 2, loss = 0.01417535\n",
      "Iteration 3, loss = 0.01317483\n",
      "Iteration 4, loss = 0.01294072\n",
      "Iteration 5, loss = 0.01283122\n",
      "Iteration 6, loss = 0.01279653\n",
      "Iteration 7, loss = 0.01275337\n",
      "Iteration 8, loss = 0.01275359\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   15 | 02m22s |    0.50000 |  162.6109 |   68.5861 |  166.2029 |    0.3946 | \n",
      "Iteration 1, loss = 0.01444690\n",
      "Iteration 2, loss = 0.00757882\n",
      "Iteration 3, loss = 0.00736076\n",
      "Iteration 4, loss = 0.00722844\n",
      "Iteration 5, loss = 0.00710995\n",
      "Iteration 6, loss = 0.00706428\n",
      "Iteration 7, loss = 0.00705079\n",
      "Iteration 8, loss = 0.00701850\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   16 | 03m02s |    0.95230 |   59.6327 |   38.8771 |   31.1648 |    0.0050 | \n",
      "Iteration 1, loss = 0.02372005\n",
      "Iteration 2, loss = 0.01357671\n",
      "Iteration 3, loss = 0.01310474\n",
      "Iteration 4, loss = 0.01295989\n",
      "Iteration 5, loss = 0.01286334\n",
      "Iteration 6, loss = 0.01282587\n",
      "Iteration 7, loss = 0.01280382\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   17 | 01m47s |    0.95228 |  118.9102 |   39.9617 |  194.4173 |    0.7367 | \n",
      "Iteration 1, loss = 0.02513518\n",
      "Iteration 2, loss = 0.01364272\n",
      "Iteration 3, loss = 0.01315461\n",
      "Iteration 4, loss = 0.01298473\n",
      "Iteration 5, loss = 0.01286748\n",
      "Iteration 6, loss = 0.01283835\n",
      "Iteration 7, loss = 0.01279884\n",
      "Iteration 8, loss = 0.01277039\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected features.\n",
      "   18 | 01m51s |    0.04766 |   39.3856 |   70.8663 |  185.0109 |    0.9558 | \n",
      "Iteration 1, loss = 0.02696712\n",
      "Iteration 2, loss = 0.01435707\n",
      "Iteration 3, loss = 0.01354445\n",
      "Iteration 4, loss = 0.01319261\n",
      "Iteration 5, loss = 0.01300981\n",
      "Iteration 6, loss = 0.01289042\n",
      "Iteration 7, loss = 0.01281019\n",
      "Iteration 8, loss = 0.01276652\n",
      "Iteration 9, loss = 0.01272297\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   19 | 03m06s |    0.95212 |  190.6947 |  147.1240 |   34.7259 |    0.8206 | \n",
      "Iteration 1, loss = 0.02259393\n",
      "Iteration 2, loss = 0.01470273\n",
      "Iteration 3, loss = 0.01462508\n",
      "Iteration 4, loss = 0.01404491\n",
      "Iteration 5, loss = 0.01332289\n",
      "Iteration 6, loss = 0.01305312\n",
      "Iteration 7, loss = 0.01293119\n",
      "Iteration 8, loss = 0.01283964\n",
      "Iteration 9, loss = 0.01278164\n",
      "Iteration 10, loss = 0.01273478\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   20 | 01m54s |    0.04768 |   45.4879 |  109.8809 |   62.4532 |    0.3450 | \n",
      "Iteration 1, loss = 0.02277086\n",
      "Iteration 2, loss = 0.01459894\n",
      "Iteration 3, loss = 0.01332311\n",
      "Iteration 4, loss = 0.01298281\n",
      "Iteration 5, loss = 0.01287184\n",
      "Iteration 6, loss = 0.01280849\n",
      "Iteration 7, loss = 0.01276254\n",
      "Iteration 8, loss = 0.01275126\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   21 | 00m54s |    0.04770 |   19.6975 |   10.6102 |  138.6965 |    0.3786 | \n",
      "Iteration 1, loss = 0.02636776\n",
      "Iteration 2, loss = 0.01437856\n",
      "Iteration 3, loss = 0.01334353\n",
      "Iteration 4, loss = 0.01303806\n",
      "Iteration 5, loss = 0.01290396\n",
      "Iteration 6, loss = 0.01283659\n",
      "Iteration 7, loss = 0.01277371\n",
      "Iteration 8, loss = 0.01275888\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   22 | 01m06s |    0.04770 |   59.7168 |   29.0610 |   77.0179 |    0.7848 | \n",
      "Iteration 1, loss = 0.01406295\n",
      "Iteration 2, loss = 0.00963853\n",
      "Iteration 3, loss = 0.00958067\n",
      "Iteration 4, loss = 0.00957772\n",
      "Iteration 5, loss = 0.00957110\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   23 | 03m04s |    0.95231 |   41.0262 |  123.5162 |   89.5153 |    0.0862 | \n",
      "Iteration 1, loss = 0.02693518\n",
      "Iteration 2, loss = 0.01461882\n",
      "Iteration 3, loss = 0.01354957\n",
      "Iteration 4, loss = 0.01312288\n",
      "Iteration 5, loss = 0.01294055\n",
      "Iteration 6, loss = 0.01284797\n",
      "Iteration 7, loss = 0.01277510\n",
      "Iteration 8, loss = 0.01274486\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   24 | 01m29s |    0.04767 |  164.6281 |   35.4948 |   59.6526 |    0.5461 | \n",
      "Iteration 1, loss = 0.01685885\n",
      "Iteration 2, loss = 0.01129306\n",
      "Iteration 3, loss = 0.01127431\n",
      "Iteration 4, loss = 0.01126376\n",
      "Iteration 5, loss = 0.01125370\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   25 | 01m02s |    0.95231 |   36.5309 |   99.4850 |   27.1605 |    0.1622 | \n",
      "Iteration 1, loss = 0.01990437\n",
      "Iteration 2, loss = 0.01383144\n",
      "Iteration 3, loss = 0.01379974\n",
      "Iteration 4, loss = 0.01379959\n",
      "Iteration 5, loss = 0.01377685\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   26 | 01m35s |    0.95232 |  146.4072 |   71.4337 |  107.8808 |    0.2901 | \n",
      "Iteration 1, loss = 0.01253318\n",
      "Iteration 2, loss = 0.00862738\n",
      "Iteration 3, loss = 0.00856418\n",
      "Iteration 4, loss = 0.00849720\n",
      "Iteration 5, loss = 0.00850358\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   27 | 08m09s |    0.95231 |   91.1539 |  193.5876 |   86.4950 |    0.0450 | \n",
      "Iteration 1, loss = 0.02113260\n",
      "Iteration 2, loss = 0.01452223\n",
      "Iteration 3, loss = 0.01450100\n",
      "Iteration 4, loss = 0.01378403\n",
      "Iteration 5, loss = 0.01309674\n",
      "Iteration 6, loss = 0.01289521\n",
      "Iteration 7, loss = 0.01281231\n",
      "Iteration 8, loss = 0.01275731\n",
      "Iteration 9, loss = 0.01273718\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   28 | 02m30s |    0.50000 |   45.2879 |   89.1619 |  168.8022 |    0.3322 | \n",
      "Iteration 1, loss = 0.02590740\n",
      "Iteration 2, loss = 0.01404399\n",
      "Iteration 3, loss = 0.01332867\n",
      "Iteration 4, loss = 0.01304469\n",
      "Iteration 5, loss = 0.01290917\n",
      "Iteration 6, loss = 0.01283913\n",
      "Iteration 7, loss = 0.01276325\n",
      "Iteration 8, loss = 0.01274006\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   29 | 01m19s |    0.04770 |   37.9896 |   74.5698 |   49.3203 |    0.7365 | \n",
      "Iteration 1, loss = 0.02759410\n",
      "Iteration 2, loss = 0.01370332\n",
      "Iteration 3, loss = 0.01320334\n",
      "Iteration 4, loss = 0.01302004\n",
      "Iteration 5, loss = 0.01289083\n",
      "Iteration 6, loss = 0.01284198\n",
      "Iteration 7, loss = 0.01281091\n",
      "Iteration 8, loss = 0.01279348\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   30 | 03m17s |    0.95228 |  107.8476 |  142.4362 |  144.2439 |    0.8392 | \n",
      "Iteration 1, loss = 0.02450267\n",
      "Iteration 2, loss = 0.01453507\n",
      "Iteration 3, loss = 0.01339783\n",
      "Iteration 4, loss = 0.01302615\n",
      "Iteration 5, loss = 0.01289644\n",
      "Iteration 6, loss = 0.01280976\n",
      "Iteration 7, loss = 0.01277028\n",
      "Iteration 8, loss = 0.01273333\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   31 | 01m21s |    0.95231 |  186.0619 |   19.1753 |   63.7318 |    0.7094 | \n",
      "Iteration 1, loss = 0.01789781\n",
      "Iteration 2, loss = 0.01188371\n",
      "Iteration 3, loss = 0.01183605\n",
      "Iteration 4, loss = 0.01182674\n",
      "Iteration 5, loss = 0.01184278\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   32 | 01m45s |    0.95231 |   10.3331 |  149.4549 |  109.5836 |    0.1884 | \n",
      "Iteration 1, loss = 0.02777606\n",
      "Iteration 2, loss = 0.01382247\n",
      "Iteration 3, loss = 0.01325187\n",
      "Iteration 4, loss = 0.01303557\n",
      "Iteration 5, loss = 0.01292283\n",
      "Iteration 6, loss = 0.01284993\n",
      "Iteration 7, loss = 0.01280589\n",
      "Iteration 8, loss = 0.01279973\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   33 | 02m33s |    0.04766 |   37.0325 |  119.0916 |  181.5541 |    0.9857 | \n",
      "Iteration 1, loss = 0.02131389\n",
      "Iteration 2, loss = 0.01443504\n",
      "Iteration 3, loss = 0.01322518\n",
      "Iteration 4, loss = 0.01293602\n",
      "Iteration 5, loss = 0.01283116\n",
      "Iteration 6, loss = 0.01277121\n",
      "Iteration 7, loss = 0.01275182\n",
      "Iteration 8, loss = 0.01273682\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   34 | 01m26s |    0.50000 |   78.6784 |   28.8389 |  164.2947 |    0.3880 | \n",
      "Iteration 1, loss = 0.02463861\n",
      "Iteration 2, loss = 0.01484215\n",
      "Iteration 3, loss = 0.01349769\n",
      "Iteration 4, loss = 0.01306060\n",
      "Iteration 5, loss = 0.01289897\n",
      "Iteration 6, loss = 0.01282920\n",
      "Iteration 7, loss = 0.01277297\n",
      "Iteration 8, loss = 0.01275477\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   35 | 01m18s |    0.95207 |  192.0531 |   14.9033 |   68.8198 |    0.5032 | \n",
      "Iteration 1, loss = 0.03132957\n",
      "Iteration 2, loss = 0.01551071\n",
      "Iteration 3, loss = 0.01414172\n",
      "Iteration 4, loss = 0.01357250\n",
      "Iteration 5, loss = 0.01329366\n",
      "Iteration 6, loss = 0.01311462\n",
      "Iteration 7, loss = 0.01302144\n",
      "Iteration 8, loss = 0.01292423\n",
      "Iteration 9, loss = 0.01286090\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   36 | 01m12s |    0.04771 |   16.8780 |  118.7673 |   20.4376 |    0.7635 | \n",
      "Iteration 1, loss = 0.02599177\n",
      "Iteration 2, loss = 0.01415728\n",
      "Iteration 3, loss = 0.01324584\n",
      "Iteration 4, loss = 0.01299099\n",
      "Iteration 5, loss = 0.01289760\n",
      "Iteration 6, loss = 0.01281726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.01279173\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   37 | 01m04s |    0.04770 |  128.1215 |   14.6908 |  107.9998 |    0.6837 | \n",
      "Iteration 1, loss = 0.02865082\n",
      "Iteration 2, loss = 0.01415810\n",
      "Iteration 3, loss = 0.01339232\n",
      "Iteration 4, loss = 0.01309542\n",
      "Iteration 5, loss = 0.01295149\n",
      "Iteration 6, loss = 0.01286207\n",
      "Iteration 7, loss = 0.01279973\n",
      "Iteration 8, loss = 0.01276512\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   38 | 01m26s |    0.95231 |   74.0596 |   58.1342 |   82.8534 |    0.7220 | \n",
      "Iteration 1, loss = 0.01768165\n",
      "Iteration 2, loss = 0.01241947\n",
      "Iteration 3, loss = 0.01236736\n",
      "Iteration 4, loss = 0.01236734\n",
      "Iteration 5, loss = 0.01234383\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   39 | 02m12s |    0.95231 |  122.5912 |  106.0740 |  173.6062 |    0.2142 | \n",
      "Iteration 1, loss = 0.02726278\n",
      "Iteration 2, loss = 0.01498759\n",
      "Iteration 3, loss = 0.01383707\n",
      "Iteration 4, loss = 0.01331604\n",
      "Iteration 5, loss = 0.01306931\n",
      "Iteration 6, loss = 0.01293615\n",
      "Iteration 7, loss = 0.01284017\n",
      "Iteration 8, loss = 0.01277685\n",
      "Iteration 9, loss = 0.01273162\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   40 | 02m04s | \u001b[35m   0.95234\u001b[0m | \u001b[32m 199.7388\u001b[0m | \u001b[32m  76.6926\u001b[0m | \u001b[32m  14.4816\u001b[0m | \u001b[32m   0.7631\u001b[0m | \n",
      "Iteration 1, loss = 0.01284355\n",
      "Iteration 2, loss = 0.00940192\n",
      "Iteration 3, loss = 0.00938248\n",
      "Iteration 4, loss = 0.00931472\n",
      "Iteration 5, loss = 0.00934031\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   41 | 08m00s |    0.95231 |   90.7861 |  107.7396 |  167.9014 |    0.0752 | \n",
      "Iteration 1, loss = 0.02563312\n",
      "Iteration 2, loss = 0.01427067\n",
      "Iteration 3, loss = 0.01332256\n",
      "Iteration 4, loss = 0.01301995\n",
      "Iteration 5, loss = 0.01288606\n",
      "Iteration 6, loss = 0.01280785\n",
      "Iteration 7, loss = 0.01276404\n",
      "Iteration 8, loss = 0.01274886\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   42 | 01m50s |    0.04769 |  191.6914 |   56.6308 |   73.7662 |    0.6109 | \n",
      "Iteration 1, loss = 0.02249414\n",
      "Iteration 2, loss = 0.01364632\n",
      "Iteration 3, loss = 0.01311598\n",
      "Iteration 4, loss = 0.01292884\n",
      "Iteration 5, loss = 0.01282492\n",
      "Iteration 6, loss = 0.01277660\n",
      "Iteration 7, loss = 0.01276340\n",
      "Iteration 8, loss = 0.01274235\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   43 | 01m41s |    0.04764 |   39.8304 |   67.9790 |  138.8673 |    0.4256 | \n",
      "Iteration 1, loss = 0.01905384\n",
      "Iteration 2, loss = 0.01278765\n",
      "Iteration 3, loss = 0.01278083\n",
      "Iteration 4, loss = 0.01276535\n",
      "Iteration 5, loss = 0.01276343\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   44 | 01m32s |    0.95231 |   51.2419 |  144.8884 |   81.5054 |    0.2344 | \n",
      "Iteration 1, loss = 0.02395119\n",
      "Iteration 2, loss = 0.01451096\n",
      "Iteration 3, loss = 0.01371112\n",
      "Iteration 4, loss = 0.01333936\n",
      "Iteration 5, loss = 0.01316001\n",
      "Iteration 6, loss = 0.01304856\n",
      "Iteration 7, loss = 0.01296347\n",
      "Iteration 8, loss = 0.01288639\n",
      "Iteration 9, loss = 0.01282997\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   45 | 01m25s |    0.04769 |   18.7276 |  147.4276 |   22.9445 |    0.5490 | \n",
      "Iteration 1, loss = 0.02321981\n",
      "Iteration 2, loss = 0.01361848\n",
      "Iteration 3, loss = 0.01314593\n",
      "Iteration 4, loss = 0.01296660\n",
      "Iteration 5, loss = 0.01285354\n",
      "Iteration 6, loss = 0.01279232\n",
      "Iteration 7, loss = 0.01276225\n",
      "Iteration 8, loss = 0.01273528\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   46 | 02m20s |    0.04768 |   21.4194 |  137.1475 |  150.5467 |    0.5769 | \n",
      "Iteration 1, loss = 0.02882620\n",
      "Iteration 2, loss = 0.01525535\n",
      "Iteration 3, loss = 0.01396122\n",
      "Iteration 4, loss = 0.01336692\n",
      "Iteration 5, loss = 0.01307597\n",
      "Iteration 6, loss = 0.01293357\n",
      "Iteration 7, loss = 0.01283831\n",
      "Iteration 8, loss = 0.01278981\n",
      "Iteration 9, loss = 0.01274665\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   47 | 01m14s |    0.04772 |  107.5603 |   28.4653 |   22.5295 |    0.5480 | \n",
      "Iteration 1, loss = 0.01556757\n",
      "Iteration 2, loss = 0.01118075\n",
      "Iteration 3, loss = 0.01114570\n",
      "Iteration 4, loss = 0.01112587\n",
      "Iteration 5, loss = 0.01111579\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   48 | 06m01s |    0.95231 |   31.7818 |  190.5932 |  199.4805 |    0.1549 | \n",
      "Iteration 1, loss = 0.02544322\n",
      "Iteration 2, loss = 0.01372339\n",
      "Iteration 3, loss = 0.01323522\n",
      "Iteration 4, loss = 0.01296591\n",
      "Iteration 5, loss = 0.01287218\n",
      "Iteration 6, loss = 0.01281468\n",
      "Iteration 7, loss = 0.01278527\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   49 | 359m53s |    0.04771 |  175.2787 |  199.9297 |  164.2123 |    0.6326 | \n",
      "Iteration 1, loss = 0.01658405\n",
      "Iteration 2, loss = 0.01015239\n",
      "Iteration 3, loss = 0.01010031\n",
      "Iteration 4, loss = 0.01012092\n",
      "Iteration 5, loss = 0.01009270\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   50 | 01m49s |    0.95231 |  172.7145 |   30.5161 |   87.4388 |    0.1092 | \n",
      "Iteration 1, loss = 0.01601769\n",
      "Iteration 2, loss = 0.01032372\n",
      "Iteration 3, loss = 0.01026066\n",
      "Iteration 4, loss = 0.01025837\n",
      "Iteration 5, loss = 0.01023509\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   51 | 03m30s |    0.95231 |   94.4511 |  164.3267 |  130.9837 |    0.1147 | \n",
      "Iteration 1, loss = 0.02502757\n",
      "Iteration 2, loss = 0.01425080\n",
      "Iteration 3, loss = 0.01329158\n",
      "Iteration 4, loss = 0.01299786\n",
      "Iteration 5, loss = 0.01288401\n",
      "Iteration 6, loss = 0.01281454\n",
      "Iteration 7, loss = 0.01277951\n",
      "Iteration 8, loss = 0.01274732\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   52 | 02m33s |    0.04775 |  192.2350 |   77.9879 |  113.9788 |    0.4962 | \n",
      "Iteration 1, loss = 0.02191828\n",
      "Iteration 2, loss = 0.01483812\n",
      "Iteration 3, loss = 0.01381830\n",
      "Iteration 4, loss = 0.01321493\n",
      "Iteration 5, loss = 0.01297707\n",
      "Iteration 6, loss = 0.01286639\n",
      "Iteration 7, loss = 0.01279515\n",
      "Iteration 8, loss = 0.01274980\n",
      "Iteration 9, loss = 0.01272533\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   53 | 02m32s |    0.04776 |  189.8292 |   76.1347 |   56.5130 |    0.3772 | \n",
      "Iteration 1, loss = 0.02808118\n",
      "Iteration 2, loss = 0.01446495\n",
      "Iteration 3, loss = 0.01346979\n",
      "Iteration 4, loss = 0.01311835\n",
      "Iteration 5, loss = 0.01295135\n",
      "Iteration 6, loss = 0.01284074\n",
      "Iteration 7, loss = 0.01276595\n",
      "Iteration 8, loss = 0.01271867\n",
      "Iteration 9, loss = 0.01268760\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   54 | 00m51s |    0.95226 |   12.7135 |   51.2848 |   21.3878 |    0.9791 | \n",
      "Iteration 1, loss = 0.02491171\n",
      "Iteration 2, loss = 0.01418611\n",
      "Iteration 3, loss = 0.01326299\n",
      "Iteration 4, loss = 0.01298235\n",
      "Iteration 5, loss = 0.01286245\n",
      "Iteration 6, loss = 0.01279991\n",
      "Iteration 7, loss = 0.01277047\n",
      "Iteration 8, loss = 0.01274110\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   55 | 01m16s |    0.95232 |  166.8246 |   10.5351 |   81.6493 |    0.7133 | \n",
      "Iteration 1, loss = 0.01305953\n",
      "Iteration 2, loss = 0.00904949\n",
      "Iteration 3, loss = 0.00900301\n",
      "Iteration 4, loss = 0.00897440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.00895295\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   56 | 10m29s |    0.95231 |  125.7681 |  177.4500 |  123.0401 |    0.0614 | \n",
      "Iteration 1, loss = 0.02826262\n",
      "Iteration 2, loss = 0.01393317\n",
      "Iteration 3, loss = 0.01326702\n",
      "Iteration 4, loss = 0.01301034\n",
      "Iteration 5, loss = 0.01289063\n",
      "Iteration 6, loss = 0.01284299\n",
      "Iteration 7, loss = 0.01279609\n",
      "Iteration 8, loss = 0.01275392\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "   57 | 02m59s |    0.95231 |  157.9416 |  120.3326 |  111.7916 |    0.9003 | \n",
      "Iteration 1, loss = 0.03073818\n",
      "Iteration 2, loss = 0.01420939\n",
      "Loaded classifiers.\n",
      "Collected features.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7d0804ad4ff3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Finally we call .maximize method of the optimizer with the appropriate arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ucb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgp_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, init_points)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Evaluate target function at all initialization points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observe_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Add the points from `self.initialize` to the observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36m_observe_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_observe_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mobserve_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# measure the target function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-7d0804ad4ff3>\u001b[0m in \u001b[0;36mmaximizer\u001b[0;34m(alpha, N1, N2, N3)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# calculate out-of-the-box roc_score using validation set 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecondary_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val2_trans_pl2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mval2_roc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \"\"\"\n\u001b[1;32m   1049\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coefs_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    676\u001b[0m                                          layer_units[i + 1])))\n\u001b[1;32m    677\u001b[0m         \u001b[0;31m# forward propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[0;34m(self, activations)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[0;32m--> 105\u001b[0;31m                                                  self.coefs_[i])\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization\n",
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return validated 'roc_auc' score from Classifier\n",
    "# Note that the parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def maximizer(alpha,N1,N2,N3):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    estimator_function = MLPClassifier(alpha = alpha,\n",
    "                                       hidden_layer_sizes=(int(N1),int(N2),int(N3),),\n",
    "                                       verbose = True, warm_start = True)\n",
    "                                       \n",
    "    \n",
    "    # Fit the estimator\n",
    "    estimator_function.fit(X_train_ensemble, y_val1)\n",
    "        \n",
    "    # calculate out-of-the-box roc_score using validation set 2\n",
    "    probs = estimator_function.predict_proba(secondary_features(X_val2_trans_pl2))[:,1]\n",
    "    val2_roc = roc_auc_score(y_val2,probs)\n",
    "    \n",
    "    # return the validation score to be maximized \n",
    "    return val2_roc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "\n",
    "hyperparameter_space = {\n",
    "        'alpha': (0.00001,1),\n",
    "        'N1': (10,200),\n",
    "        'N2': (10,200),\n",
    "        'N3': (10,200)\n",
    "}\n",
    "\n",
    "BO = BayesianOptimization(f = maximizer, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "\n",
    "BO.maximize(init_points=100,n_iter=100,acq='ucb', kappa= 5, **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed test sparse matrix.\n",
      "Loaded classifiers.\n",
      "Collected features.\n",
      "Prepared MLPC ensemble probs.\n",
      "File saved as :/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/ENSB_MLPC_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare a submission using the tuned MLPC classifier\n",
    "\n",
    "import pandas as pd\n",
    "click_id = pd.read_hdf(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/click_id.h5\")\n",
    "def prepare_submission(predictions,filename = \"new_submission\", click_id = click_id):\n",
    "    \"\"\"predictions: a list containing the predicted probabilities in the test set. \"\"\"\n",
    "    is_attributed = pd.Series(predictions)\n",
    "    submission_frame = pd.DataFrame()\n",
    "    submission_frame[\"click_id\"] = click_id\n",
    "    submission_frame[\"is_attributed\"] = is_attributed.apply(lambda x: format(x,\".9f\"))  # Reformat the probabilities upto the 9th decimal point\n",
    "    filename = filename + \".csv\"\n",
    "    submission_frame.to_csv(filename,index = False)\n",
    "    print(\"File saved as :\" + filename)\n",
    "\n",
    "\n",
    "# let's perform a prediction using the test set\n",
    "# Load the sparse matrix \n",
    "import scipy.sparse as sp\n",
    "test_proc_p12 = sp.load_npz(\"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/test_proc_pl2.npz\").tocsr()\n",
    "print(\"Loaded processed test sparse matrix.\")\n",
    "\n",
    "probs = ensb.predict_proba(secondary_features(test_proc_p12))[:,1]\n",
    "print(\"Prepared MLPC ensemble probs.\")\n",
    "# Prepare the submission file\n",
    "prepare_submission(predictions= probs, \n",
    "                   filename= \"/Volumes/Iomega_HDD/2016/Data science/Kaggle/User-click-detection-predictive-modeling/ENSB_MLPC_submission\") \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
