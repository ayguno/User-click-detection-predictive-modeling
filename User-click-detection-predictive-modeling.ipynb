{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Through the initial analysis and exploration of the train_sample set (100K observations) we developed some initial expectations about the data. Here we are going to build a pipeline in order to effectively process data sets for modeling.\n",
    "\n",
    "# Planing for the Pipeline\n",
    "\n",
    "1. We will prepare 3 data sets from the training set. The training set provided is 200 million samples, we don't have computational power to use all of this data at the moment. Instead, we will extract 3 data sets each contain ~1 million observations. We will refer these sets as:\n",
    "\n",
    "    - training_set (1:100,000th rows of the original training set)\n",
    "    - validation_set1 (100,001:200,000th rows of the original training set)\n",
    "    - validation_set2 (200,001:300,000th rows of the original training set)\n",
    "\n",
    "2. Build the feature extraction and selection pipeline using the training set:\n",
    "\n",
    "    - Using the insights we obtained from data exploration, the following features will be used to create dummy variables: device, app, os and channel. We will perform this by converting these features to string, tokenization and selecting 300 best features.\n",
    "    - We will write custom processing functions to add the log_total_clicks and log_total_click_time features, and remove the unwanted base features\n",
    "    \n",
    "3. We will prepare the remainder of the pipeline to incorporate interaction terms and perform scaling and standardization.    \n",
    "\n",
    "## Prepare training and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training_set\n",
      "Finished validation_set1\n",
      "Finished validation_set2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "training_set = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/train.csv\",\n",
    "                           nrows=1000000,\n",
    "                           dtype = \"str\")\n",
    "print(\"Finished training_set\")\n",
    "validation_set1 = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/train.csv\",\n",
    "                           skiprows = 1000000,names = list(training_set.columns),\n",
    "                           nrows=1000000,\n",
    "                           dtype = \"str\")\n",
    "print(\"Finished validation_set1\")\n",
    "validation_set2 = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/train.csv\",\n",
    "                           skiprows = 2000000,names = list(training_set.columns),\n",
    "                           nrows=1000000,\n",
    "                           dtype = \"str\")\n",
    "print(\"Finished validation_set2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121848</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>105</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2698</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>259</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5729</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>237</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122891</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>280</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105433</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>245</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip app device  os channel           click_time attributed_time  \\\n",
       "0  121848  24      1  19     105  2017-11-06 16:21:51             NaN   \n",
       "1    2698  25      1  30     259  2017-11-06 16:21:51             NaN   \n",
       "2    5729   2      1  19     237  2017-11-06 16:21:51             NaN   \n",
       "3  122891   3      1  35     280  2017-11-06 16:21:51             NaN   \n",
       "4  105433  15      2  25     245  2017-11-06 16:21:51             NaN   \n",
       "\n",
       "  is_attributed  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 8 columns):\n",
      "ip                 1000000 non-null object\n",
      "app                1000000 non-null object\n",
      "device             1000000 non-null object\n",
      "os                 1000000 non-null object\n",
      "channel            1000000 non-null object\n",
      "click_time         1000000 non-null object\n",
      "attributed_time    1693 non-null object\n",
      "is_attributed      1000000 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 61.0+ MB\n"
     ]
    }
   ],
   "source": [
    "training_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote training_set to disk\n",
      "Wrote validation_set1 to disk\n",
      "Wrote validation_set2 to disk\n"
     ]
    }
   ],
   "source": [
    "# Let's save them for future easier individual loading\n",
    "training_set.to_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/training_set.csv\")\n",
    "print(\"Wrote training_set to disk\")\n",
    "\n",
    "validation_set1.to_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/validation_set1.csv\")\n",
    "print(\"Wrote validation_set1 to disk\")\n",
    "\n",
    "validation_set2.to_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/validation_set2.csv\")\n",
    "print(\"Wrote validation_set2 to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/training_set.csv\",\n",
    "                          index_col = 0, dtype = \"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83230</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:32:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17357</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:33:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35810</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:34:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45745</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>478</td>\n",
       "      <td>2017-11-06 14:34:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161007</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:35:08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip app device  os channel           click_time attributed_time  \\\n",
       "0   83230   3      1  13     379  2017-11-06 14:32:21             NaN   \n",
       "1   17357   3      1  19     379  2017-11-06 14:33:34             NaN   \n",
       "2   35810   3      1  13     379  2017-11-06 14:34:12             NaN   \n",
       "3   45745  14      1  13     478  2017-11-06 14:34:52             NaN   \n",
       "4  161007   3      1  13     379  2017-11-06 14:35:08             NaN   \n",
       "\n",
       "  is_attributed  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ip',\n",
       " 'app',\n",
       " 'device',\n",
       " 'os',\n",
       " 'channel',\n",
       " 'click_time',\n",
       " 'attributed_time',\n",
       " 'is_attributed']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(training_set.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate target labels from feature matrix \n",
    "\n",
    "We will seperate target labels from features for each of these data sets and pickle them for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "X_train = training_set.drop([\"is_attributed\",\"attributed_time\"], axis = 1)\n",
    "y_train = pd.to_numeric(training_set.is_attributed) \n",
    "\n",
    "X_train.to_pickle(\"X_train.pkl\")\n",
    "y_train.to_pickle(\"y_train.pkl\")\n",
    "\n",
    "X_val1 = validation_set1.drop([\"is_attributed\",\"attributed_time\"], axis = 1)\n",
    "y_val1 = pd.to_numeric(validation_set1.is_attributed) \n",
    "\n",
    "X_val1.to_pickle(\"X_val1.pkl\")\n",
    "y_val1.to_pickle(\"y_val1.pkl\")\n",
    "\n",
    "X_val2 = validation_set2.drop([\"is_attributed\",\"attributed_time\"], axis = 1)\n",
    "y_val2 = pd.to_numeric(validation_set2.is_attributed) \n",
    "\n",
    "X_val2.to_pickle(\"X_val2.pkl\")\n",
    "y_val2.to_pickle(\"y_val2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " '.Rhistory',\n",
       " 'app_dummy.rds',\n",
       " 'channel_dummy.rds',\n",
       " 'device_dummy.rds',\n",
       " 'os_dummy.rds',\n",
       " 'test_processed.csv',\n",
       " 'train_sample.csv',\n",
       " 'User-click-detection-predictive-modeling.ipynb',\n",
       " 'UserClickDetectionPredictiveModeling.Rmd',\n",
       " 'X_train.pkl',\n",
       " 'X_val1.pkl',\n",
       " 'X_val2.pkl',\n",
       " 'y_train.pkl',\n",
       " 'y_val1.pkl',\n",
       " 'y_val2.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the feature extraction and selection pipeline using the training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Read the pickled training set\n",
    "X_train = pd.read_pickle(\"X_train.pkl\")\n",
    "y_train = pd.read_pickle(\"y_train.pkl\")\n",
    "\n",
    "# Label text features\n",
    "Text_features = [\"app\",\"device\",\"os\",\"channel\"]\n",
    "\n",
    "##############################################################\n",
    "# Define utility function to parse and process text features\n",
    "##############################################################\n",
    "# Note we avoid lambda functions since they don't pickle when we want to save the pipeline later   \n",
    "def column_text_processer_nolambda(df,text_columns = Text_features):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \"\"\"\"A function that will merge/join all text in a given row to make it ready for tokenization. \n",
    "    - This function should take care of converting missing values to empty strings. \n",
    "    - It should also convert the text to lowercase.\n",
    "    df= pandas dataframe\n",
    "    text_columns = names of the text features in df\n",
    "    \"\"\" \n",
    "    # Select only non-text columns that are in the df\n",
    "    text_data = df[text_columns]\n",
    "    \n",
    "    # Fill the missing values in text_data using empty strings\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    \n",
    "    # Concatenate feature name to each category encoding for each row\n",
    "    # E.g: encoding 3 at device column will read as device3 to make each encoding unique for a given feature\n",
    "    for col_index in list(text_data.columns):\n",
    "        text_data[col_index] = col_index + text_data[col_index].astype(str)\n",
    "    \n",
    "    # Join all the strings in a given row to make a vector\n",
    "    # text_vector = text_data.apply(lambda x: \" \".join(x), axis = 1)\n",
    "    text_vector = []\n",
    "    for index,rows in text_data.iterrows():\n",
    "        text_item = \" \".join(rows).lower()\n",
    "        text_vector.append(text_item)\n",
    "\n",
    "    # return text_vector as pd.Series object to enter the tokenization pipeline\n",
    "    return pd.Series(text_vector)\n",
    "\n",
    "#######################################################################\n",
    "# Define custom processing functions to add the log_total_clicks and \n",
    "# log_total_click_time features, and remove the unwanted base features\n",
    "#######################################################################\n",
    "def column_time_processer(X_train):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert click_time to datetime64 dtype \n",
    "    X_train.click_time = pd.to_datetime(X_train.click_time)\n",
    "\n",
    "    # Calculate the log_total_clicks for each ip and add as a new feature to temp_data\n",
    "    temp_data = pd.DataFrame(np.log(X_train.groupby([\"ip\"]).size()),\n",
    "                                    columns = [\"log_total_clicks\"]).reset_index()\n",
    "\n",
    "\n",
    "    # Calculate the log_total_click_time for each ip and add as a new feature to temp_data\n",
    "    # First define a function to process selected ip group \n",
    "    def get_log_total_click_time(group):\n",
    "        diff = (max(group.click_time) - min(group.click_time)).seconds\n",
    "        return np.log(diff+1)\n",
    "\n",
    "    # Then apply this function to each ip group and extract the total click time per ip group\n",
    "    log_time_frame = pd.DataFrame(X_train.groupby([\"ip\"]).apply(get_log_total_click_time),\n",
    "                                  columns=[\"log_total_click_time\"]).reset_index()\n",
    "\n",
    "    # Then add this new feature to the temp_data\n",
    "    temp_data = pd.merge(temp_data,log_time_frame, how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Combine temp_data with X_train to maintain X_train key order\n",
    "    temp_data = pd.merge(X_train,temp_data,how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Drop features that are not needed\n",
    "    temp_data = temp_data[[\"log_total_clicks\",\"log_total_click_time\"]]\n",
    "\n",
    "    # Return only the numeric features as a tensor to integrate into the numeric feature branch of the pipeline\n",
    "    return temp_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
