{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Through the initial analysis and exploration of the train_sample set (100K observations) we developed some initial expectations about the data. Here we are going to build a pipeline in order to effectively process data sets for modeling.\n",
    "\n",
    "# Planing for the Pipeline\n",
    "\n",
    "1. We will prepare 3 data sets from the training set. The training set provided is 200 million samples, we don't have computational power to use all of this data at the moment. Instead, we will extract 3 data sets each contain ~1 million observations. We will refer these sets as:\n",
    "\n",
    "    - training_set (1:100,000th rows of the original training set)\n",
    "    - validation_set1 (100,001:200,000th rows of the original training set)\n",
    "    - validation_set2 (200,001:300,000th rows of the original training set)\n",
    "\n",
    "2. Build the feature extraction and selection pipeline using the training set:\n",
    "\n",
    "    - Using the insights we obtained from data exploration, the following features will be used to create dummy variables: device, app, os and channel. We will perform this by converting these features to string, tokenization and selecting 300 best features.\n",
    "    - We will write custom processing functions to add the log_total_clicks and log_total_click_time features, and remove the unwanted base features\n",
    "    \n",
    "3. We will prepare the remainder of the pipeline to incorporate interaction terms and perform scaling and standardization.    \n",
    "\n",
    "## Prepare training and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training_set\n",
      "Finished validation_set1\n",
      "Finished validation_set2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "training_set = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/train.csv\",\n",
    "                           nrows=1000000,\n",
    "                           dtype = \"str\")\n",
    "print(\"Finished training_set\")\n",
    "validation_set1 = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/train.csv\",\n",
    "                           skiprows = 1000000,names = list(training_set.columns),\n",
    "                           nrows=1000000,\n",
    "                           dtype = \"str\")\n",
    "print(\"Finished validation_set1\")\n",
    "validation_set2 = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/train.csv\",\n",
    "                           skiprows = 2000000,names = list(training_set.columns),\n",
    "                           nrows=1000000,\n",
    "                           dtype = \"str\")\n",
    "print(\"Finished validation_set2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121848</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>105</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2698</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>259</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5729</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>237</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122891</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>280</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105433</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>245</td>\n",
       "      <td>2017-11-06 16:21:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip app device  os channel           click_time attributed_time  \\\n",
       "0  121848  24      1  19     105  2017-11-06 16:21:51             NaN   \n",
       "1    2698  25      1  30     259  2017-11-06 16:21:51             NaN   \n",
       "2    5729   2      1  19     237  2017-11-06 16:21:51             NaN   \n",
       "3  122891   3      1  35     280  2017-11-06 16:21:51             NaN   \n",
       "4  105433  15      2  25     245  2017-11-06 16:21:51             NaN   \n",
       "\n",
       "  is_attributed  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 8 columns):\n",
      "ip                 1000000 non-null object\n",
      "app                1000000 non-null object\n",
      "device             1000000 non-null object\n",
      "os                 1000000 non-null object\n",
      "channel            1000000 non-null object\n",
      "click_time         1000000 non-null object\n",
      "attributed_time    1693 non-null object\n",
      "is_attributed      1000000 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 61.0+ MB\n"
     ]
    }
   ],
   "source": [
    "training_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote training_set to disk\n",
      "Wrote validation_set1 to disk\n",
      "Wrote validation_set2 to disk\n"
     ]
    }
   ],
   "source": [
    "# Let's save them for future easier individual loading\n",
    "training_set.to_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/training_set.csv\")\n",
    "print(\"Wrote training_set to disk\")\n",
    "\n",
    "validation_set1.to_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/validation_set1.csv\")\n",
    "print(\"Wrote validation_set1 to disk\")\n",
    "\n",
    "validation_set2.to_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/validation_set2.csv\")\n",
    "print(\"Wrote validation_set2 to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(\"/Volumes/500GB/Data_science/Kaggle/User-click-detection-predictive-modeling/training_set.csv\",\n",
    "                          index_col = 0, dtype = \"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83230</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:32:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17357</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:33:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35810</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:34:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45745</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>478</td>\n",
       "      <td>2017-11-06 14:34:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161007</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:35:08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip app device  os channel           click_time attributed_time  \\\n",
       "0   83230   3      1  13     379  2017-11-06 14:32:21             NaN   \n",
       "1   17357   3      1  19     379  2017-11-06 14:33:34             NaN   \n",
       "2   35810   3      1  13     379  2017-11-06 14:34:12             NaN   \n",
       "3   45745  14      1  13     478  2017-11-06 14:34:52             NaN   \n",
       "4  161007   3      1  13     379  2017-11-06 14:35:08             NaN   \n",
       "\n",
       "  is_attributed  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ip',\n",
       " 'app',\n",
       " 'device',\n",
       " 'os',\n",
       " 'channel',\n",
       " 'click_time',\n",
       " 'attributed_time',\n",
       " 'is_attributed']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(training_set.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate target labels from feature matrix \n",
    "\n",
    "We will seperate target labels from features for each of these data sets and pickle them for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "X_train = training_set.drop([\"is_attributed\",\"attributed_time\"], axis = 1)\n",
    "y_train = pd.to_numeric(training_set.is_attributed) \n",
    "\n",
    "X_train.to_pickle(\"X_train.pkl\")\n",
    "y_train.to_pickle(\"y_train.pkl\")\n",
    "\n",
    "X_val1 = validation_set1.drop([\"is_attributed\",\"attributed_time\"], axis = 1)\n",
    "y_val1 = pd.to_numeric(validation_set1.is_attributed) \n",
    "\n",
    "X_val1.to_pickle(\"X_val1.pkl\")\n",
    "y_val1.to_pickle(\"y_val1.pkl\")\n",
    "\n",
    "X_val2 = validation_set2.drop([\"is_attributed\",\"attributed_time\"], axis = 1)\n",
    "y_val2 = pd.to_numeric(validation_set2.is_attributed) \n",
    "\n",
    "X_val2.to_pickle(\"X_val2.pkl\")\n",
    "y_val2.to_pickle(\"y_val2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " '.Rhistory',\n",
       " 'app_dummy.rds',\n",
       " 'channel_dummy.rds',\n",
       " 'device_dummy.rds',\n",
       " 'os_dummy.rds',\n",
       " 'test_processed.csv',\n",
       " 'train_sample.csv',\n",
       " 'User-click-detection-predictive-modeling.ipynb',\n",
       " 'UserClickDetectionPredictiveModeling.Rmd',\n",
       " 'X_train.pkl',\n",
       " 'X_val1.pkl',\n",
       " 'X_val2.pkl',\n",
       " 'y_train.pkl',\n",
       " 'y_val1.pkl',\n",
       " 'y_val2.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the feature extraction and selection pipeline using the training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Read the pickled training set\n",
    "X_train = pd.read_pickle(\"X_train.pkl\")\n",
    "y_train = pd.read_pickle(\"y_train.pkl\")\n",
    "\n",
    "# Label text features\n",
    "Text_features = [\"app\",\"device\",\"os\",\"channel\"]\n",
    "\n",
    "##############################################################\n",
    "# Define utility function to parse and process text features\n",
    "##############################################################\n",
    "# Note we avoid lambda functions since they don't pickle when we want to save the pipeline later   \n",
    "def column_text_processer_nolambda(df,text_columns = Text_features):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \"\"\"\"A function that will merge/join all text in a given row to make it ready for tokenization. \n",
    "    - This function should take care of converting missing values to empty strings. \n",
    "    - It should also convert the text to lowercase.\n",
    "    df= pandas dataframe\n",
    "    text_columns = names of the text features in df\n",
    "    \"\"\" \n",
    "    # Select only non-text columns that are in the df\n",
    "    text_data = df[text_columns]\n",
    "    \n",
    "    # Fill the missing values in text_data using empty strings\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    \n",
    "    # Concatenate feature name to each category encoding for each row\n",
    "    # E.g: encoding 3 at device column will read as device3 to make each encoding unique for a given feature\n",
    "    for col_index in list(text_data.columns):\n",
    "        text_data[col_index] = col_index + text_data[col_index].astype(str)\n",
    "    \n",
    "    # Join all the strings in a given row to make a vector\n",
    "    # text_vector = text_data.apply(lambda x: \" \".join(x), axis = 1)\n",
    "    text_vector = []\n",
    "    for index,rows in text_data.iterrows():\n",
    "        text_item = \" \".join(rows).lower()\n",
    "        text_vector.append(text_item)\n",
    "\n",
    "    # return text_vector as pd.Series object to enter the tokenization pipeline\n",
    "    return pd.Series(text_vector)\n",
    "\n",
    "#######################################################################\n",
    "# Define custom processing functions to add the log_total_clicks and \n",
    "# log_total_click_time features, and remove the unwanted base features\n",
    "#######################################################################\n",
    "def column_time_processer(X_train):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert click_time to datetime64 dtype \n",
    "    X_train.click_time = pd.to_datetime(X_train.click_time)\n",
    "\n",
    "    # Calculate the log_total_clicks for each ip and add as a new feature to temp_data\n",
    "    temp_data = pd.DataFrame(np.log(X_train.groupby([\"ip\"]).size()),\n",
    "                                    columns = [\"log_total_clicks\"]).reset_index()\n",
    "\n",
    "\n",
    "    # Calculate the log_total_click_time for each ip and add as a new feature to temp_data\n",
    "    # First define a function to process selected ip group \n",
    "    def get_log_total_click_time(group):\n",
    "        diff = (max(group.click_time) - min(group.click_time)).seconds\n",
    "        return np.log(diff+1)\n",
    "\n",
    "    # Then apply this function to each ip group and extract the total click time per ip group\n",
    "    log_time_frame = pd.DataFrame(X_train.groupby([\"ip\"]).apply(get_log_total_click_time),\n",
    "                                  columns=[\"log_total_click_time\"]).reset_index()\n",
    "\n",
    "    # Then add this new feature to the temp_data\n",
    "    temp_data = pd.merge(temp_data,log_time_frame, how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Combine temp_data with X_train to maintain X_train key order\n",
    "    temp_data = pd.merge(X_train,temp_data,how = \"left\",on = \"ip\")\n",
    "\n",
    "    # Drop features that are not needed\n",
    "    temp_data = temp_data[[\"log_total_clicks\",\"log_total_click_time\"]]\n",
    "\n",
    "    # Return only the numeric features as a tensor to integrate into the numeric feature branch of the pipeline\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# We need to wrap these custom utility functions using FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# FunctionTransformer wrapper of utility functions to parse text and numeric features\n",
    "# Note how we avoid putting any arguments into column_text_processer or column_time_processer\n",
    "#############################################################################\n",
    "get_numeric_data = FunctionTransformer(func = column_time_processer, validate=False) \n",
    "get_text_data = FunctionTransformer(func = column_text_processer_nolambda,validate=False) \n",
    "\n",
    "#############################################################################\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "# #Note this regex will match either a whitespace or a punctuation to tokenize \n",
    "# the string vector on these preferences, in our case we only have white spaces in our text  \n",
    "#############################################################################\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "\n",
    "###############################################\n",
    "# Construct our feature extraction pipeline\n",
    "###############################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, chi2 # We will use chi-squared as a scoring function to select features for classification\n",
    "from sklearn.metrics import auc\n",
    "from SparseInteractions import * #Load SparseInteractions (from : https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) as a module since it was saved into working directory as SparseInteractions.py\n",
    "\n",
    "userclick_pipeline1 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion(\n",
    "        # Note that FeatureUnion() also accepts list of tuples, the first half of each tuple \n",
    "        # is the name of the transformer within the FeatureUnion\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\",Pipeline([        # Note we have subpipeline branches inside the main pipeline\n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()) # Step2: impute any missing data using default (mean), note we don't expect missing values in this case. \n",
    "            ])), # End of: numeric_subpipeline\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC, # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                             ngram_range = (1,1),\n",
    "                                             non_negative=True, \n",
    "                                             norm=None, binary=True )), # Note here we use binary=True since our hack is to use tokenization to generate dummy variables  \n",
    "                ('dim_red', SelectKBest(chi2,300)) # Step3: use dimension reduction to select 300 best features using chi2 as scoring function\n",
    "            ]))\n",
    "        ]\n",
    "        \n",
    "    )),# End of step: union, this is the fusion point to main pipeline, all features are numeric at this stage\n",
    "    \n",
    "    # Common steps:\n",
    "            \n",
    "    (\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms up to the second degree polynomial\n",
    "    (\"scaler\",MaxAbsScaler()) # Scale the features between 0 and 1.       \n",
    "            \n",
    "])# End of: userclick_pipeline1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop the userclick_pipeline1 by using the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 3.3333333333333335 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "userclick_pipeline1.fit(X_train,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained our pipeline using the training set, we will pickle it and store for reuse. This will ensure the consistency every time we want to process a data set, and we will extract the same set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle and store the userclick_pipeline1\n",
    "import pickle\n",
    "with open(\"userclick_pipeline1.pkl\",\"wb\") as f:\n",
    "    pickle.dump(userclick_pipeline1,f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the features in the training set using the established pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the userclick_pipeline1 to work with\n",
    "import pickle\n",
    "with open(\"userclick_pipeline1.pkl\",\"rb\") as f:\n",
    "    userclick_pipeline1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 3.0833333333333335 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# Transform the training set features\n",
    "X_train_trans_pl1 = userclick_pipeline1.transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 45753)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_trans_pl1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csc.csc_matrix"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_trans_pl1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pickle and save this transformed version of the features from the training set. We spent about 3 minutes by training the pipeline and an additional 3 minutes for transforming the features.\n",
    "\n",
    "In the future, we will only use this pipeline with .transform method to process any datasets we would like to use in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed version of training set features\n",
    "import pickle\n",
    "with open(\"X_train_trans_pl1.pkl\",\"wb\") as f:\n",
    "    pickle.dump(X_train_trans_pl1,f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Regularized Linear Model for predicting true events\n",
    "\n",
    "We will start linear and train a Ridge classifier by setting the regularization parameter alpha to 0.5. We will see the untuned model performance first, then try to optimize the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 1.3333333333333333 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Read the transformed features and target labels from the training set\n",
    "import pickle\n",
    "with open(\"X_train_trans_pl1.pkl\",\"rb\") as f:\n",
    "    X_train_trans_pl1 = pickle.load(f)\n",
    "with open(\"y_train.pkl\",\"rb\") as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# Instantiate a Ridge classifier with a medium alpha \n",
    "Ridge = RidgeClassifier(alpha=0.5, random_state= 321)\n",
    "\n",
    "# Train the model\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "Ridge.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 0.016666666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Predict class labels using training set\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# Predict class probabilities\n",
    "# Note that there is no predict_proba on RidgeClassifier\n",
    "# So we use the trick in https://stackoverflow.com/questions/22538080/scikit-learn-ridge-classifier-extracting-class-probabilities \n",
    "\n",
    "d = Ridge.decision_function(X= X_train_trans_pl1) # Predict confidence scores for samples\n",
    "probs = np.exp(d) / np.sum(np.exp(d)) # Use softmax to convert them probabilities between 0 and 1\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94980780103952844"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate ROC score between the predicted probability and the observed target\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train,probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even this naive attempt gave us an ROC score of 0.949, Next, we will try to perform hyperparameter optimization to see where we can get further: \n",
    "\n",
    "## Hyperparameter optimization using Ridge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n",
      "[CV] alpha=0.86 ......................................................\n",
      "[CV] alpha=0.86 ......................................................\n",
      "[CV] alpha=0.25 ......................................................\n",
      "[CV] ............. alpha=0.86, score=0.8914263110278222, total=  37.6s\n",
      "[CV] alpha=0.25 ......................................................\n",
      "[CV] ............. alpha=0.86, score=0.8956513669854133, total= 1.1min\n",
      "[CV] alpha=0.45 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:  1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. alpha=0.25, score=0.884190883324053, total= 1.4min\n",
      "[CV] alpha=0.45 ......................................................\n",
      "[CV] ............. alpha=0.25, score=0.8914015886647204, total= 1.3min\n",
      "[CV] alpha=0.32 ......................................................\n",
      "[CV] ............. alpha=0.45, score=0.8886544167323791, total=  53.1s\n",
      "[CV] alpha=0.32 ......................................................\n",
      "[CV] ............. alpha=0.45, score=0.8941574912061789, total=  55.7s\n",
      "[CV] alpha=0.2 .......................................................\n",
      "[CV] ............. alpha=0.32, score=0.8875157179170297, total= 1.2min\n",
      "[CV] alpha=0.2 .......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:  3.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.32, score=0.8924351290802932, total= 1.2min\n",
      "[CV] alpha=0.06 ......................................................\n",
      "[CV] ............... alpha=0.2, score=0.882164720700341, total= 1.8min\n",
      "[CV] alpha=0.06 ......................................................\n",
      "[CV] .............. alpha=0.2, score=0.8905415407269656, total= 1.8min\n",
      "[CV] alpha=0.15 ......................................................\n",
      "[CV] ............. alpha=0.06, score=0.8786576036594884, total= 2.8min\n",
      "[CV] alpha=0.15 ......................................................\n",
      "[CV] .............. alpha=0.15, score=0.883062963076619, total= 1.8min\n",
      "[CV] alpha=0.82 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:  6.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.06, score=0.8759978611806225, total= 2.7min\n",
      "[CV] alpha=0.82 ......................................................\n",
      "[CV] ............. alpha=0.82, score=0.8913547259152571, total=  33.9s\n",
      "[CV] alpha=0.08 ......................................................\n",
      "[CV] ............. alpha=0.15, score=0.8884136760769671, total= 1.7min\n",
      "[CV] alpha=0.08 ......................................................\n",
      "[CV] ............. alpha=0.82, score=0.8954961726441188, total=  55.6s\n",
      "[CV] alpha=0.38 ......................................................\n",
      "[CV] ............. alpha=0.38, score=0.8886718559207138, total=  57.1s\n",
      "[CV] alpha=0.38 ......................................................\n",
      "[CV] .............. alpha=0.08, score=0.880409659046294, total= 2.2min\n",
      "[CV] alpha=0.69 ......................................................\n",
      "[CV] ............. alpha=0.38, score=0.8924743918088495, total= 1.0min\n",
      "[CV] alpha=0.69 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:  9.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.08, score=0.8830280871683156, total= 2.3min\n",
      "[CV] alpha=0.48 ......................................................\n",
      "[CV] ............. alpha=0.69, score=0.8903844868681919, total=  38.9s\n",
      "[CV] alpha=0.48 ......................................................\n",
      "[CV] .............. alpha=0.69, score=0.894370026181325, total=  40.2s\n",
      "[CV] alpha=0.95 ......................................................\n",
      "[CV] ............. alpha=0.48, score=0.8886348535251752, total=  48.8s\n",
      "[CV] alpha=0.95 ......................................................\n",
      "[CV] ............. alpha=0.48, score=0.8942835960807918, total=  50.9s\n",
      "[CV] alpha=0.42 ......................................................\n",
      "[CV] ............. alpha=0.95, score=0.8912950972786214, total=  29.2s\n",
      "[CV] alpha=0.42 ......................................................\n",
      "[CV] .............. alpha=0.42, score=0.888656706320648, total=  54.5s\n",
      "[CV] alpha=0.67 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed: 12.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.42, score=0.8933858578180848, total=  58.5s\n",
      "[CV] alpha=0.67 ......................................................\n",
      "[CV] ............. alpha=0.95, score=0.8976710720537423, total= 1.3min\n",
      "[CV] alpha=0.74 ......................................................\n",
      "[CV] ............. alpha=0.67, score=0.8904195284489833, total=  38.5s\n",
      "[CV] alpha=0.74 ......................................................\n",
      "[CV] ............. alpha=0.67, score=0.8943406596495505, total=  42.0s\n",
      "[CV] alpha=0.7 .......................................................\n",
      "[CV] ............. alpha=0.74, score=0.8902902991673008, total=  36.1s\n",
      "[CV] alpha=0.7 .......................................................\n",
      "[CV] ............. alpha=0.74, score=0.8951258526506547, total=  43.6s\n",
      "[CV] alpha=0.44 ......................................................\n",
      "[CV] .............. alpha=0.7, score=0.8904273622468626, total=  38.9s\n",
      "[CV] alpha=0.44 ......................................................\n",
      "[CV] .............. alpha=0.7, score=0.8944044414547937, total=  41.6s\n",
      "[CV] alpha=0.51 ......................................................\n",
      "[CV] ............. alpha=0.44, score=0.8886681708189335, total=  53.0s\n",
      "[CV] alpha=0.51 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed: 14.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. alpha=0.51, score=0.8885397528273091, total=  47.1s\n",
      "[CV] alpha=0.53 ......................................................\n",
      "[CV] ............. alpha=0.44, score=0.8940703814673404, total=  55.5s\n",
      "[CV] alpha=0.53 ......................................................\n",
      "[CV] ............. alpha=0.51, score=0.8943254518544484, total=  51.8s\n",
      "[CV] ............. alpha=0.53, score=0.8885548291037833, total=  47.6s\n",
      "[CV] ............. alpha=0.53, score=0.8943041770442346, total=  49.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  40 out of  40 | elapsed: 15.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done  40 out of  40 | elapsed: 15.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 16.15 minutes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "params_space = {\"alpha\": np.arange(0,1,0.01)}\n",
    "RidgeSearch = RandomizedSearchCV(Ridge,cv = 2,verbose=10,\n",
    "                                 n_iter=20,\n",
    "                                 n_jobs=3,\n",
    "                                 param_distributions=params_space,\n",
    "                                 random_state= 321,\n",
    "                                 scoring= \"roc_auc\")\n",
    "\n",
    "RidgeSearch.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89448307829020712"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RidgeSearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.95000000000000007}"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RidgeSearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC score is:0.949325548082\n",
      "It took: 1.05 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Use 'alpha': 0.95 to re-fit the Ridge classifier and calculate the performance\n",
    "# Read the transformed features and target labels from the training set\n",
    "import pickle\n",
    "with open(\"X_train_trans_pl1.pkl\",\"rb\") as f:\n",
    "    X_train_trans_pl1 = pickle.load(f)\n",
    "with open(\"y_train.pkl\",\"rb\") as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# Instantiate a Ridge classifier with a medium alpha \n",
    "Ridge = RidgeClassifier(alpha=0.95, random_state= 321)\n",
    "\n",
    "# Train the model\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "Ridge.fit(X_train_trans_pl1,y_train)\n",
    "d = Ridge.decision_function(X= X_train_trans_pl1) # Predict confidence scores for samples\n",
    "probs = np.exp(d) / np.sum(np.exp(d)) # Use softmax to convert them probabilities between 0 and 1\n",
    "\n",
    "# Calculate ROC score between the predicted probability and the observed target\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print( \"The ROC score is:\" + str(roc_auc_score(y_train,probs)))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could not improve the performance of the Ridge model using this approach. Let's save this model and continue to explore other types of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"Ridge_classifier.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Ridge, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the transformed features and target labels from the training set\n",
    "import pickle\n",
    "import numpy as np\n",
    "with open(\"X_train_trans_pl1.pkl\",\"rb\") as f:\n",
    "    X_train_trans_pl1 = pickle.load(f)\n",
    "with open(\"y_train.pkl\",\"rb\") as f:\n",
    "    y_train = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 0.016666666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probability predictions\n",
    "probs = mnb.predict_proba(X_train_trans_pl1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   6.32335454e-15],\n",
       "       [  1.00000000e+00,   7.52020576e-15],\n",
       "       [  1.00000000e+00,   1.22347330e-14],\n",
       "       ..., \n",
       "       [  1.00000000e+00,   1.74420279e-12],\n",
       "       [  1.00000000e+00,   5.90993405e-16],\n",
       "       [  1.00000000e+00,   5.74730378e-13]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs # We need the second column which is the probabilities for class label: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = probs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB roc score is: 0.946744947288\n"
     ]
    }
   ],
   "source": [
    "# Calculate the roc score for the training set\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"NB roc score is: \" + str(roc_auc_score(y_train,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our untuned classifier, which has similar performance to Ridge. Can we try to tune it to perform better? Looking into description, we find the following parameters that can be tuned:\n",
    "\n",
    "Parameters\n",
    "\n",
    " |  alpha : float, optional (default=1.0)\n",
    " |      Additive (Laplace/Lidstone) smoothing parameter\n",
    " |      (0 for no smoothing).\n",
    "\n",
    "\n",
    " |  fit_prior : boolean, optional (default=True)\n",
    " |      Whether to learn class prior probabilities or not.\n",
    " |      If false, a uniform prior will be used.\n",
    "\n",
    "\n",
    " |  class_prior : array-like, size (n_classes,), optional (default=None)\n",
    " |      Prior probabilities of the classes. If specified the priors are not\n",
    " |      adjusted according to the data.\n",
    " \n",
    "\n",
    "- We can make search across the alpha (0-1). \n",
    "- We will leave fit_prior = True\n",
    "- We have some idea about the probability of being in class 1, which is (sum(y_train)/len(y_train)) (0.00169) in our training set. Why don't we use this information in our hyperparameter search and see if it makes any difference.\n",
    "\n",
    "Since the classifier trained very fast, we can perform exhaustive GridSearch with 3-fold CV.\n",
    "\n",
    "## Hyperparameter optimization for Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 22 candidates, totalling 66 fits\n",
      "[CV] alpha=0.0, class_prior=None, fit_prior=True .....................\n",
      "[CV] alpha=0.0, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] alpha=0.0, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=None, fit_prior=True, score=0.8959370258094743, total=   1.6s\n",
      "[CV] alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=None, fit_prior=True, score=0.8868648185207342, total=   1.9s\n",
      "[CV] alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    5.2s\n",
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=None, fit_prior=True, score=0.8857097885869714, total=   2.0s\n",
      "[CV] alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.8959370204907351, total=   1.7s\n",
      "[CV] alpha=0.1, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.886864834505243, total=   1.7s\n",
      "[CV] alpha=0.1, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.8857097859228865, total=   1.6s\n",
      "[CV] alpha=0.1, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.1, class_prior=None, fit_prior=True, score=0.9435606570419294, total=   1.5s\n",
      "[CV] alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    9.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.1, class_prior=None, fit_prior=True, score=0.9327180517680261, total=   1.7s\n",
      "[CV] alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.1, class_prior=None, fit_prior=True, score=0.9505139967923142, total=   1.5s\n",
      "[CV] alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9435606517231905, total=   1.5s\n",
      "[CV] alpha=0.2, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9327180624243653, total=   1.6s\n",
      "[CV] alpha=0.2, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.1, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9505139914641447, total=   1.7s\n",
      "[CV] alpha=0.2, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   13.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.2, class_prior=None, fit_prior=True, score=0.9444671084783707, total=   1.6s\n",
      "[CV] alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.2, class_prior=None, fit_prior=True, score=0.9326124714247604, total=   1.6s\n",
      "[CV] alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.2, class_prior=None, fit_prior=True, score=0.9520073601628835, total=   1.5s\n",
      "[CV] alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9444671270939577, total=   1.4s\n",
      "[CV] alpha=0.3, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9326125300346252, total=   1.7s\n",
      "[CV] alpha=0.3, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.2, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9520073814755616, total=   1.7s\n",
      "[CV] alpha=0.3, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.3, class_prior=None, fit_prior=True, score=0.9446672473134183, total=   1.6s\n",
      "[CV] alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   18.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.3, class_prior=None, fit_prior=True, score=0.931901677625326, total=   1.5s\n",
      "[CV] alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.3, class_prior=None, fit_prior=True, score=0.9524465345361612, total=   1.6s\n",
      "[CV] alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9446671994447662, total=   1.8s\n",
      "[CV] alpha=0.4, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9319016589767328, total=   1.7s\n",
      "[CV] alpha=0.4, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.3, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9524465345361612, total=   1.6s\n",
      "[CV] alpha=0.4, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.4, class_prior=None, fit_prior=True, score=0.9445732731709474, total=   1.4s\n",
      "[CV] alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.4, class_prior=None, fit_prior=True, score=0.9310744313527055, total=   1.4s\n",
      "[CV] alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   24.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.4, class_prior=None, fit_prior=True, score=0.9524166168642662, total=   1.4s\n",
      "[CV] alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9445732731709476, total=   1.4s\n",
      "[CV] alpha=0.5, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9310744313527057, total=   1.4s\n",
      "[CV] alpha=0.5, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.4, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9524165928875032, total=   1.5s\n",
      "[CV] alpha=0.5, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.5, class_prior=None, fit_prior=True, score=0.9442627013551642, total=   1.5s\n",
      "[CV] alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.5, class_prior=None, fit_prior=True, score=0.9302755229497157, total=   1.5s\n",
      "[CV] alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.5, class_prior=None, fit_prior=True, score=0.9521864532611158, total=   1.4s\n",
      "[CV] alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9442626960364251, total=   1.5s\n",
      "[CV] alpha=0.6, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9302755122933766, total=   1.6s\n",
      "[CV] alpha=0.6, class_prior=None, fit_prior=True .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:   31.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.5, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9521864452688615, total=   1.5s\n",
      "[CV] alpha=0.6, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.6, class_prior=None, fit_prior=True, score=0.9438514218731604, total=   1.6s\n",
      "[CV] alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.6, class_prior=None, fit_prior=True, score=0.9294995310038618, total=   1.7s\n",
      "[CV] alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.6, class_prior=None, fit_prior=True, score=0.9518519294655212, total=   1.4s\n",
      "[CV] alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9438514218731604, total=   1.5s\n",
      "[CV] alpha=0.7, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9294995310038618, total=   1.6s\n",
      "[CV] alpha=0.7, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.6, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9518519294655213, total=   1.5s\n",
      "[CV] alpha=0.7, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.7, class_prior=None, fit_prior=True, score=0.9433635658444083, total=   1.6s\n",
      "[CV] alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.7, class_prior=None, fit_prior=True, score=0.9287724996077935, total=   1.5s\n",
      "[CV] alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   38.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.7, class_prior=None, fit_prior=True, score=0.9514674114552534, total=   1.6s\n",
      "[CV] alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9433635685037778, total=   1.6s\n",
      "[CV] alpha=0.8, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9287725022718782, total=   1.6s\n",
      "[CV] alpha=0.8, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.7, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9514674061270838, total=   1.8s\n",
      "[CV] alpha=0.8, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.8, class_prior=None, fit_prior=True, score=0.942827681601234, total=   2.0s\n",
      "[CV] alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.8, class_prior=None, fit_prior=True, score=0.9280890526384574, total=   2.1s\n",
      "[CV] alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.8, class_prior=None, fit_prior=True, score=0.9510515265109788, total=   1.8s\n",
      "[CV] alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9428276789418646, total=   1.6s\n",
      "[CV] alpha=0.9, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9280890286616944, total=   1.6s\n",
      "[CV] alpha=0.9, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.8, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9510515265109788, total=   1.6s\n",
      "[CV] alpha=0.9, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.9, class_prior=None, fit_prior=True, score=0.9422726206440957, total=   1.7s\n",
      "[CV] alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:   48.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.9, class_prior=None, fit_prior=True, score=0.9274512973869017, total=   1.7s\n",
      "[CV] alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.9, class_prior=None, fit_prior=True, score=0.9506482853129924, total=   1.5s\n",
      "[CV] alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9422726206440957, total=   1.9s\n",
      "[CV] alpha=1.0, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9274512973869015, total=   2.1s\n",
      "[CV] alpha=1.0, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=0.9, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9506482853129923, total=   2.2s\n",
      "[CV] alpha=1.0, class_prior=None, fit_prior=True .....................\n",
      "[CV]  alpha=1.0, class_prior=None, fit_prior=True, score=0.9417146237429526, total=   2.3s\n",
      "[CV] alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=1.0, class_prior=None, fit_prior=True, score=0.926872215938179, total=   2.0s\n",
      "[CV] alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=1.0, class_prior=None, fit_prior=True, score=0.9502528152502612, total=   1.7s\n",
      "[CV] alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True .......\n",
      "[CV]  alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9417146264023222, total=   1.6s\n",
      "[CV]  alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9268722079459248, total=   1.7s\n",
      "[CV]  alpha=1.0, class_prior=[0.99831, 0.00169], fit_prior=True, score=0.9502528205784307, total=   1.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  66 out of  66 | elapsed:   58.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 1.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "params_space = {\n",
    "    \"alpha\":np.arange(0,1.1,0.1),\n",
    "    \"fit_prior\":[True],\n",
    "    \"class_prior\":[None,[1-0.00169,0.00169]]\n",
    "}\n",
    "\n",
    "MNBsearch = GridSearchCV(mnb,\n",
    "                         param_grid= params_space,\n",
    "                         scoring=\"roc_auc\",\n",
    "                         cv =3, n_jobs=3,verbose=10)\n",
    "\n",
    "MNBsearch.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.20000000000000001,\n",
       " 'class_prior': [0.99831, 0.00169],\n",
       " 'fit_prior': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNBsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94302901430616237"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNBsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95332437154555072"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "probs = MNBsearch.best_estimator_.predict_proba(X_train_trans_pl1)[:,1]\n",
    "roc_auc_score(y_train,probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the hyperparameter search improved MNB model performance. We will store the best estimator we obtained for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"mnb.pkl\",\"wb\") as f:\n",
    "    pickle.dump(MNBsearch.best_estimator_,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Support Vector Machine Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the transformed features and target labels from the training set\n",
    "import pickle\n",
    "import numpy as np\n",
    "with open(\"X_train_trans_pl1.pkl\",\"rb\") as f:\n",
    "    X_train_trans_pl1 = pickle.load(f)\n",
    "with open(\"y_train.pkl\",\"rb\") as f:\n",
    "    y_train = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear]It took: 2.25 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "# Note that we can't get probabilities directly from this LinearSVC function\n",
    "# We need to wrap into Calibrated Classifier \n",
    "# (see: https://stackoverflow.com/questions/35212213/sklearn-how-to-get-decision-probabilities-for-linearsvc-classifier)\n",
    "\n",
    "lsvc = LinearSVC(verbose=10)\n",
    "\n",
    "cal_lsvc = CalibratedClassifierCV(base_estimator = lsvc,\n",
    "                                  cv = 3, # Also performs cross-validation\n",
    "                                  method= \"sigmoid\") # We use sigmoid function to get probabilities\n",
    "\n",
    "cal_lsvc.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = cal_lsvc.predict_proba(X_train_trans_pl1)[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95099179905081954"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate ROC score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train,probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good start for an untuned classifier, let's try to perform hyperparameter search to see if we can improve this performance.\n",
    "\n",
    "## Hyperparameter tuning for SVC classifier\n",
    "\n",
    "We need to understand what SVC parameters we can tune in the context of calibrated classifier wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "      verbose=10),\n",
       " 'base_estimator__C': 1.0,\n",
       " 'base_estimator__class_weight': None,\n",
       " 'base_estimator__dual': True,\n",
       " 'base_estimator__fit_intercept': True,\n",
       " 'base_estimator__intercept_scaling': 1,\n",
       " 'base_estimator__loss': 'squared_hinge',\n",
       " 'base_estimator__max_iter': 1000,\n",
       " 'base_estimator__multi_class': 'ovr',\n",
       " 'base_estimator__penalty': 'l2',\n",
       " 'base_estimator__random_state': None,\n",
       " 'base_estimator__tol': 0.0001,\n",
       " 'base_estimator__verbose': 10,\n",
       " 'cv': 3,\n",
       " 'method': 'sigmoid'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_lsvc.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Parameters\n",
    " \n",
    " |  penalty : string, 'l1' or 'l2' (default='l2')\n",
    " |      Specifies the norm used in the penalization. The 'l2'\n",
    " |      penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
    " |      vectors that are sparse.\n",
    " |  \n",
    " \n",
    " \n",
    " |  loss : string, 'hinge' or 'squared_hinge' (default='squared_hinge')\n",
    " |      Specifies the loss function. 'hinge' is the standard SVM loss\n",
    " |      (used e.g. by the SVC class) while 'squared_hinge' is the\n",
    " |      square of the hinge loss.\n",
    " \n",
    " \n",
    " |  dual : bool, (default=True)\n",
    " |      Select the algorithm to either solve the dual or primal\n",
    " |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
    " \n",
    " \n",
    " |  tol : float, optional (default=1e-4)\n",
    " |      Tolerance for stopping criteria.\n",
    " \n",
    " \n",
    " |  C : float, optional (default=1.0)\n",
    " |      Penalty parameter C of the error term.\n",
    " \n",
    " \n",
    " |  multi_class : string, 'ovr' or 'crammer_singer' (default='ovr')\n",
    " |      Determines the multi-class strategy if `y` contains more than\n",
    " |      two classes.\n",
    " |      ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
    " |      ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
    " |      While `crammer_singer` is interesting from a theoretical perspective\n",
    " |      as it is consistent, it is seldom used in practice as it rarely leads\n",
    " |      to better accuracy and is more expensive to compute.\n",
    " |      If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
    " |      will be ignored.\n",
    " \n",
    " \n",
    " |  fit_intercept : boolean, optional (default=True)\n",
    " |      Whether to calculate the intercept for this model. If set\n",
    " |      to false, no intercept will be used in calculations\n",
    " |      (i.e. data is expected to be already centered).\n",
    " \n",
    " \n",
    " |  intercept_scaling : float, optional (default=1)\n",
    " |      When self.fit_intercept is True, instance vector x becomes\n",
    " |      ``[x, self.intercept_scaling]``,\n",
    " |      i.e. a \"synthetic\" feature with constant value equals to\n",
    " |      intercept_scaling is appended to the instance vector.\n",
    " |      The intercept becomes intercept_scaling * synthetic feature weight\n",
    " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    " |      as all other features.\n",
    " |      To lessen the effect of regularization on synthetic feature weight\n",
    " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
    " \n",
    " \n",
    " |  class_weight : {dict, 'balanced'}, optional\n",
    " |      Set the parameter C of class i to ``class_weight[i]*C`` for\n",
    " |      SVC. If not given, all classes are supposed to have\n",
    " |      weight one.\n",
    " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
    " |      weights inversely proportional to class frequencies in the input data\n",
    " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
    " \n",
    " \n",
    " |  verbose : int, (default=0)\n",
    " |      Enable verbose output. Note that this setting takes advantage of a\n",
    " |      per-process runtime setting in liblinear that, if enabled, may not work\n",
    " |      properly in a multithreaded context.\n",
    " \n",
    " \n",
    " |  random_state : int, RandomState instance or None, optional (default=None)\n",
    " |      The seed of the pseudo random number generator to use when shuffling\n",
    " |      the data.  If int, random_state is the seed used by the random number\n",
    " |      generator; If RandomState instance, random_state is the random number\n",
    " |      generator; If None, the random number generator is the RandomState\n",
    " |      instance used by `np.random`.\n",
    " \n",
    " \n",
    " |  max_iter : int, (default=1000)\n",
    " |      The maximum number of iterations to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] base_estimator__penalty=l1, base_estimator__dual=True, base_estimator__C=1.65361574567e+15 \n",
      "[CV] base_estimator__penalty=l1, base_estimator__dual=True, base_estimator__C=1.65361574567e+15 \n",
      "[CV] base_estimator__penalty=l1, base_estimator__dual=True, base_estimator__C=1.65361574567e+15 \n"
     ]
    },
    {
     "ename": "JoblibValueError",
     "evalue": "JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x10659a6f0, file \"/Use...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/OZANA.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x10659a6f0, file \"/Use...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/OZANA.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 3, 30, 15, 20, 51, 429826, tzinfo=tzutc()), 'msg_id': '51FA63EB565D4EA58DDEABB29D33E8F3', 'msg_type': 'execute_request', 'session': '2298C9A937DC4557997947C4492FD686', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '51FA63EB565D4EA58DDEABB29D33E8F3', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'2298C9A937DC4557997947C4492FD686']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 3, 30, 15, 20, 51, 429826, tzinfo=tzutc()), 'msg_id': '51FA63EB565D4EA58DDEABB29D33E8F3', 'msg_type': 'execute_request', 'session': '2298C9A937DC4557997947C4492FD686', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '51FA63EB565D4EA58DDEABB29D33E8F3', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'2298C9A937DC4557997947C4492FD686'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 3, 30, 15, 20, 51, 429826, tzinfo=tzutc()), 'msg_id': '51FA63EB565D4EA58DDEABB29D33E8F3', 'msg_type': 'execute_request', 'session': '2298C9A937DC4557997947C4492FD686', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '51FA63EB565D4EA58DDEABB29D33E8F3', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")',), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")',)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Import object>, <_ast.Assign object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>], cell_name='<ipython-input-25-de1c77550da9>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 1a290950f0, execution..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x10bac21e0, file \"<ipython-input-25-de1c77550da9>\", line 25>\n        result = <ExecutionResult object at 1a290950f0, execution..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x10bac21e0, file \"<ipython-input-25-de1c77550da9>\", line 25>, result=<ExecutionResult object at 1a290950f0, execution..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x10bac21e0, file \"<ipython-input-25-de1c77550da9>\", line 25>\n        self.user_global_ns = {'CAL_LSVC_search': RandomizedSearchCV(cv=3, error_score='raise',\n  ...urn_train_score='warn', scoring=None, verbose=10), 'CalibratedClassifierCV': <class 'sklearn.calibration.CalibratedClassifierCV'>, 'In': ['', 'from sklearn.svm import LinearSVC\\nfrom sklearn.calibration import CalibratedClassifierCV', 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', '# Read the transformed features and target label...in.pkl\",\"rb\") as f:\\n    y_train = pickle.load(f) ', 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', 'probs = cal_lsvc.predict_proba(X_train_trans_pl1) ', 'probs', 'probs = cal_lsvc.predict_proba(X_train_trans_pl1)[:,1] ', 'probs', '# Calculate ROC score\\nfrom sklearn.metrics import roc_auc_score', '# Calculate ROC score\\nfrom sklearn.metrics import roc_auc_score\\nroc_auc_score(y_train,probs)', 'cal_lsvc.get_params', 'cal_lsvc.get_params()', 'help(LinearSVC)', 'np.logspace(0.1,1000)', 'np.logspace(1,1000)', 'np.logspace(1000)', 'np.logspace(0.1,1000,2)', 'np.logspace(0.1,1000,100)', 'np.logspace(0.1,1000,10)', ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'Out': {6: array([[  9.98604863e-01,   1.39513749e-03],\n   ...3],\n       [  9.98843163e-01,   1.15683681e-03]]), 8: array([  1.39513749e-03,   1.37266016e-03,   1.3...1099911e-05,   1.23671390e-03,   1.15683681e-03]), 10: 0.95099179905081954, 11: <bound method BaseEstimator.get_params of Calibr...verbose=10),\n            cv=3, method='sigmoid')>, 12: {'base_estimator': LinearSVC(C=1.0, class_weight=None, dual=True, f..., random_state=None, tol=0.0001,\n     verbose=10), 'base_estimator__C': 1.0, 'base_estimator__class_weight': None, 'base_estimator__dual': True, 'base_estimator__fit_intercept': True, 'base_estimator__intercept_scaling': 1, 'base_estimator__loss': 'squared_hinge', 'base_estimator__max_iter': 1000, 'base_estimator__multi_class': 'ovr', 'base_estimator__penalty': 'l2', ...}, 14: array([  1.25892541e+000,   3.20717346e+020,   8...nf,\n                     inf,               inf]), 15: array([  1.00000000e+001,   2.44205309e+021,   5...nf,\n                     inf,               inf]), 17: array([ 1.25892541,         inf]), 18: array([  1.25892541e+000,   1.58489319e+010,   1...nf,               inf,\n                     inf]), 19: array([  1.25892541e+000,   1.58489319e+111,   1...nf,               inf,\n                     inf]), ...}, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'X_train_trans_pl1': <1000000x45753 sparse matrix of type '<class 'nu...ored elements in Compressed Sparse Column format>, '_': array([  1.07177346e+00,   2.15709679e+00,   4.3...e+29,   6.29843910e+29,\n         1.26765060e+30]), '_10': 0.95099179905081954, '_11': <bound method BaseEstimator.get_params of Calibr...verbose=10),\n            cv=3, method='sigmoid')>, ...}\n        self.user_ns = {'CAL_LSVC_search': RandomizedSearchCV(cv=3, error_score='raise',\n  ...urn_train_score='warn', scoring=None, verbose=10), 'CalibratedClassifierCV': <class 'sklearn.calibration.CalibratedClassifierCV'>, 'In': ['', 'from sklearn.svm import LinearSVC\\nfrom sklearn.calibration import CalibratedClassifierCV', 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', '# Read the transformed features and target label...in.pkl\",\"rb\") as f:\\n    y_train = pickle.load(f) ', 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', 'probs = cal_lsvc.predict_proba(X_train_trans_pl1) ', 'probs', 'probs = cal_lsvc.predict_proba(X_train_trans_pl1)[:,1] ', 'probs', '# Calculate ROC score\\nfrom sklearn.metrics import roc_auc_score', '# Calculate ROC score\\nfrom sklearn.metrics import roc_auc_score\\nroc_auc_score(y_train,probs)', 'cal_lsvc.get_params', 'cal_lsvc.get_params()', 'help(LinearSVC)', 'np.logspace(0.1,1000)', 'np.logspace(1,1000)', 'np.logspace(1000)', 'np.logspace(0.1,1000,2)', 'np.logspace(0.1,1000,100)', 'np.logspace(0.1,1000,10)', ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'Out': {6: array([[  9.98604863e-01,   1.39513749e-03],\n   ...3],\n       [  9.98843163e-01,   1.15683681e-03]]), 8: array([  1.39513749e-03,   1.37266016e-03,   1.3...1099911e-05,   1.23671390e-03,   1.15683681e-03]), 10: 0.95099179905081954, 11: <bound method BaseEstimator.get_params of Calibr...verbose=10),\n            cv=3, method='sigmoid')>, 12: {'base_estimator': LinearSVC(C=1.0, class_weight=None, dual=True, f..., random_state=None, tol=0.0001,\n     verbose=10), 'base_estimator__C': 1.0, 'base_estimator__class_weight': None, 'base_estimator__dual': True, 'base_estimator__fit_intercept': True, 'base_estimator__intercept_scaling': 1, 'base_estimator__loss': 'squared_hinge', 'base_estimator__max_iter': 1000, 'base_estimator__multi_class': 'ovr', 'base_estimator__penalty': 'l2', ...}, 14: array([  1.25892541e+000,   3.20717346e+020,   8...nf,\n                     inf,               inf]), 15: array([  1.00000000e+001,   2.44205309e+021,   5...nf,\n                     inf,               inf]), 17: array([ 1.25892541,         inf]), 18: array([  1.25892541e+000,   1.58489319e+010,   1...nf,               inf,\n                     inf]), 19: array([  1.25892541e+000,   1.58489319e+111,   1...nf,               inf,\n                     inf]), ...}, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'X_train_trans_pl1': <1000000x45753 sparse matrix of type '<class 'nu...ored elements in Compressed Sparse Column format>, '_': array([  1.07177346e+00,   2.15709679e+00,   4.3...e+29,   6.29843910e+29,\n         1.26765060e+30]), '_10': 0.95099179905081954, '_11': <bound method BaseEstimator.get_params of Calibr...verbose=10),\n            cv=3, method='sigmoid')>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/Users/OZANAYGUN/Desktop/2016/Data_science/Kaggle/User-click-detection-predictive-modeling/<ipython-input-25-de1c77550da9> in <module>()\n     20 CAL_LSVC_search = RandomizedSearchCV(cal_lsvc,\n     21                                      param_distributions= params_space,\n     22                                      n_jobs=3, cv = 3, \n     23                                      n_iter = 20,verbose=10)\n     24 \n---> 25 CAL_LSVC_search.fit(X_train_trans_pl1,y_train)\n     26 \n     27 end = datetime.datetime.now()\n     28 process_time = end - start\n     29 print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py in fit(self=RandomizedSearchCV(cv=3, error_score='raise',\n  ...urn_train_score='warn', scoring=None, verbose=10), X=<1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, y=0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64, groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=3, random_state=None, shuffle=False)>\n        X = <1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>\n        y = 0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=3), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=3)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Mar 30 11:20:57 2018\nPID: 3028                Python 3.6.1: /Users/OZANAYGUN/anaconda/bin/python\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), <1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, 0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64, {'score': <function _passthrough_scorer>}, memmap([332819, 333335, 333336, ..., 999997, 999998, 999999]), memmap([     0,      1,      2, ..., 333332, 333333, 333334]), 10, {'base_estimator__C': 1653615745673469.2, 'base_estimator__dual': True, 'base_estimator__penalty': 'l1'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), <1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, 0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64, {'score': <function _passthrough_scorer>}, memmap([332819, 333335, 333336, ..., 999997, 999998, 999999]), memmap([     0,      1,      2, ..., 333332, 333333, 333334]), 10, {'base_estimator__C': 1653615745673469.2, 'base_estimator__dual': True, 'base_estimator__penalty': 'l1'})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), X=<1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, y=0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64, scorer={'score': <function _passthrough_scorer>}, train=memmap([332819, 333335, 333336, ..., 999997, 999998, 999999]), test=memmap([     0,      1,      2, ..., 333332, 333333, 333334]), verbose=10, parameters={'base_estimator__C': 1653615745673469.2, 'base_estimator__dual': True, 'base_estimator__penalty': 'l1'}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method CalibratedClassifierCV.fit of Cali...verbose=10),\n            cv=3, method='sigmoid')>\n        X_train = <666666x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>\n        y_train = 332819    1\n333335    0\n333336    0\n333337    0\n...Name: is_attributed, Length: 666666, dtype: int64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/calibration.py in fit(self=CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), X=<666666x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([1, 0, 0, ..., 0, 0, 0]), sample_weight=None)\n    176                 if base_estimator_sample_weight is not None:\n    177                     this_estimator.fit(\n    178                         X[train], y[train],\n    179                         sample_weight=base_estimator_sample_weight[train])\n    180                 else:\n--> 181                     this_estimator.fit(X[train], y[train])\n        this_estimator.fit = <bound method LinearSVC.fit of LinearSVC(C=16536... random_state=None,\n     tol=0.0001, verbose=10)>\n        X = <666666x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>\n        train = array([215398, 215495, 216584, ..., 666663, 666664, 666665])\n        y = array([1, 0, 0, ..., 0, 0, 0])\n    182 \n    183                 calibrated_classifier = _CalibratedClassifier(\n    184                     this_estimator, method=self.method,\n    185                     classes=self.classes_)\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/classes.py in fit(self=LinearSVC(C=1653615745673469.2, class_weight=Non..., random_state=None,\n     tol=0.0001, verbose=10), X=<444444x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 0, 0, 0]), sample_weight=None)\n    230 \n    231         self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n    232             X, y, self.C, self.fit_intercept, self.intercept_scaling,\n    233             self.class_weight, self.penalty, self.dual, self.verbose,\n    234             self.max_iter, self.tol, self.random_state, self.multi_class,\n--> 235             self.loss, sample_weight=sample_weight)\n        self.loss = 'squared_hinge'\n        sample_weight = None\n    236 \n    237         if self.multi_class == \"crammer_singer\" and len(self.classes_) == 2:\n    238             self.coef_ = (self.coef_[1] - self.coef_[0]).reshape(1, -1)\n    239             if self.fit_intercept:\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py in _fit_liblinear(X=<444444x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 0, 0, 0]), C=1653615745673469.2, fit_intercept=True, intercept_scaling=1, class_weight=None, penalty='l1', dual=True, verbose=10, max_iter=1000, tol=0.0001, random_state=None, multi_class='ovr', loss='squared_hinge', epsilon=0.1, sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.]))\n    881         sample_weight = np.ones(X.shape[0])\n    882     else:\n    883         sample_weight = np.array(sample_weight, dtype=np.float64, order='C')\n    884         check_consistent_length(sample_weight, X)\n    885 \n--> 886     solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n        solver_type = undefined\n        multi_class = 'ovr'\n        penalty = 'l1'\n        loss = 'squared_hinge'\n        dual = True\n    887     raw_coef_, n_iter_ = liblinear.train_wrap(\n    888         X, y_ind, sp.isspmatrix(X), solver_type, tol, bias, C,\n    889         class_weight_, max_iter, rnd.randint(np.iinfo('i').max),\n    890         epsilon, sample_weight)\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py in _get_liblinear_solver_type(multi_class='ovr', penalty='l1', loss='squared_hinge', dual=True)\n    742                                 % (penalty, loss, dual))\n    743             else:\n    744                 return solver_num\n    745     raise ValueError('Unsupported set of arguments: %s, '\n    746                      'Parameters: penalty=%r, loss=%r, dual=%r'\n--> 747                      % (error_string, penalty, loss, dual))\n        error_string = \"The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True\"\n        penalty = 'l1'\n        loss = 'squared_hinge'\n        dual = True\n    748 \n    749 \n    750 def _fit_liblinear(X, y, C, fit_intercept, intercept_scaling, class_weight,\n    751                    penalty, dual, verbose, max_iter, tol,\n\nValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 458, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/calibration.py\", line 181, in fit\n    this_estimator.fit(X[train], y[train])\n  File \"/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/classes.py\", line 235, in fit\n    self.loss, sample_weight=sample_weight)\n  File \"/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py\", line 886, in _fit_liblinear\n    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n  File \"/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py\", line 747, in _get_liblinear_solver_type\n    % (error_string, penalty, loss, dual))\nValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/OZANAYGUN/anaconda/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nValueError                                         Fri Mar 30 11:20:57 2018\nPID: 3028                Python 3.6.1: /Users/OZANAYGUN/anaconda/bin/python\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), <1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, 0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64, {'score': <function _passthrough_scorer>}, memmap([332819, 333335, 333336, ..., 999997, 999998, 999999]), memmap([     0,      1,      2, ..., 333332, 333333, 333334]), 10, {'base_estimator__C': 1653615745673469.2, 'base_estimator__dual': True, 'base_estimator__penalty': 'l1'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), <1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, 0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64, {'score': <function _passthrough_scorer>}, memmap([332819, 333335, 333336, ..., 999997, 999998, 999999]), memmap([     0,      1,      2, ..., 333332, 333333, 333334]), 10, {'base_estimator__C': 1653615745673469.2, 'base_estimator__dual': True, 'base_estimator__penalty': 'l1'})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), X=<1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, y=0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64, scorer={'score': <function _passthrough_scorer>}, train=memmap([332819, 333335, 333336, ..., 999997, 999998, 999999]), test=memmap([     0,      1,      2, ..., 333332, 333333, 333334]), verbose=10, parameters={'base_estimator__C': 1653615745673469.2, 'base_estimator__dual': True, 'base_estimator__penalty': 'l1'}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method CalibratedClassifierCV.fit of Cali...verbose=10),\n            cv=3, method='sigmoid')>\n        X_train = <666666x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>\n        y_train = 332819    1\n333335    0\n333336    0\n333337    0\n...Name: is_attributed, Length: 666666, dtype: int64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/calibration.py in fit(self=CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), X=<666666x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([1, 0, 0, ..., 0, 0, 0]), sample_weight=None)\n    176                 if base_estimator_sample_weight is not None:\n    177                     this_estimator.fit(\n    178                         X[train], y[train],\n    179                         sample_weight=base_estimator_sample_weight[train])\n    180                 else:\n--> 181                     this_estimator.fit(X[train], y[train])\n        this_estimator.fit = <bound method LinearSVC.fit of LinearSVC(C=16536... random_state=None,\n     tol=0.0001, verbose=10)>\n        X = <666666x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>\n        train = array([215398, 215495, 216584, ..., 666663, 666664, 666665])\n        y = array([1, 0, 0, ..., 0, 0, 0])\n    182 \n    183                 calibrated_classifier = _CalibratedClassifier(\n    184                     this_estimator, method=self.method,\n    185                     classes=self.classes_)\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/classes.py in fit(self=LinearSVC(C=1653615745673469.2, class_weight=Non..., random_state=None,\n     tol=0.0001, verbose=10), X=<444444x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 0, 0, 0]), sample_weight=None)\n    230 \n    231         self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n    232             X, y, self.C, self.fit_intercept, self.intercept_scaling,\n    233             self.class_weight, self.penalty, self.dual, self.verbose,\n    234             self.max_iter, self.tol, self.random_state, self.multi_class,\n--> 235             self.loss, sample_weight=sample_weight)\n        self.loss = 'squared_hinge'\n        sample_weight = None\n    236 \n    237         if self.multi_class == \"crammer_singer\" and len(self.classes_) == 2:\n    238             self.coef_ = (self.coef_[1] - self.coef_[0]).reshape(1, -1)\n    239             if self.fit_intercept:\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py in _fit_liblinear(X=<444444x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 0, 0, 0]), C=1653615745673469.2, fit_intercept=True, intercept_scaling=1, class_weight=None, penalty='l1', dual=True, verbose=10, max_iter=1000, tol=0.0001, random_state=None, multi_class='ovr', loss='squared_hinge', epsilon=0.1, sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.]))\n    881         sample_weight = np.ones(X.shape[0])\n    882     else:\n    883         sample_weight = np.array(sample_weight, dtype=np.float64, order='C')\n    884         check_consistent_length(sample_weight, X)\n    885 \n--> 886     solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n        solver_type = undefined\n        multi_class = 'ovr'\n        penalty = 'l1'\n        loss = 'squared_hinge'\n        dual = True\n    887     raw_coef_, n_iter_ = liblinear.train_wrap(\n    888         X, y_ind, sp.isspmatrix(X), solver_type, tol, bias, C,\n    889         class_weight_, max_iter, rnd.randint(np.iinfo('i').max),\n    890         epsilon, sample_weight)\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py in _get_liblinear_solver_type(multi_class='ovr', penalty='l1', loss='squared_hinge', dual=True)\n    742                                 % (penalty, loss, dual))\n    743             else:\n    744                 return solver_num\n    745     raise ValueError('Unsupported set of arguments: %s, '\n    746                      'Parameters: penalty=%r, loss=%r, dual=%r'\n--> 747                      % (error_string, penalty, loss, dual))\n        error_string = \"The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True\"\n        penalty = 'l1'\n        loss = 'squared_hinge'\n        dual = True\n    748 \n    749 \n    750 def _fit_liblinear(X, y, C, fit_intercept, intercept_scaling, class_weight,\n    751                    penalty, dual, verbose, max_iter, tol,\n\nValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nValueError                                         Fri Mar 30 11:20:57 2018\nPID: 3028                Python 3.6.1: /Users/OZANAYGUN/anaconda/bin/python\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), <1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, 0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64, {'score': <function _passthrough_scorer>}, memmap([332819, 333335, 333336, ..., 999997, 999998, 999999]), memmap([     0,      1,      2, ..., 333332, 333333, 333334]), 10, {'base_estimator__C': 1653615745673469.2, 'base_estimator__dual': True, 'base_estimator__penalty': 'l1'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), <1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, 0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64, {'score': <function _passthrough_scorer>}, memmap([332819, 333335, 333336, ..., 999997, 999998, 999999]), memmap([     0,      1,      2, ..., 333332, 333333, 333334]), 10, {'base_estimator__C': 1653615745673469.2, 'base_estimator__dual': True, 'base_estimator__penalty': 'l1'})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), X=<1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, y=0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64, scorer={'score': <function _passthrough_scorer>}, train=memmap([332819, 333335, 333336, ..., 999997, 999998, 999999]), test=memmap([     0,      1,      2, ..., 333332, 333333, 333334]), verbose=10, parameters={'base_estimator__C': 1653615745673469.2, 'base_estimator__dual': True, 'base_estimator__penalty': 'l1'}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method CalibratedClassifierCV.fit of Cali...verbose=10),\n            cv=3, method='sigmoid')>\n        X_train = <666666x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>\n        y_train = 332819    1\n333335    0\n333336    0\n333337    0\n...Name: is_attributed, Length: 666666, dtype: int64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/calibration.py in fit(self=CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), X=<666666x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([1, 0, 0, ..., 0, 0, 0]), sample_weight=None)\n    176                 if base_estimator_sample_weight is not None:\n    177                     this_estimator.fit(\n    178                         X[train], y[train],\n    179                         sample_weight=base_estimator_sample_weight[train])\n    180                 else:\n--> 181                     this_estimator.fit(X[train], y[train])\n        this_estimator.fit = <bound method LinearSVC.fit of LinearSVC(C=16536... random_state=None,\n     tol=0.0001, verbose=10)>\n        X = <666666x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>\n        train = array([215398, 215495, 216584, ..., 666663, 666664, 666665])\n        y = array([1, 0, 0, ..., 0, 0, 0])\n    182 \n    183                 calibrated_classifier = _CalibratedClassifier(\n    184                     this_estimator, method=self.method,\n    185                     classes=self.classes_)\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/classes.py in fit(self=LinearSVC(C=1653615745673469.2, class_weight=Non..., random_state=None,\n     tol=0.0001, verbose=10), X=<444444x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 0, 0, 0]), sample_weight=None)\n    230 \n    231         self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n    232             X, y, self.C, self.fit_intercept, self.intercept_scaling,\n    233             self.class_weight, self.penalty, self.dual, self.verbose,\n    234             self.max_iter, self.tol, self.random_state, self.multi_class,\n--> 235             self.loss, sample_weight=sample_weight)\n        self.loss = 'squared_hinge'\n        sample_weight = None\n    236 \n    237         if self.multi_class == \"crammer_singer\" and len(self.classes_) == 2:\n    238             self.coef_ = (self.coef_[1] - self.coef_[0]).reshape(1, -1)\n    239             if self.fit_intercept:\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py in _fit_liblinear(X=<444444x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 0, 0, 0]), C=1653615745673469.2, fit_intercept=True, intercept_scaling=1, class_weight=None, penalty='l1', dual=True, verbose=10, max_iter=1000, tol=0.0001, random_state=None, multi_class='ovr', loss='squared_hinge', epsilon=0.1, sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.]))\n    881         sample_weight = np.ones(X.shape[0])\n    882     else:\n    883         sample_weight = np.array(sample_weight, dtype=np.float64, order='C')\n    884         check_consistent_length(sample_weight, X)\n    885 \n--> 886     solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n        solver_type = undefined\n        multi_class = 'ovr'\n        penalty = 'l1'\n        loss = 'squared_hinge'\n        dual = True\n    887     raw_coef_, n_iter_ = liblinear.train_wrap(\n    888         X, y_ind, sp.isspmatrix(X), solver_type, tol, bias, C,\n    889         class_weight_, max_iter, rnd.randint(np.iinfo('i').max),\n    890         epsilon, sample_weight)\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py in _get_liblinear_solver_type(multi_class='ovr', penalty='l1', loss='squared_hinge', dual=True)\n    742                                 % (penalty, loss, dual))\n    743             else:\n    744                 return solver_num\n    745     raise ValueError('Unsupported set of arguments: %s, '\n    746                      'Parameters: penalty=%r, loss=%r, dual=%r'\n--> 747                      % (error_string, penalty, loss, dual))\n        error_string = \"The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True\"\n        penalty = 'l1'\n        loss = 'squared_hinge'\n        dual = True\n    748 \n    749 \n    750 def _fit_liblinear(X, y, C, fit_intercept, intercept_scaling, class_weight,\n    751                    penalty, dual, verbose, max_iter, tol,\n\nValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-de1c77550da9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                                      n_iter = 20,verbose=10)\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mCAL_LSVC_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_trans_pl1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibValueError\u001b[0m: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x10659a6f0, file \"/Use...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/OZANA.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x10659a6f0, file \"/Use...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/OZANA.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 3, 30, 15, 20, 51, 429826, tzinfo=tzutc()), 'msg_id': '51FA63EB565D4EA58DDEABB29D33E8F3', 'msg_type': 'execute_request', 'session': '2298C9A937DC4557997947C4492FD686', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '51FA63EB565D4EA58DDEABB29D33E8F3', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'2298C9A937DC4557997947C4492FD686']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 3, 30, 15, 20, 51, 429826, tzinfo=tzutc()), 'msg_id': '51FA63EB565D4EA58DDEABB29D33E8F3', 'msg_type': 'execute_request', 'session': '2298C9A937DC4557997947C4492FD686', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '51FA63EB565D4EA58DDEABB29D33E8F3', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'2298C9A937DC4557997947C4492FD686'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 3, 30, 15, 20, 51, 429826, tzinfo=tzutc()), 'msg_id': '51FA63EB565D4EA58DDEABB29D33E8F3', 'msg_type': 'execute_request', 'session': '2298C9A937DC4557997947C4492FD686', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '51FA63EB565D4EA58DDEABB29D33E8F3', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")',), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")',)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Import object>, <_ast.Assign object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>], cell_name='<ipython-input-25-de1c77550da9>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 1a290950f0, execution..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x10bac21e0, file \"<ipython-input-25-de1c77550da9>\", line 25>\n        result = <ExecutionResult object at 1a290950f0, execution..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x10bac21e0, file \"<ipython-input-25-de1c77550da9>\", line 25>, result=<ExecutionResult object at 1a290950f0, execution..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x10bac21e0, file \"<ipython-input-25-de1c77550da9>\", line 25>\n        self.user_global_ns = {'CAL_LSVC_search': RandomizedSearchCV(cv=3, error_score='raise',\n  ...urn_train_score='warn', scoring=None, verbose=10), 'CalibratedClassifierCV': <class 'sklearn.calibration.CalibratedClassifierCV'>, 'In': ['', 'from sklearn.svm import LinearSVC\\nfrom sklearn.calibration import CalibratedClassifierCV', 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', '# Read the transformed features and target label...in.pkl\",\"rb\") as f:\\n    y_train = pickle.load(f) ', 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', 'probs = cal_lsvc.predict_proba(X_train_trans_pl1) ', 'probs', 'probs = cal_lsvc.predict_proba(X_train_trans_pl1)[:,1] ', 'probs', '# Calculate ROC score\\nfrom sklearn.metrics import roc_auc_score', '# Calculate ROC score\\nfrom sklearn.metrics import roc_auc_score\\nroc_auc_score(y_train,probs)', 'cal_lsvc.get_params', 'cal_lsvc.get_params()', 'help(LinearSVC)', 'np.logspace(0.1,1000)', 'np.logspace(1,1000)', 'np.logspace(1000)', 'np.logspace(0.1,1000,2)', 'np.logspace(0.1,1000,100)', 'np.logspace(0.1,1000,10)', ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'Out': {6: array([[  9.98604863e-01,   1.39513749e-03],\n   ...3],\n       [  9.98843163e-01,   1.15683681e-03]]), 8: array([  1.39513749e-03,   1.37266016e-03,   1.3...1099911e-05,   1.23671390e-03,   1.15683681e-03]), 10: 0.95099179905081954, 11: <bound method BaseEstimator.get_params of Calibr...verbose=10),\n            cv=3, method='sigmoid')>, 12: {'base_estimator': LinearSVC(C=1.0, class_weight=None, dual=True, f..., random_state=None, tol=0.0001,\n     verbose=10), 'base_estimator__C': 1.0, 'base_estimator__class_weight': None, 'base_estimator__dual': True, 'base_estimator__fit_intercept': True, 'base_estimator__intercept_scaling': 1, 'base_estimator__loss': 'squared_hinge', 'base_estimator__max_iter': 1000, 'base_estimator__multi_class': 'ovr', 'base_estimator__penalty': 'l2', ...}, 14: array([  1.25892541e+000,   3.20717346e+020,   8...nf,\n                     inf,               inf]), 15: array([  1.00000000e+001,   2.44205309e+021,   5...nf,\n                     inf,               inf]), 17: array([ 1.25892541,         inf]), 18: array([  1.25892541e+000,   1.58489319e+010,   1...nf,               inf,\n                     inf]), 19: array([  1.25892541e+000,   1.58489319e+111,   1...nf,               inf,\n                     inf]), ...}, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'X_train_trans_pl1': <1000000x45753 sparse matrix of type '<class 'nu...ored elements in Compressed Sparse Column format>, '_': array([  1.07177346e+00,   2.15709679e+00,   4.3...e+29,   6.29843910e+29,\n         1.26765060e+30]), '_10': 0.95099179905081954, '_11': <bound method BaseEstimator.get_params of Calibr...verbose=10),\n            cv=3, method='sigmoid')>, ...}\n        self.user_ns = {'CAL_LSVC_search': RandomizedSearchCV(cv=3, error_score='raise',\n  ...urn_train_score='warn', scoring=None, verbose=10), 'CalibratedClassifierCV': <class 'sklearn.calibration.CalibratedClassifierCV'>, 'In': ['', 'from sklearn.svm import LinearSVC\\nfrom sklearn.calibration import CalibratedClassifierCV', 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', '# Read the transformed features and target label...in.pkl\",\"rb\") as f:\\n    y_train = pickle.load(f) ', 'import datetime\\nstart = datetime.datetime.now()\\n...: \" + str(process_time.seconds/60) + \" minutes.\")', 'probs = cal_lsvc.predict_proba(X_train_trans_pl1) ', 'probs', 'probs = cal_lsvc.predict_proba(X_train_trans_pl1)[:,1] ', 'probs', '# Calculate ROC score\\nfrom sklearn.metrics import roc_auc_score', '# Calculate ROC score\\nfrom sklearn.metrics import roc_auc_score\\nroc_auc_score(y_train,probs)', 'cal_lsvc.get_params', 'cal_lsvc.get_params()', 'help(LinearSVC)', 'np.logspace(0.1,1000)', 'np.logspace(1,1000)', 'np.logspace(1000)', 'np.logspace(0.1,1000,2)', 'np.logspace(0.1,1000,100)', 'np.logspace(0.1,1000,10)', ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'Out': {6: array([[  9.98604863e-01,   1.39513749e-03],\n   ...3],\n       [  9.98843163e-01,   1.15683681e-03]]), 8: array([  1.39513749e-03,   1.37266016e-03,   1.3...1099911e-05,   1.23671390e-03,   1.15683681e-03]), 10: 0.95099179905081954, 11: <bound method BaseEstimator.get_params of Calibr...verbose=10),\n            cv=3, method='sigmoid')>, 12: {'base_estimator': LinearSVC(C=1.0, class_weight=None, dual=True, f..., random_state=None, tol=0.0001,\n     verbose=10), 'base_estimator__C': 1.0, 'base_estimator__class_weight': None, 'base_estimator__dual': True, 'base_estimator__fit_intercept': True, 'base_estimator__intercept_scaling': 1, 'base_estimator__loss': 'squared_hinge', 'base_estimator__max_iter': 1000, 'base_estimator__multi_class': 'ovr', 'base_estimator__penalty': 'l2', ...}, 14: array([  1.25892541e+000,   3.20717346e+020,   8...nf,\n                     inf,               inf]), 15: array([  1.00000000e+001,   2.44205309e+021,   5...nf,\n                     inf,               inf]), 17: array([ 1.25892541,         inf]), 18: array([  1.25892541e+000,   1.58489319e+010,   1...nf,               inf,\n                     inf]), 19: array([  1.25892541e+000,   1.58489319e+111,   1...nf,               inf,\n                     inf]), ...}, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'X_train_trans_pl1': <1000000x45753 sparse matrix of type '<class 'nu...ored elements in Compressed Sparse Column format>, '_': array([  1.07177346e+00,   2.15709679e+00,   4.3...e+29,   6.29843910e+29,\n         1.26765060e+30]), '_10': 0.95099179905081954, '_11': <bound method BaseEstimator.get_params of Calibr...verbose=10),\n            cv=3, method='sigmoid')>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/Users/OZANAYGUN/Desktop/2016/Data_science/Kaggle/User-click-detection-predictive-modeling/<ipython-input-25-de1c77550da9> in <module>()\n     20 CAL_LSVC_search = RandomizedSearchCV(cal_lsvc,\n     21                                      param_distributions= params_space,\n     22                                      n_jobs=3, cv = 3, \n     23                                      n_iter = 20,verbose=10)\n     24 \n---> 25 CAL_LSVC_search.fit(X_train_trans_pl1,y_train)\n     26 \n     27 end = datetime.datetime.now()\n     28 process_time = end - start\n     29 print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py in fit(self=RandomizedSearchCV(cv=3, error_score='raise',\n  ...urn_train_score='warn', scoring=None, verbose=10), X=<1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, y=0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64, groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=3, random_state=None, shuffle=False)>\n        X = <1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>\n        y = 0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=3), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=3)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Mar 30 11:20:57 2018\nPID: 3028                Python 3.6.1: /Users/OZANAYGUN/anaconda/bin/python\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), <1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, 0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64, {'score': <function _passthrough_scorer>}, memmap([332819, 333335, 333336, ..., 999997, 999998, 999999]), memmap([     0,      1,      2, ..., 333332, 333333, 333334]), 10, {'base_estimator__C': 1653615745673469.2, 'base_estimator__dual': True, 'base_estimator__penalty': 'l1'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), <1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, 0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64, {'score': <function _passthrough_scorer>}, memmap([332819, 333335, 333336, ..., 999997, 999998, 999999]), memmap([     0,      1,      2, ..., 333332, 333333, 333334]), 10, {'base_estimator__C': 1653615745673469.2, 'base_estimator__dual': True, 'base_estimator__penalty': 'l1'})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), X=<1000000x45753 sparse matrix of type '<class 'nu... stored elements in Compressed Sparse Row format>, y=0         0\n1         0\n2         0\n3         0\n...ame: is_attributed, Length: 1000000, dtype: int64, scorer={'score': <function _passthrough_scorer>}, train=memmap([332819, 333335, 333336, ..., 999997, 999998, 999999]), test=memmap([     0,      1,      2, ..., 333332, 333333, 333334]), verbose=10, parameters={'base_estimator__C': 1653615745673469.2, 'base_estimator__dual': True, 'base_estimator__penalty': 'l1'}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method CalibratedClassifierCV.fit of Cali...verbose=10),\n            cv=3, method='sigmoid')>\n        X_train = <666666x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>\n        y_train = 332819    1\n333335    0\n333336    0\n333337    0\n...Name: is_attributed, Length: 666666, dtype: int64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/calibration.py in fit(self=CalibratedClassifierCV(base_estimator=LinearSVC(... verbose=10),\n            cv=3, method='sigmoid'), X=<666666x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([1, 0, 0, ..., 0, 0, 0]), sample_weight=None)\n    176                 if base_estimator_sample_weight is not None:\n    177                     this_estimator.fit(\n    178                         X[train], y[train],\n    179                         sample_weight=base_estimator_sample_weight[train])\n    180                 else:\n--> 181                     this_estimator.fit(X[train], y[train])\n        this_estimator.fit = <bound method LinearSVC.fit of LinearSVC(C=16536... random_state=None,\n     tol=0.0001, verbose=10)>\n        X = <666666x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>\n        train = array([215398, 215495, 216584, ..., 666663, 666664, 666665])\n        y = array([1, 0, 0, ..., 0, 0, 0])\n    182 \n    183                 calibrated_classifier = _CalibratedClassifier(\n    184                     this_estimator, method=self.method,\n    185                     classes=self.classes_)\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/classes.py in fit(self=LinearSVC(C=1653615745673469.2, class_weight=Non..., random_state=None,\n     tol=0.0001, verbose=10), X=<444444x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 0, 0, 0]), sample_weight=None)\n    230 \n    231         self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n    232             X, y, self.C, self.fit_intercept, self.intercept_scaling,\n    233             self.class_weight, self.penalty, self.dual, self.verbose,\n    234             self.max_iter, self.tol, self.random_state, self.multi_class,\n--> 235             self.loss, sample_weight=sample_weight)\n        self.loss = 'squared_hinge'\n        sample_weight = None\n    236 \n    237         if self.multi_class == \"crammer_singer\" and len(self.classes_) == 2:\n    238             self.coef_ = (self.coef_[1] - self.coef_[0]).reshape(1, -1)\n    239             if self.fit_intercept:\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py in _fit_liblinear(X=<444444x45753 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([1, 1, 1, ..., 0, 0, 0]), C=1653615745673469.2, fit_intercept=True, intercept_scaling=1, class_weight=None, penalty='l1', dual=True, verbose=10, max_iter=1000, tol=0.0001, random_state=None, multi_class='ovr', loss='squared_hinge', epsilon=0.1, sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.]))\n    881         sample_weight = np.ones(X.shape[0])\n    882     else:\n    883         sample_weight = np.array(sample_weight, dtype=np.float64, order='C')\n    884         check_consistent_length(sample_weight, X)\n    885 \n--> 886     solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n        solver_type = undefined\n        multi_class = 'ovr'\n        penalty = 'l1'\n        loss = 'squared_hinge'\n        dual = True\n    887     raw_coef_, n_iter_ = liblinear.train_wrap(\n    888         X, y_ind, sp.isspmatrix(X), solver_type, tol, bias, C,\n    889         class_weight_, max_iter, rnd.randint(np.iinfo('i').max),\n    890         epsilon, sample_weight)\n\n...........................................................................\n/Users/OZANAYGUN/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py in _get_liblinear_solver_type(multi_class='ovr', penalty='l1', loss='squared_hinge', dual=True)\n    742                                 % (penalty, loss, dual))\n    743             else:\n    744                 return solver_num\n    745     raise ValueError('Unsupported set of arguments: %s, '\n    746                      'Parameters: penalty=%r, loss=%r, dual=%r'\n--> 747                      % (error_string, penalty, loss, dual))\n        error_string = \"The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True\"\n        penalty = 'l1'\n        loss = 'squared_hinge'\n        dual = True\n    748 \n    749 \n    750 def _fit_liblinear(X, y, C, fit_intercept, intercept_scaling, class_weight,\n    751                    penalty, dual, verbose, max_iter, tol,\n\nValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "lsvc = LinearSVC(verbose=10)\n",
    "\n",
    "cal_lsvc = CalibratedClassifierCV(base_estimator = lsvc,\n",
    "                                  cv = 3, # Also performs cross-validation if needed\n",
    "                                  method= \"sigmoid\") # We use sigmoid function to get probabilities\n",
    "\n",
    "params_space = {\n",
    "    \"base_estimator__penalty\":['l1','l2'],\n",
    "    \"base_estimator__dual\":[False,True],\n",
    "    \"base_estimator__C\":np.logspace(0.1,100,base = 2, num=100)   \n",
    "}\n",
    "\n",
    "CAL_LSVC_search = RandomizedSearchCV(cal_lsvc,\n",
    "                                     param_distributions= params_space,\n",
    "                                     n_jobs=3, cv = 3, \n",
    "                                     n_iter = 20,verbose=10)\n",
    "\n",
    "CAL_LSVC_search.fit(X_train_trans_pl1,y_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "process_time = end - start\n",
    "print(\"It took: \" + str(process_time.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
